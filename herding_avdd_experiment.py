#!/usr/bin/env python3
"""
Herdingç®—æ³•å¢å¼ºçš„AVDDå®éªŒ
è§£å†³å°æ ·æœ¬è’¸é¦ä¸­çš„ä»£è¡¨æ€§é—®é¢˜
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

class HerdingSelector:
    """Herdingç®—æ³•é€‰æ‹©å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def herding_selection(self, features, num_select):
        """
        Herdingç®—æ³•é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
        Args:
            features: ç‰¹å¾çŸ©é˜µ [N, D]
            num_select: è¦é€‰æ‹©çš„æ ·æœ¬æ•°é‡
        Returns:
            selected_indices: é€‰æ‹©çš„æ ·æœ¬ç´¢å¼•
        """
        features = features.detach().cpu().numpy()
        n_samples = features.shape[0]
        
        if num_select >= n_samples:
            return torch.arange(n_samples)
        
        # è®¡ç®—ç‰¹å¾å‡å€¼
        mean_feature = np.mean(features, axis=0, keepdims=True)
        
        # åˆå§‹åŒ–
        selected_indices = []
        remaining_indices = list(range(n_samples))
        
        # ç¬¬ä¸€ä¸ªé€‰æ‹©æœ€æ¥è¿‘å‡å€¼çš„æ ·æœ¬
        similarities = cosine_similarity(mean_feature, features)[0]
        first_idx = np.argmax(similarities)
        selected_indices.append(first_idx)
        remaining_indices.remove(first_idx)
        
        # è¿­ä»£é€‰æ‹©
        selected_features = features[selected_indices]
        
        for _ in range(1, num_select):
            if not remaining_indices:
                break
                
            # è®¡ç®—å½“å‰é€‰æ‹©é›†çš„ç‰¹å¾å‡å€¼
            current_mean = np.mean(selected_features, axis=0, keepdims=True)
            
            # è®¡ç®—å‰©ä½™æ ·æœ¬ä¸å½“å‰å‡å€¼çš„ç›¸ä¼¼åº¦
            remaining_features = features[remaining_indices]
            similarities = cosine_similarity(current_mean, remaining_features)[0]
            
            # é€‰æ‹©æœ€ç›¸ä¼¼çš„æ ·æœ¬
            best_idx_in_remaining = np.argmax(similarities)
            best_idx = remaining_indices[best_idx_in_remaining]
            
            selected_indices.append(best_idx)
            remaining_indices.remove(best_idx)
            selected_features = features[selected_indices]
        
        return torch.tensor(selected_indices, dtype=torch.long)

class HerdingAVDD:
    """Herdingå¢å¼ºçš„AVDDç®—æ³•"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = HerdingSelector(device)
    
    def run_herding_experiment(self, num_real_samples=100, num_syn_samples=20, 
                             feature_dim=1024, iterations=200):
        """è¿è¡ŒHerdingå¢å¼ºçš„AVDDå®éªŒ"""
        
        print("=" * 80)
        print("ğŸ¯ Herdingå¢å¼ºAVDDå®éªŒ")
        print("=" * 80)
        print(f"ğŸ“Š çœŸå®æ ·æœ¬: {num_real_samples}")
        print(f"ğŸ¯ åˆæˆæ ·æœ¬: {num_syn_samples}")
        print(f"âš™ï¸  ç‰¹å¾ç»´åº¦: {feature_dim}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # ç”ŸæˆçœŸå®æ•°æ®
        print("\nğŸ“Š ç”ŸæˆçœŸå®æ•°æ®...")
        real_audio_features = F.normalize(torch.randn(num_real_samples, feature_dim, device=device), dim=-1)
        real_text_features = F.normalize(torch.randn(num_real_samples, feature_dim, device=device), dim=-1)
        
        # åˆ›å»ºçœŸå®ç›¸å…³æ€§
        correlation_strength = 0.8
        for i in range(num_real_samples):
            real_text_features[i] = correlation_strength * real_audio_features[i] + \
                                   (1 - correlation_strength) * torch.randn(feature_dim, device=device)
        real_text_features = F.normalize(real_text_features, dim=-1)
        
        # ä½¿ç”¨Herdingé€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
        print("\nğŸ¯ ä½¿ç”¨Herdingé€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬...")
        audio_indices = self.herding.herding_selection(real_audio_features, num_syn_samples)
        text_indices = self.herding.herding_selection(real_text_features, num_syn_samples)
        
        print(f"  é€‰æ‹©éŸ³é¢‘æ ·æœ¬ç´¢å¼•: {audio_indices[:10].tolist()}...")
        print(f"  é€‰æ‹©æ–‡æœ¬æ ·æœ¬ç´¢å¼•: {text_indices[:10].tolist()}...")
        
        selected_audio = real_audio_features[audio_indices]
        selected_text = real_text_features[text_indices]
        
        # åˆå§‹åŒ–åˆæˆæ•°æ®ï¼ˆåŸºäºé€‰æ‹©çš„æ ·æœ¬ï¼‰
        print("\nğŸ¯ åˆå§‹åŒ–åˆæˆæ•°æ®...")
        syn_audio = selected_audio.clone() + 0.1 * torch.randn_like(selected_audio)
        syn_text = selected_text.clone() + 0.1 * torch.randn_like(selected_text)
        
        syn_audio.requires_grad = True
        syn_text.requires_grad = True
        
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, iterations)
        
        loss_history = []
        
        # è¿è¡Œè’¸é¦
        print("\nğŸ”¬ å¼€å§‹Herdingå¢å¼ºè’¸é¦...")
        start_time = time.time()
        
        for iteration in range(iterations):
            # ç”Ÿæˆåˆæˆç‰¹å¾
            syn_audio_feat = F.normalize(syn_audio, dim=-1)
            syn_text_feat = F.normalize(syn_text, dim=-1)
            
            # è®¡ç®—AVDDæŸå¤±
            # 1. åˆ†å¸ƒåŒ¹é…æŸå¤±
            target_audio = real_audio_features[audio_indices]
            target_text = real_text_features[text_indices]
            loss_dm = F.mse_loss(syn_audio_feat, target_audio) + \
                     F.mse_loss(syn_text_feat, target_text)
            
            # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤±
            real_sim = torch.matmul(target_audio, target_text.t())
            syn_sim = torch.matmul(syn_audio_feat, syn_text_feat.t())
            loss_icm = F.mse_loss(syn_sim, real_sim)
            
            # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤±
            real_gap = target_audio.mean(0) - target_text.mean(0)
            syn_gap = syn_audio_feat.mean(0) - syn_text_feat.mean(0)
            loss_cgm = F.mse_loss(syn_gap, real_gap)
            
            # 4. Herdingæ­£åˆ™åŒ–ï¼ˆä¿æŒå¤šæ ·æ€§ï¼‰
            # é˜²æ­¢æ‰€æœ‰åˆæˆæ ·æœ¬å˜å¾—ç›¸åŒ
            audio_diversity = torch.std(syn_audio_feat, dim=0).mean()
            text_diversity = torch.std(syn_text_feat, dim=0).mean()
            loss_diversity = -0.1 * (audio_diversity + text_diversity)  # è´Ÿå·è¡¨ç¤ºé¼“åŠ±å¤šæ ·æ€§
            
            loss = loss_dm + 5.0 * loss_icm + 2.0 * loss_cgm + loss_diversity
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            loss_history.append(loss.item())
            
            if iteration % 40 == 0 or iteration == iterations - 1:
                print(f"è¿­ä»£ {iteration:3d}/{iterations}, æŸå¤±: {loss.item():.6f}")
        
        elapsed_time = time.time() - start_time
        
        # è¯„ä¼°æœ€ç»ˆæ€§èƒ½
        print("\nğŸ“ˆ è¯„ä¼°æœ€ç»ˆæ£€ç´¢æ€§èƒ½...")
        final_audio_feat = F.normalize(syn_audio, dim=-1)
        final_text_feat = F.normalize(syn_text, dim=-1)
        
        # åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¯„ä¼°
        results = evaluate_herding_retrieval(final_audio_feat, final_text_feat, 
                                           real_audio_features, real_text_features)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ“Š Herdingå¢å¼ºå®éªŒç»“æœ")
        print("=" * 80)
        
        print(f"\nğŸ” æ£€ç´¢æ€§èƒ½:")
        print("-" * 50)
        for k in [1, 5, 10]:
            print(f"  Recall@{k}: A2T={results[f'R@{k}_a2t']:.4f} ({results[f'R@{k}_a2t']*100:.2f}%), "
                  f"T2A={results[f'R@{k}_t2a']:.4f} ({results[f'R@{k}_t2a']*100:.2f}%)")
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"  åˆå§‹æŸå¤±: {loss_history[0]:.6f}")
        print(f"  æœ€ç»ˆæŸå¤±: {loss_history[-1]:.6f}")
        print(f"  æŸå¤±ä¸‹é™: {(loss_history[0] - loss_history[-1]):.6f}")
        print(f"  è®­ç»ƒæ—¶é—´: {elapsed_time:.2f}ç§’")
        
        # ä¿å­˜ç»“æœ
        results_dict = {
            'method': 'herding_avdd',
            'num_real_samples': num_real_samples,
            'num_syn_samples': num_syn_samples,
            'feature_dim': feature_dim,
            'iterations': iterations,
            'retrieval_results': results,
            'loss_history': loss_history,
            'elapsed_time': elapsed_time,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        filename = f'herding_avdd_{feature_dim}d_{num_syn_samples}s_results.pth'
        torch.save(results_dict, filename)
        
        return results_dict

def evaluate_herding_retrieval(syn_audio, syn_text, real_audio, real_text, k_values=[1, 5, 10]):
    """è¯„ä¼°Herdingå¢å¼ºçš„æ£€ç´¢æ€§èƒ½"""
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.matmul(real_audio, syn_text.t())
    sim_t2a = torch.matmul(real_text, syn_audio.t())
    
    results = {}
    
    # Audio-to-Textæ£€ç´¢ (çœŸå®éŸ³é¢‘ vs åˆæˆæ–‡æœ¬)
    _, top_indices_a2t = torch.topk(sim_matrix, k=min(max(k_values), sim_matrix.size(1)), dim=1)
    
    # Text-to-Audioæ£€ç´¢ (çœŸå®æ–‡æœ¬ vs åˆæˆéŸ³é¢‘)
    _, top_indices_t2a = torch.topk(sim_t2a, k=min(max(k_values), sim_t2a.size(1)), dim=1)
    
    for k in k_values:
        # å®é™…kå€¼ä¸èƒ½è¶…è¿‡åˆæˆæ ·æœ¬æ•°é‡
        actual_k = min(k, syn_audio.size(0))
        
        # A2Tå¬å›ç‡
        correct_a2t = 0
        for i in range(min(len(real_audio), len(syn_text))):
            if i < top_indices_a2t.size(1):
                # æ‰¾åˆ°æœ€è¿‘çš„åˆæˆæ ·æœ¬
                closest_syn_idx = top_indices_a2t[i, 0]
                if closest_syn_idx < len(syn_text):
                    # è®¡ç®—çœŸå®æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦
                    true_sim = torch.matmul(real_audio[i:i+1], real_text[i:i+1].t())
                    pred_sim = torch.matmul(real_audio[i:i+1], syn_text[closest_syn_idx:closest_syn_idx+1].t())
                    if abs(true_sim.item() - pred_sim.item()) < 0.1:
                        correct_a2t += 1
        recall_a2t = correct_a2t / min(len(real_audio), len(syn_text))
        
        # T2Aå¬å›ç‡
        correct_t2a = 0
        for i in range(min(len(real_text), len(syn_audio))):
            if i < top_indices_t2a.size(1):
                closest_syn_idx = top_indices_t2a[i, 0]
                if closest_syn_idx < len(syn_audio):
                    true_sim = torch.matmul(real_text[i:i+1], real_audio[i:i+1].t())
                    pred_sim = torch.matmul(real_text[i:i+1], syn_audio[closest_syn_idx:closest_syn_idx+1].t())
                    if abs(true_sim.item() - pred_sim.item()) < 0.1:
                        correct_t2a += 1
        recall_t2a = correct_t2a / min(len(real_text), len(syn_audio))
        
        results[f'R@{k}_a2t'] = recall_a2t
        results[f'R@{k}_t2a'] = recall_t2a
    
    return results

if __name__ == "__main__":
    print("ğŸ† Herdingå¢å¼ºAVDDå®éªŒ")
    print("=" * 80)
    
    # è¿è¡ŒHerdingå®éªŒ
    herding_engine = HerdingAVDD(device)
    
    # ConvNetå®éªŒ
    results_convnet = herding_engine.run_herding_experiment(
        num_real_samples=100, num_syn_samples=20, feature_dim=6272, iterations=200
    )
    
    print("\n" + "=" * 80)
    
    # ImageBindå®éªŒ  
    results_imagebind = herding_engine.run_herding_experiment(
        num_real_samples=100, num_syn_samples=20, feature_dim=1024, iterations=200
    )
    
    print("\n" + "=" * 80)
    print("ğŸ¯ Herdingå®éªŒæ€»ç»“")
    print("=" * 80)
    
    print(f"{'ç‰¹å¾ç±»å‹':<12} {'R@1_A2T':<10} {'R@1_T2A':<10} {'R@5_A2T':<10} {'R@5_T2A':<10}")
    print("-" * 60)
    
    for results in [results_convnet, results_imagebind]:
        ft = 'convnet' if results['feature_dim'] == 6272 else 'imagebind'
        r1_a2t = results['retrieval_results']['R@1_a2t']
        r1_t2a = results['retrieval_results']['R@1_t2a']
        r5_a2t = results['retrieval_results']['R@5_a2t']
        r5_t2a = results['retrieval_results']['R@5_t2a']
        
        print(f"{ft:<12} {r1_a2t:<10.4f} {r1_t2a:<10.4f} {r5_a2t:<10.4f} {r5_t2a:<10.4f}")