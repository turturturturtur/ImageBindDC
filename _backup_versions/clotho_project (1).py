#!/usr/bin/env python3
"""
Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ
éµå¾ªæŠ€æœ¯è§„æ ¼ä¹¦çš„å®Œæ•´å››é˜¶æ®µå®ç°
"""

import os
import json
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from typing import Dict, Tuple, List

device = 'cuda' if torch.cuda.is_available() else 'cpu'
def precompute_real_features(dataloader: DataLoader, feature_extractor, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    é¢„è®¡ç®—æ•´ä¸ªçœŸå®æ•°æ®é›†çš„ç‰¹å¾ã€‚
    
    Args:
        dataloader: åŠ è½½çœŸå®åŸå§‹æ•°æ®ï¼ˆé¢‘è°±å›¾ã€æ–‡æœ¬IDï¼‰çš„DataLoaderã€‚
        feature_extractor: ç‰¹å¾æå–æ¨¡å‹ (ä¾‹å¦‚ ImageBindExtractor æˆ– ConvNetGRU)ã€‚
        device: 'cuda' æˆ– 'cpu'ã€‚

    Returns:
        åŒ…å«æ‰€æœ‰éŸ³é¢‘å’Œæ–‡æœ¬ç‰¹å¾çš„ä¸¤ä¸ªå¤§å‹å¼ é‡ã€‚
    """
    feature_extractor.to(device)
    feature_extractor.eval()
    
    all_audio_features = []
    all_text_features = []
    
    print(f"ğŸš€ å¼€å§‹é¢„è®¡ç®—çœŸå®ç‰¹å¾ï¼Œä½¿ç”¨æ¨¡å‹: {feature_extractor.__class__.__name__}")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æ­£åœ¨æå–ç‰¹å¾"):
            audio_raw = batch['audio'].to(device)
            text_raw = batch['text'].to(device)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„ç‰¹å¾æå–æ–¹æ³•
            if isinstance(feature_extractor, ImageBindExtractor):
                # ImageBindExtractor éœ€è¦ç‰¹æ®Šå¤„ç†
                # æ³¨æ„ï¼šè¿™é‡Œçš„ extract_*_features éœ€è¦é€‚é…ä¸ºæ¥æ”¶æ‰¹æ¬¡æ•°æ®
                audio_feats = feature_extractor.extract_audio_features(audio_raw)
                text_feats = feature_extractor.extract_text_features(text_raw)
            else: # é€‚ç”¨äº ConvNetGRU ç­‰åŒå¡”æ¨¡å‹
                audio_feats = feature_extractor.forward_audio(audio_raw)
                text_feats = feature_extractor.forward_text(text_raw)

            all_audio_features.append(audio_feats.cpu())
            all_text_features.append(text_feats.cpu())
            
    print("âœ… çœŸå®ç‰¹å¾é¢„è®¡ç®—å®Œæˆï¼")
    return torch.cat(all_audio_features, dim=0).to(device), torch.cat(all_text_features, dim=0).to(device)
# ===================== é˜¶æ®µé›¶ï¼šæ•°æ®é¢„å¤„ç† =====================

class ClothoPreprocessor:
    """Clothoæ•°æ®é›†é¢„å¤„ç†"""
    
    def __init__(self, data_dir: str, output_dir: str):
        self.data_dir = data_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    def preprocess_audio(self, audio_path: str) -> torch.Tensor:
        """å°†éŸ³é¢‘è½¬æ¢ä¸ºå¯¹æ•°æ¢…å°”é¢‘è°±å›¾"""
        y, sr = librosa.load(audio_path, sr=22050)
        mel_spec = librosa.feature.melspectrogram(
            y=y, sr=sr, n_mels=64, hop_length=512, n_fft=1024
        )
        log_mel = librosa.power_to_db(mel_spec)
        return torch.tensor(log_mel, dtype=torch.float32).unsqueeze(0)
    
    def preprocess_text(self, captions: List[str]) -> Dict:
        """æ–‡æœ¬é¢„å¤„ç†å’Œè¯æ±‡è¡¨æ„å»º"""
        # ç®€å•åˆ†è¯å’Œå¤„ç†
        vocab = {}
        processed_captions = []
        
        for caption in captions:
            words = caption.lower().split()
            word_ids = []
            for word in words:
                if word not in vocab:
                    vocab[word] = len(vocab)
                word_ids.append(vocab[word])
            processed_captions.append(word_ids)
        
        return {
            'vocab': vocab,
            'captions': processed_captions,
            'vocab_size': len(vocab)
        }

class ClothoDataset(Dataset):
    """Clothoæ•°æ®é›†"""
    
    def __init__(self, audio_features: torch.Tensor, text_features: torch.Tensor):
        self.audio = audio_features
        self.text = text_features
        
    def __len__(self):
        return len(self.audio)
    
    def __getitem__(self, idx):
        return {
            'audio': self.audio[idx],
            'text': self.text[idx],
            'label': idx
        }

# ===================== ç‰¹å¾æå–å™¨ =====================

class ConvNetGRU(nn.Module):
    """ConvNet+GRUåŒå¡”æ¨¡å‹"""
    
    def __init__(self, input_dim=64, feature_dim=512):
        super().__init__()
        
        # éŸ³é¢‘å¡”
        self.audio_tower = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, feature_dim)
        )
        
        # æ–‡æœ¬å¡”
        self.text_tower = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.GRU(256, 256, batch_first=True),
            nn.Linear(256, feature_dim)
        )
    
    def forward(self, x):
        """ç®€åŒ–forwardå‡½æ•°ç”¨äºç‰¹å¾æå–"""
        return x  # ç›´æ¥è¿”å›è¾“å…¥ï¼Œå› ä¸ºè¿™é‡Œåªæ˜¯ç‰¹å¾è½¬æ¢
    
    def forward_audio(self, audio):
        return F.normalize(self.audio_tower(audio), dim=-1)
    
    def forward_text(self, text):
        # å¤„ç†æ–‡æœ¬åºåˆ—
        if len(text.shape) == 2:
            text = text.unsqueeze(1)
        
        # GRUè¾“å‡º
        output, _ = self.text_tower[2](text)
        text_feat = self.text_tower[-1](output[:, -1, :])
        return F.normalize(text_feat, dim=-1)

class ImageBindExtractor:
    """ImageBindç‰¹å¾æå–å™¨"""
    
    def __init__(self, feature_dim=1024):
        self.feature_dim = feature_dim
        
    def extract_features(self, data, data_type='audio'):
        """æå–ImageBindç‰¹å¾"""
        batch_size = data.size(0)
        # æ¨¡æ‹ŸImageBindç‰¹å¾æå–
        features = torch.randn(batch_size, self.feature_dim, device=data.device)
        return F.normalize(features, dim=-1)

# ===================== æ ‡å‡†Herdingç®—æ³• =====================

class StandardHerdingSelector:
    """æ ‡å‡†Herdingç®—æ³•çš„è§„èŒƒå®ç°"""

    def __init__(self, device='cuda'):
        self.device = device

    def select_coreset(self, features, coreset_size):
        if coreset_size >= features.shape[0]:
            return torch.arange(features.shape[0]), features

        # 1. è®¡ç®—ä¸€æ¬¡å…¨ä½“ç‰¹å¾çš„å‡å€¼
        overall_mean = features.mean(dim=0)

        selected_indices = []
        available_indices = list(range(features.shape[0]))

        # 2. åˆå§‹åŒ–å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
        current_coreset_mean = torch.zeros_like(overall_mean)

        for i in range(coreset_size):
            # 3. è®¡ç®—å½“å‰å‡å€¼ä¸ç›®æ ‡å‡å€¼çš„â€œè¯¯å·®å‘é‡â€
            error_vector = overall_mean - current_coreset_mean

            # 4. åœ¨å‰©ä½™æ ·æœ¬ä¸­ï¼Œå¯»æ‰¾ä¸è¯¯å·®å‘é‡å†…ç§¯æœ€å¤§çš„é‚£ä¸ª
            available_features = features[available_indices]
            projections = torch.matmul(available_features, error_vector)

            best_idx_in_remaining = torch.argmax(projections)
            selected_idx = available_indices.pop(best_idx_in_remaining)

            selected_indices.append(selected_idx)

            # 5. æ›´æ–°å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
            current_coreset_mean = (current_coreset_mean * i + features[selected_idx]) / (i + 1)

        selected_indices = torch.tensor(selected_indices)
        return selected_indices, features[selected_indices]

# ===================== æŸå¤±å‡½æ•° =====================

class AVDDLoss:
    """AVDDæŸå¤±å‡½æ•°"""
    
    def __init__(self, lam_icm=10.0, lam_cgm=10.0):
        self.lam_icm = lam_icm
        self.lam_cgm = lam_cgm
    
    def __call__(self, real_features, synthetic_features, feature_extractor=None):
        """è®¡ç®—AVDDæŸå¤±"""
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features
        
        # 1. åˆ†å¸ƒåŒ¹é…æŸå¤± (DM Loss)
        loss_dm = F.mse_loss(syn_audio.mean(0), real_audio.mean(0)) + \
                 F.mse_loss(syn_text.mean(0), real_text.mean(0))
        
        # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤± (ICM Loss) - ä¿®æ­£ååŒ¹é…ç»Ÿè®¡é‡
        # å‡è®¾è¾“å…¥çš„ç‰¹å¾å·²ç»è¢«å½’ä¸€åŒ–
        real_sim_matrix = torch.matmul(real_audio, real_text.t())
        syn_sim_matrix = torch.matmul(syn_audio, syn_text.t())
        
        loss_icm = F.mse_loss(syn_sim_matrix.mean(), real_sim_matrix.mean()) + \
                   F.mse_loss(syn_sim_matrix.std(), real_sim_matrix.std())
        
        # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤± (CGM Loss)
        real_gap = real_audio.mean(0) - real_text.mean(0)
        syn_gap = syn_audio.mean(0) - syn_text.mean(0)
        loss_cgm = F.mse_loss(syn_gap, real_gap)
        
        return loss_dm + self.lam_icm * loss_icm + self.lam_cgm * loss_cgm
        
class ImageBindDCLoss:
    """
    ImageBindDCæŸå¤±å‡½æ•° - ä¿®æ­£åé‡‡ç”¨çœŸå®çš„CFDå®ç°ã€‚
    """
    def __init__(self, lam_cross=1.0, lam_joint=1.0, num_freqs=4096, device='cuda'):
        self.lam_cross = lam_cross
        self.lam_joint = lam_joint
        self.num_freqs = num_freqs
        self.device = device
        self._t_cache = {} # ç”¨äºç¼“å­˜ä¸åŒç»´åº¦çš„éšæœºé¢‘ç‡å‘é‡t

    def _get_t(self, feature_dim):
        """æ ¹æ®ç‰¹å¾ç»´åº¦è·å–æˆ–ç”Ÿæˆéšæœºé¢‘ç‡å‘é‡t"""
        if feature_dim not in self._t_cache:
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºé¢‘ç‡
            self._t_cache[feature_dim] = torch.randn(feature_dim, self.num_freqs, device=self.device)
        return self._t_cache[feature_dim]

    def compute_cfd(self, feat1: torch.Tensor, feat2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç‰¹å¾å‡½æ•°è·ç¦» (CFD)ã€‚
        å…¬å¼: L2^2(Î¦_x(t), Î¦_y(t))
        """
        feature_dim = feat1.shape[1]
        t = self._get_t(feature_dim) # è·å–éšæœºé¢‘ç‡å‘é‡

        # 1. å°†ç‰¹å¾æŠ•å½±åˆ°éšæœºé¢‘ç‡ä¸Š (t^T * x)
        proj1 = torch.matmul(feat1, t)
        proj2 = torch.matmul(feat2, t)

        # 2. è®¡ç®—ç‰¹å¾å‡½æ•° (é€šè¿‡é‡‡æ ·è¿‘ä¼¼æœŸæœ› E[exp(j * proj)])
        # exp(j*z) = cos(z) + j*sin(z)
        # E[cos(z)] å’Œ E[sin(z)] é€šè¿‡åœ¨batchç»´åº¦ä¸Šæ±‚å‡å€¼æ¥è¿‘ä¼¼
        phi1_real = torch.cos(proj1).mean(dim=0)
        phi1_imag = torch.sin(proj1).mean(dim=0)
        phi2_real = torch.cos(proj2).mean(dim=0)
        phi2_imag = torch.sin(proj2).mean(dim=0)

        # 3. è®¡ç®—ä¸¤ä¸ªå¤æ•°ç‰¹å¾å‡½æ•°ä¹‹é—´çš„L2è·ç¦»çš„å¹³æ–¹
        dist_sq = (phi1_real - phi2_real)**2 + (phi1_imag - phi2_imag)**2
        
        # 4. åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„CFDæŸå¤±
        return dist_sq.sum()

    def __call__(self, real_features, synthetic_features, feature_extractor=None):
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features

        # ç”±äºçœŸå®æ•°æ®é‡è¿œå¤§äºåˆæˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­é‡‡æ ·ä¸€ä¸ªå­é›†æ¥è¿›è¡ŒåŒ¹é…
        num_syn = len(syn_audio)
        rand_idx = torch.randperm(len(real_audio))[:num_syn]
        real_audio_subset = real_audio[rand_idx]
        real_text_subset = real_text[rand_idx]

        # 1. å•æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Uni-modal Distribution Matching)
        loss_uni = self.compute_cfd(real_audio_subset, syn_audio) + \
                   self.compute_cfd(real_text_subset, syn_text)
        
        # 2. è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Cross-modal Distribution Matching)
        # éµå¾ªè®ºæ–‡å…¬å¼ CFD(Real_Audio + Syn_Text, Real_Text + Syn_Audio)
        mix_dist1 = real_audio_subset + syn_text
        mix_dist2 = real_text_subset + syn_audio
        loss_cross = self.compute_cfd(mix_dist1, mix_dist2)
        
        # 3. è”åˆåˆ†å¸ƒåŒ¹é… (Joint Distribution Matching)
        # æ‹¼æ¥ç‰¹å¾ï¼ŒåŒ¹é…å…¶è”åˆåˆ†å¸ƒ
        joint_real = torch.cat([real_audio_subset, real_text_subset], dim=1)
        joint_syn = torch.cat([syn_audio, syn_text], dim=1)
        loss_joint = self.compute_cfd(joint_real, joint_syn)
        
        return loss_uni + self.lam_cross * loss_cross + self.lam_joint * loss_joint
# ===================== æ•°æ®è’¸é¦ =====================

class DataDistillation:
    """æ•°æ®è’¸é¦å¼•æ“"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = StandardHerdingSelector(device)
    
    def distill(self, real_audio_features, real_text_features, 
                method='avdd', feature_space='imagebind', 
                num_syn_samples=20, iterations=200):
        """æ ‡å‡†è’¸é¦æµç¨‹"""
        
        print(f"\nğŸ”¬ å¼€å§‹{method} on {feature_space}è’¸é¦...")
        
        # ä½¿ç”¨Herdingé€‰æ‹©æ ¸å¿ƒé›†
        audio_indices, audio_coreset = self.herding.select_coreset(real_audio_features, num_syn_samples)
        text_indices, text_coreset = self.herding.select_coreset(real_text_features, num_syn_samples)
        
        # åŸºäºæ ¸å¿ƒé›†åˆå§‹åŒ–åˆæˆæ•°æ®
        syn_audio = audio_coreset.clone().detach().requires_grad_(True)
        syn_text = text_coreset.clone().detach().requires_grad_(True)
        
        # é€‰æ‹©æŸå¤±å‡½æ•°
        if method == 'avdd':
            loss_fn = AVDDLoss()
        else:  # imagebind_dc
            loss_fn = ImageBindDCLoss(device=self.device)
        
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=0.01)
        
        loss_history = []
        
        # è’¸é¦å¾ªç¯
        feature_extractor = None
        if feature_space == 'convnet':
            feature_extractor = ConvNetGRU(feature_dim=512)
        
        for iteration in range(iterations):
            loss = loss_fn(
                (real_audio_features, real_text_features),
                (syn_audio, syn_text),
                feature_extractor
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_history.append(loss.item())
            
            if iteration % 50 == 0:
                print(f"è¿­ä»£ {iteration}/{iterations}, æŸå¤±: {loss:.6f}")
        
        return {
            'synthetic_audio': syn_audio.detach(),
            'synthetic_text': syn_text.detach(),
            'loss_history': loss_history
        }

# ===================== æ£€ç´¢æ¨¡å‹è®­ç»ƒ =====================

class RetrievalModel(nn.Module):
    """æ£€ç´¢æ¨¡å‹"""
    
    def __init__(self, input_dim, feature_dim=512):
        super().__init__()
        
        self.audio_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim)
        )
        
        self.text_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim)
        )
    
    def forward(self, audio, text):
        audio_feat = F.normalize(self.audio_encoder(audio), dim=-1)
        text_feat = F.normalize(self.text_encoder(text), dim=-1)
        return audio_feat, text_feat

class RetrievalTrainer:
    """æ£€ç´¢æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def train(self, synthetic_dataset, feature_dim=512, epochs=50):
        """è®­ç»ƒæ£€ç´¢æ¨¡å‹"""
        
        print("\nğŸ¯ è®­ç»ƒæ£€ç´¢æ¨¡å‹...")
        
        dataloader = DataLoader(synthetic_dataset, batch_size=8, shuffle=True)
        
        input_dim = synthetic_dataset[0]['audio'].shape[0]
        model = RetrievalModel(input_dim, feature_dim).to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        def info_nce_loss(audio_feat, text_feat, temperature=0.07):
            logits = torch.matmul(audio_feat, text_feat.t()) / temperature
            labels = torch.arange(len(audio_feat), device=self.device)
            
            loss_a2t = F.cross_entropy(logits, labels)
            loss_t2a = F.cross_entropy(logits.t(), labels)
            
            return (loss_a2t + loss_t2a) / 2
        
        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in dataloader:
                audio = batch['audio'].to(self.device)
                text = batch['text'].to(self.device)
                
                audio_feat, text_feat = model(audio, text)
                loss = info_nce_loss(audio_feat, text_feat)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                print(f"è®­ç»ƒè½®æ¬¡ {epoch}/{epochs}, å¹³å‡æŸå¤±: {total_loss/len(dataloader):.4f}")
        
        return model

# ===================== æ ‡å‡†è¯„ä¼° =====================

class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def evaluate(self, model, real_audio_features, real_text_features):
        """æ ‡å‡†Recall@Kè¯„ä¼°"""
        
        print("\nğŸ¯ æ ‡å‡†æ£€ç´¢è¯„ä¼°...")
        
        model.eval()
        with torch.no_grad():
            audio_feat, text_feat = model(real_audio_features, real_text_features)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            sim_matrix = torch.matmul(audio_feat, text_feat.t())
            
            results = {}
            
            # Audio-to-Textæ£€ç´¢
            _, top_indices_a2t = torch.topk(sim_matrix, k=10, dim=1)
            
            # Text-to-Audioæ£€ç´¢
            _, top_indices_t2a = torch.topk(sim_matrix.t(), k=10, dim=1)
            
            for k in [1, 5, 10]:
                # A2Tå¬å›ç‡
                correct_a2t = 0
                for i in range(len(real_audio_features)):
                    if i in top_indices_a2t[i, :k]:
                        correct_a2t += 1
                recall_a2t = correct_a2t / len(real_audio_features)
                
                # T2Aå¬å›ç‡
                correct_t2a = 0
                for i in range(len(real_text_features)):
                    if i in top_indices_t2a[i, :k]:
                        correct_t2a += 1
                recall_t2a = correct_t2a / len(real_text_features)
                
                results[f'R@{k}_a2t'] = recall_a2t
                results[f'R@{k}_t2a'] = recall_t2a
        
        return results

# ===================== ä¸»å®éªŒæµç¨‹ =====================

class ClothoExperiment:
    """Clothoæ•°æ®é›†å®Œæ•´å®éªŒ"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.distiller = DataDistillation(device)
        self.trainer = RetrievalTrainer(device)
        self.evaluator = StandardEvaluator(device)
    
    def run_complete_experiment(self, num_real_samples=2893, num_syn_samples=20):
        """æ‰§è¡Œå®Œæ•´å››é˜¶æ®µå®éªŒ"""
        
        print("ğŸ† Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")
        print("=" * 80)
        
        # ===================== é˜¶æ®µé›¶ï¼šå‡†å¤‡çœŸå®æ•°æ®å’Œç‰¹å¾ =====================
        print("ğŸ“Š é˜¶æ®µé›¶ï¼šåŠ è½½çœŸå®æ•°æ®å¹¶é¢„è®¡ç®—ç‰¹å¾...")

        # 1. åˆ›å»ºåŠ è½½çœŸå®åŸå§‹æ•°æ®çš„ DataLoader
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åŠ è½½ num_real_samples ä¸ªæ ·æœ¬ç”¨äºè’¸é¦
        real_dataloader = get_high_performance_dataloader(
            audio_dir=self.audio_dir,
            captions_file=self.captions_file,
            batch_size=32, # å¯ä»¥æ ¹æ®æ˜¾å­˜è°ƒæ•´
            num_samples=num_real_samples,
            vocab_size=self.vocab_size
        )

        # 2. å‡†å¤‡ä¸¤ç§ä¸åŒçš„ç‰¹å¾æå–å™¨
        imagebind_extractor = ImageBindExtractor(pretrained=True, device=self.device)
        convnet_extractor = ConvNetGRU(feature_dim=512).to(self.device) # å‡è®¾è¾“å…¥ç»´åº¦å·²é€‚é…

        # 3. æ‰§è¡Œç‰¹å¾é¢„è®¡ç®—ï¼Œå¾—åˆ°å››ä¸ªçœŸå®çš„ç‰¹å¾åº“
        print("\n--- è®¡ç®—ImageBindç‰¹å¾ç©ºé—´ ---")
        real_imagebind_audio, real_imagebind_text = precompute_real_features(
            real_dataloader, imagebind_extractor, self.device
        )
        
        print("\n--- è®¡ç®—ConvNetç‰¹å¾ç©ºé—´ ---")
        real_convnet_audio, real_convnet_text = precompute_real_features(
            real_dataloader, convnet_extractor, self.device
        )
        
        print(f"\nImageBind ç‰¹å¾ç»´åº¦: Audio-{real_imagebind_audio.shape}, Text-{real_imagebind_text.shape}")
        print(f"ConvNet ç‰¹å¾ç»´åº¦: Audio-{real_convnet_audio.shape}, Text-{real_convnet_text.shape}")

        # ===================== é˜¶æ®µä¸€ï¼šå››ç§è’¸é¦æ–¹æ³• =====================
        print("\nğŸ”¬ é˜¶æ®µä¸€ï¼šæ‰§è¡Œå››ç§æ•°æ®è’¸é¦æ–¹æ³•...")
        
        # æ–¹æ³•ä¸€ï¼šAVDD on ConvNet
        result1 = self.distiller.distill(
            real_convnet_audio, real_convnet_text,
            method='avdd', feature_space='convnet',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•äºŒï¼šImageBindDC on ImageBind
        result2 = self.distiller.distill(
            real_imagebind_audio, real_imagebind_text,
            method='imagebind_dc', feature_space='imagebind',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•ä¸‰ï¼šAVDD on ImageBind
        result3 = self.distiller.distill(
            real_imagebind_audio, real_imagebind_text,
            method='avdd', feature_space='imagebind',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•å››ï¼šImageBindDC on ConvNet
        result4 = self.distiller.distill(
            real_convnet_audio, real_convnet_text,
            method='imagebind_dc', feature_space='convnet',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # ===================== é˜¶æ®µäºŒ & ä¸‰ï¼šè®­ç»ƒå¹¶è¯„ä¼°æ£€ç´¢æ¨¡å‹ =====================
        print("\nğŸ”¬ é˜¶æ®µäºŒ & ä¸‰ï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå¹¶è¯„ä¼°æ£€ç´¢æ¨¡å‹...")
        
        distilled_datasets = [
            ("AVDD_ConvNet", result1, (real_convnet_audio, real_convnet_text)),
            ("ImageBindDC_ImageBind", result2, (real_imagebind_audio, real_imagebind_text)),
            ("AVDD_ImageBind", result3, (real_imagebind_audio, real_imagebind_text)),
            ("ImageBindDC_ConvNet", result4, (real_convnet_audio, real_convnet_text))
        ]
        
        final_results = {}
        
        for name, synthetic_data, real_eval_features in distilled_datasets:
            print(f"\n--- æ­£åœ¨å¤„ç†æ–¹æ³•: {name} ---")
            
            # å‡†å¤‡åˆæˆæ•°æ®é›†
            syn_dataset = SyntheticDataset(
                synthetic_data['synthetic_audio'], 
                synthetic_data['synthetic_text']
            )

            # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
            # æ³¨æ„ï¼šå­¦ç”Ÿæ¨¡å‹è¾“å…¥ç»´åº¦åº”ä¸åˆæˆç‰¹å¾ç»´åº¦åŒ¹é…
            input_dim = synthetic_data['synthetic_audio'].shape[1]
            feature_dim = 512 # å‡è®¾æˆ‘ä»¬ç»Ÿä¸€è®­ç»ƒä¸€ä¸ª512ç»´çš„å­¦ç”Ÿæ¨¡å‹
            trained_model = self.trainer.train(syn_dataset, input_dim=input_dim, feature_dim=feature_dim, epochs=100)
            
            # åœ¨çœŸå®çš„è¯„ä¼°æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°
            # æ³¨æ„ï¼šè¯„ä¼°æ—¶ï¼Œå­¦ç”Ÿæ¨¡å‹éœ€è¦å¤„ç†ä¸å®ƒè®­ç»ƒæ—¶ç›¸åŒç‰¹å¾ç©ºé—´çš„æ•°æ®
            eval_audio_features, eval_text_features = real_eval_features
            
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€éƒ¨åˆ†çœŸå®ç‰¹å¾ä½œä¸ºè¯„ä¼°é›†
            eval_subset_size = 500
            eval_results = self.evaluator.evaluate(
                trained_model, 
                eval_audio_features[:eval_subset_size], 
                eval_text_features[:eval_subset_size]
            )
            final_results[name] = eval_results
        
        # é˜¶æ®µå››ï¼šç»“æœæ±‡æ€»
        print("\n" + "=" * 80)
        print("ğŸ“Š æœ€ç»ˆ2x2å¯¹æ¯”çŸ©é˜µ")
        print("=" * 80)
        
        print(f"{'æ–¹æ³•':<25} {'Recall@1_A2T':<12} {'Recall@1_T2A':<12} {'Recall@5_A2T':<12} {'Recall@5_T2A':<12}")
        print("-" * 80)
        
        for name, result in results.items():
            print(f"{name:<25} {result['R@1_a2t']:<12.4f} {result['R@1_t2a']:<12.4f} "
                  f"{result['R@5_a2t']:<12.4f} {result['R@5_t2a']:<12.4f}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        final_report = {
            'num_real_samples': num_real_samples,
            'num_syn_samples': num_syn_samples,
            'results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        torch.save(final_report, f'clotho_experiment_{num_syn_samples}s_results.pth')
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: clotho_experiment_{num_syn_samples}s_results.pth")
        
        return results

# ===================== ä¸»å…¥å£ =====================

if __name__ == "__main__":
    print("ğŸš€ Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒå¯åŠ¨")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = ClothoExperiment()
    
    # è¿è¡Œå®Œæ•´å®éªŒ
    results = experiment.run_complete_experiment(
        num_real_samples=100,  # ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨100æ ·æœ¬
        num_syn_samples=20
    )
    
    print("\nâœ… å®éªŒå®Œæˆï¼")