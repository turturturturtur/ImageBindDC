#!/usr/bin/env python3
"""
Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ
éµå¾ªæŠ€æœ¯è§„æ ¼ä¹¦çš„å®Œæ•´å››é˜¶æ®µå®ç°
"""

import os
import json
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from typing import Dict, Tuple, List

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# æ·»åŠ è¿™ä¸ªç¼ºå¤±çš„ç±»
class SyntheticDataset(Dataset):
    """ç”¨äºå·²è’¸é¦çš„åˆæˆç‰¹å¾çš„æ•°æ®é›†"""
    def __init__(self, synthetic_audio: torch.Tensor, synthetic_text: torch.Tensor):
        self.audio = synthetic_audio
        self.text = synthetic_text

    def __len__(self):
        return len(self.audio)

    def __getitem__(self, idx):
        return {
            'audio': self.audio[idx],
            'text': self.text[idx],
            'label': idx
        }

# æ·»åŠ è¿™ä¸ªç¼ºå¤±çš„å‡½æ•° (å®ƒä¾èµ–äºä¸€ä¸ªä¿®æ”¹ç‰ˆçš„RawClothoDatasetï¼Œè¿™é‡Œä¸€å¹¶æä¾›)
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
from tqdm import tqdm

class RawClothoDataset(Dataset):
    def __init__(self, audio_dir: str, captions_df: pd.DataFrame, vocab: Dict, sr=22050, n_mels=64):
        self.audio_dir = audio_dir
        self.captions_df = captions_df
        self.vocab = vocab
        self.sr = sr
        self.n_mels = n_mels

    def __len__(self):
        return len(self.captions_df)

    def __getitem__(self, idx):
        row = self.captions_df.iloc[idx]
        audio_path = os.path.join(self.audio_dir, row['file_name'])
        y, _ = librosa.load(audio_path, sr=self.sr)
        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)
        audio_tensor = torch.tensor(librosa.power_to_db(mel_spec), dtype=torch.float32).unsqueeze(0)

        caption = row['caption_1']
        words = caption.lower().split()
        word_ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]
        text_tensor = torch.tensor(word_ids, dtype=torch.long)

        return {'audio': audio_tensor, 'text': text_tensor}

def build_vocab(captions: List[str], min_freq=5) -> Dict:
    word_counts = {}
    for caption in captions:
        for word in caption.lower().split():
            word_counts[word] = word_counts.get(word, 0) + 1

    vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items()) if count >= min_freq}
    vocab['<pad>'] = 0
    vocab['<unk>'] = 1
    return vocab

def collate_fn(batch):
    audios = [item['audio'] for item in batch]
    texts = [item['text'] for item in batch]
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)
    audios_stacked = torch.stack(audios)
    return {'audio': audios_stacked, 'text': texts_padded}

def get_high_performance_dataloader(audio_dir: str, captions_file: str, batch_size: int, num_samples: int) -> Tuple[DataLoader, Dict]:
    df = pd.read_csv(captions_file)
    if num_samples:
        df = df.sample(n=num_samples, random_state=42).reset_index(drop=True)

    vocab = build_vocab(df['caption_1'].tolist())
    dataset = RawClothoDataset(audio_dir, df, vocab)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)
    return dataloader, vocab

def precompute_real_features(dataloader: DataLoader, feature_extractor, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    é¢„è®¡ç®—æ•´ä¸ªçœŸå®æ•°æ®é›†çš„ç‰¹å¾ã€‚
    
    Args:
        dataloader: åŠ è½½çœŸå®åŸå§‹æ•°æ®ï¼ˆé¢‘è°±å›¾ã€æ–‡æœ¬IDï¼‰çš„DataLoaderã€‚
        feature_extractor: ç‰¹å¾æå–æ¨¡å‹ (ä¾‹å¦‚ ImageBindExtractor æˆ– ConvNetGRU)ã€‚
        device: 'cuda' æˆ– 'cpu'ã€‚

    Returns:
        åŒ…å«æ‰€æœ‰éŸ³é¢‘å’Œæ–‡æœ¬ç‰¹å¾çš„ä¸¤ä¸ªå¤§å‹å¼ é‡ã€‚
    """
    feature_extractor.to(device)
    feature_extractor.eval()
    
    all_audio_features = []
    all_text_features = []
    
    print(f"ğŸš€ å¼€å§‹é¢„è®¡ç®—çœŸå®ç‰¹å¾ï¼Œä½¿ç”¨æ¨¡å‹: {feature_extractor.__class__.__name__}")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æ­£åœ¨æå–ç‰¹å¾"):
            audio_raw = batch['audio'].to(device)
            text_raw = batch['text'].to(device)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„ç‰¹å¾æå–æ–¹æ³•
            if isinstance(feature_extractor, ImageBindExtractor):
                # ImageBindExtractor éœ€è¦ç‰¹æ®Šå¤„ç†
                # æ³¨æ„ï¼šè¿™é‡Œçš„ extract_*_features éœ€è¦é€‚é…ä¸ºæ¥æ”¶æ‰¹æ¬¡æ•°æ®
                audio_feats = feature_extractor.extract_audio_features(audio_raw)
                text_feats = feature_extractor.extract_text_features(text_raw)
            else: # é€‚ç”¨äº ConvNetGRU ç­‰åŒå¡”æ¨¡å‹
                audio_feats = feature_extractor.forward_audio(audio_raw)
                text_feats = feature_extractor.forward_text(text_raw)

            all_audio_features.append(audio_feats.cpu())
            all_text_features.append(text_feats.cpu())
            
    print("âœ… çœŸå®ç‰¹å¾é¢„è®¡ç®—å®Œæˆï¼")
    return torch.cat(all_audio_features, dim=0).to(device), torch.cat(all_text_features, dim=0).to(device)



# ===================== ç‰¹å¾æå–å™¨ =====================

class ConvNetGRU(nn.Module):
    """ConvNet+GRUåŒå¡”æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 256, feature_dim: int = 512):
        super().__init__()
        
        # éŸ³é¢‘å¡” (è¿™éƒ¨åˆ†ä¿æŒä¸å˜)
        self.audio_tower = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, feature_dim)
        )
        
        # æ–‡æœ¬å¡” (é‡æ„ä¸ºç‹¬ç«‹çš„å±‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªSequential)
        self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.text_gru = nn.GRU(embedding_dim, feature_dim, batch_first=True)
    
    def forward(self, x):
        """ç®€åŒ–forwardå‡½æ•°ç”¨äºç‰¹å¾æå–"""
        return x  # ç›´æ¥è¿”å›è¾“å…¥ï¼Œå› ä¸ºè¿™é‡Œåªæ˜¯ç‰¹å¾è½¬æ¢
    
    def forward_audio(self, audio):
        return F.normalize(self.audio_tower(audio), dim=-1)
    
    def forward_text(self, text: torch.Tensor) -> torch.Tensor:
        # 1. å°†è¾“å…¥çš„è¯æ±‡IDåºåˆ—é€šè¿‡Embeddingå±‚è½¬æ¢ä¸ºç‰¹å¾å‘é‡åºåˆ—
        embedded_text = self.text_embedding(text)
        
        # 2. å°†ç‰¹å¾å‘é‡åºåˆ—è¾“å…¥GRU
        # GRUçš„ç¬¬äºŒä¸ªè¿”å›å€¼h_næ˜¯æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(num_layers, batch_size, hidden_size)
        _, h_n = self.text_gru(embedded_text)
        
        # 3. h_n.squeeze(0)å°†å½¢çŠ¶å˜ä¸º(batch_size, hidden_size)ï¼Œè¿™é€šå¸¸è¢«ç”¨ä½œæ•´ä¸ªåºåˆ—çš„ç‰¹å¾è¡¨ç¤º
        text_feat = h_n.squeeze(0)
        
        # 4. å½’ä¸€åŒ–ç‰¹å¾
        return F.normalize(text_feat, dim=-1)

# æ›¿æ¢æ—§çš„ ImageBindExtractor ç±»
class ImageBindExtractor:
    """ImageBindç‰¹å¾æå–å™¨ (æ¨¡æ‹Ÿ)"""
    def __init__(self, feature_dim=1024, pretrained=False, device='cpu'): # å¢åŠ å‚æ•°ä»¥åŒ¹é…è°ƒç”¨
        self.feature_dim = feature_dim
        self.device = device
        if pretrained:
            print("æ¨¡æ‹ŸåŠ è½½é¢„è®­ç»ƒçš„ ImageBind æ¨¡å‹...")

    def extract_audio_features(self, audio_data: torch.Tensor) -> torch.Tensor:
        batch_size = audio_data.size(0)
        features = torch.randn(batch_size, self.feature_dim, device=self.device)
        return F.normalize(features, dim=-1)

    def extract_text_features(self, text_data: torch.Tensor) -> torch.Tensor:
        batch_size = text_data.size(0)
        features = torch.randn(batch_size, self.feature_dim, device=self.device)
        return F.normalize(features, dim=-1)

# ===================== æ ‡å‡†Herdingç®—æ³• =====================

class StandardHerdingSelector:
    """æ ‡å‡†Herdingç®—æ³•çš„è§„èŒƒå®ç°"""

    def __init__(self, device='cuda'):
        self.device = device

    def select_coreset(self, features, coreset_size):
        if coreset_size >= features.shape[0]:
            return torch.arange(features.shape[0]), features

        # 1. è®¡ç®—ä¸€æ¬¡å…¨ä½“ç‰¹å¾çš„å‡å€¼
        overall_mean = features.mean(dim=0)

        selected_indices = []
        available_indices = list(range(features.shape[0]))

        # 2. åˆå§‹åŒ–å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
        current_coreset_mean = torch.zeros_like(overall_mean)

        for i in range(coreset_size):
            # 3. è®¡ç®—å½“å‰å‡å€¼ä¸ç›®æ ‡å‡å€¼çš„â€œè¯¯å·®å‘é‡â€
            error_vector = overall_mean - current_coreset_mean

            # 4. åœ¨å‰©ä½™æ ·æœ¬ä¸­ï¼Œå¯»æ‰¾ä¸è¯¯å·®å‘é‡å†…ç§¯æœ€å¤§çš„é‚£ä¸ª
            available_features = features[available_indices]
            projections = torch.matmul(available_features, error_vector)

            best_idx_in_remaining = torch.argmax(projections)
            selected_idx = available_indices.pop(best_idx_in_remaining)

            selected_indices.append(selected_idx)

            # 5. æ›´æ–°å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
            current_coreset_mean = (current_coreset_mean * i + features[selected_idx]) / (i + 1)

        selected_indices = torch.tensor(selected_indices)
        return selected_indices, features[selected_indices]

# ===================== æŸå¤±å‡½æ•° =====================

class AVDDLoss:
    """AVDDæŸå¤±å‡½æ•°"""
    
    def __init__(self, lam_icm=10.0, lam_cgm=10.0):
        self.lam_icm = lam_icm
        self.lam_cgm = lam_cgm
    
    def __call__(self, real_features, synthetic_features):
        """è®¡ç®—AVDDæŸå¤±"""
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features
        
        # 1. åˆ†å¸ƒåŒ¹é…æŸå¤± (DM Loss)
        loss_dm = F.mse_loss(syn_audio.mean(0), real_audio.mean(0)) + \
                 F.mse_loss(syn_text.mean(0), real_text.mean(0))
        
        # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤± (ICM Loss) - ä¿®æ­£ååŒ¹é…ç»Ÿè®¡é‡
        # å‡è®¾è¾“å…¥çš„ç‰¹å¾å·²ç»è¢«å½’ä¸€åŒ–
        real_sim_matrix = torch.matmul(real_audio, real_text.t())
        syn_sim_matrix = torch.matmul(syn_audio, syn_text.t())
        
        loss_icm = F.mse_loss(syn_sim_matrix.mean(), real_sim_matrix.mean()) + \
                   F.mse_loss(syn_sim_matrix.std(), real_sim_matrix.std())
        
        # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤± (CGM Loss)
        real_gap = real_audio.mean(0) - real_text.mean(0)
        syn_gap = syn_audio.mean(0) - syn_text.mean(0)
        loss_cgm = F.mse_loss(syn_gap, real_gap)
        
        return loss_dm + self.lam_icm * loss_icm + self.lam_cgm * loss_cgm
        
class ImageBindDCLoss:
    """
    ImageBindDCæŸå¤±å‡½æ•° - ä¿®æ­£åé‡‡ç”¨çœŸå®çš„CFDå®ç°ã€‚
    """
    def __init__(self, lam_cross=1.0, lam_joint=1.0, num_freqs=4096, device='cuda'):
        self.lam_cross = lam_cross
        self.lam_joint = lam_joint
        self.num_freqs = num_freqs
        self.device = device
        self._t_cache = {} # ç”¨äºç¼“å­˜ä¸åŒç»´åº¦çš„éšæœºé¢‘ç‡å‘é‡t

    def _get_t(self, feature_dim):
        """æ ¹æ®ç‰¹å¾ç»´åº¦è·å–æˆ–ç”Ÿæˆéšæœºé¢‘ç‡å‘é‡t"""
        if feature_dim not in self._t_cache:
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºé¢‘ç‡
            self._t_cache[feature_dim] = torch.randn(feature_dim, self.num_freqs, device=self.device)
        return self._t_cache[feature_dim]

    def compute_cfd(self, feat1: torch.Tensor, feat2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç‰¹å¾å‡½æ•°è·ç¦» (CFD)ã€‚
        å…¬å¼: L2^2(Î¦_x(t), Î¦_y(t))
        """
        feature_dim = feat1.shape[1]
        t = self._get_t(feature_dim) # è·å–éšæœºé¢‘ç‡å‘é‡

        # 1. å°†ç‰¹å¾æŠ•å½±åˆ°éšæœºé¢‘ç‡ä¸Š (t^T * x)
        proj1 = torch.matmul(feat1, t)
        proj2 = torch.matmul(feat2, t)

        # 2. è®¡ç®—ç‰¹å¾å‡½æ•° (é€šè¿‡é‡‡æ ·è¿‘ä¼¼æœŸæœ› E[exp(j * proj)])
        # exp(j*z) = cos(z) + j*sin(z)
        # E[cos(z)] å’Œ E[sin(z)] é€šè¿‡åœ¨batchç»´åº¦ä¸Šæ±‚å‡å€¼æ¥è¿‘ä¼¼
        phi1_real = torch.cos(proj1).mean(dim=0)
        phi1_imag = torch.sin(proj1).mean(dim=0)
        phi2_real = torch.cos(proj2).mean(dim=0)
        phi2_imag = torch.sin(proj2).mean(dim=0)

        # 3. è®¡ç®—ä¸¤ä¸ªå¤æ•°ç‰¹å¾å‡½æ•°ä¹‹é—´çš„L2è·ç¦»çš„å¹³æ–¹
        dist_sq = (phi1_real - phi2_real)**2 + (phi1_imag - phi2_imag)**2
        
        # 4. åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„CFDæŸå¤±
        return dist_sq.sum()

    def __call__(self, real_features, synthetic_features):
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features

        # ç”±äºçœŸå®æ•°æ®é‡è¿œå¤§äºåˆæˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­é‡‡æ ·ä¸€ä¸ªå­é›†æ¥è¿›è¡ŒåŒ¹é…
        num_syn = len(syn_audio)
        rand_idx = torch.randperm(len(real_audio))[:num_syn]
        real_audio_subset = real_audio[rand_idx]
        real_text_subset = real_text[rand_idx]

        # 1. å•æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Uni-modal Distribution Matching)
        loss_uni = self.compute_cfd(real_audio_subset, syn_audio) + \
                   self.compute_cfd(real_text_subset, syn_text)
        
        # 2. è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Cross-modal Distribution Matching)
        # éµå¾ªè®ºæ–‡å…¬å¼ CFD(Real_Audio + Syn_Text, Real_Text + Syn_Audio)
        mix_dist1 = real_audio_subset + syn_text
        mix_dist2 = real_text_subset + syn_audio
        loss_cross = self.compute_cfd(mix_dist1, mix_dist2)
        
        # 3. è”åˆåˆ†å¸ƒåŒ¹é… (Joint Distribution Matching)
        # æ‹¼æ¥ç‰¹å¾ï¼ŒåŒ¹é…å…¶è”åˆåˆ†å¸ƒ
        joint_real = torch.cat([real_audio_subset, real_text_subset], dim=1)
        joint_syn = torch.cat([syn_audio, syn_text], dim=1)
        loss_joint = self.compute_cfd(joint_real, joint_syn)
        
        return loss_uni + self.lam_cross * loss_cross + self.lam_joint * loss_joint
# ===================== æ•°æ®è’¸é¦ =====================

class DataDistillation:
    """æ•°æ®è’¸é¦å¼•æ“"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = StandardHerdingSelector(device)
    
    def distill(self, real_audio_features, real_text_features, 
                method='avdd', feature_space='imagebind', 
                num_syn_samples=20, iterations=200):
        """æ ‡å‡†è’¸é¦æµç¨‹"""
        
        print(f"\nğŸ”¬ å¼€å§‹{method} on {feature_space}è’¸é¦...")
        
        # ä½¿ç”¨Herdingé€‰æ‹©æ ¸å¿ƒé›†
        audio_indices, audio_coreset = self.herding.select_coreset(real_audio_features, num_syn_samples)
        text_indices, text_coreset = self.herding.select_coreset(real_text_features, num_syn_samples)
        
        # åŸºäºæ ¸å¿ƒé›†åˆå§‹åŒ–åˆæˆæ•°æ®
        syn_audio = audio_coreset.clone().detach().requires_grad_(True)
        syn_text = text_coreset.clone().detach().requires_grad_(True)
        
        # é€‰æ‹©æŸå¤±å‡½æ•°
        if method == 'avdd':
            loss_fn = AVDDLoss()
        else:  # imagebind_dc
            loss_fn = ImageBindDCLoss(device=self.device)
        
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=0.01)
        
        loss_history = []
        
        # ### MODIFICATION START ###
        # ç§»é™¤äº†æ— æ•ˆçš„ feature_extractor åˆ›å»ºå’Œä¼ é€’é€»è¾‘ï¼Œå› ä¸ºæŸå¤±å‡½æ•°æ˜¯åœ¨å›ºå®šçš„ç‰¹å¾ç©ºé—´ä¸Šè®¡ç®—çš„ã€‚
        for iteration in range(iterations):
            loss = loss_fn(
                (real_audio_features, real_text_features),
                (syn_audio, syn_text)
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_history.append(loss.item())
            
            if iteration % 50 == 0:
                print(f"è¿­ä»£ {iteration}/{iterations}, æŸå¤±: {loss:.6f}")
        # ### MODIFICATION END ###
        
        return {
            'synthetic_audio': syn_audio.detach(),
            'synthetic_text': syn_text.detach(),
            'loss_history': loss_history
        }

# ===================== æ£€ç´¢æ¨¡å‹è®­ç»ƒ =====================

class RetrievalModel(nn.Module):
    """æ£€ç´¢æ¨¡å‹"""
    
    def __init__(self, input_dim, feature_dim=512):
        super().__init__()
        
        self.audio_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim)
        )
        
        self.text_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim)
        )
    
    def forward(self, audio, text):
        audio_feat = F.normalize(self.audio_encoder(audio), dim=-1)
        text_feat = F.normalize(self.text_encoder(text), dim=-1)
        return audio_feat, text_feat

class RetrievalTrainer:
    """æ£€ç´¢æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def train(self, synthetic_dataset, feature_dim=512, epochs=50):
        """è®­ç»ƒæ£€ç´¢æ¨¡å‹"""
        
        print("\nğŸ¯ è®­ç»ƒæ£€ç´¢æ¨¡å‹...")
        
        dataloader = DataLoader(synthetic_dataset, batch_size=8, shuffle=True)
        
        input_dim = synthetic_dataset[0]['audio'].shape[0]
        model = RetrievalModel(input_dim, feature_dim).to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        def info_nce_loss(audio_feat, text_feat, temperature=0.07):
            logits = torch.matmul(audio_feat, text_feat.t()) / temperature
            labels = torch.arange(len(audio_feat), device=self.device)
            
            loss_a2t = F.cross_entropy(logits, labels)
            loss_t2a = F.cross_entropy(logits.t(), labels)
            
            return (loss_a2t + loss_t2a) / 2
        
        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in dataloader:
                audio = batch['audio'].to(self.device)
                text = batch['text'].to(self.device)
                
                audio_feat, text_feat = model(audio, text)
                loss = info_nce_loss(audio_feat, text_feat)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                print(f"è®­ç»ƒè½®æ¬¡ {epoch}/{epochs}, å¹³å‡æŸå¤±: {total_loss/len(dataloader):.4f}")
        
        return model

# ===================== æ ‡å‡†è¯„ä¼° =====================

class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def evaluate(self, model, real_audio_features, real_text_features):
        """æ ‡å‡†Recall@Kè¯„ä¼°"""
        
        print("\nğŸ¯ æ ‡å‡†æ£€ç´¢è¯„ä¼°...")
        
        model.eval()
        with torch.no_grad():
            audio_feat, text_feat = model(real_audio_features, real_text_features)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            sim_matrix = torch.matmul(audio_feat, text_feat.t())
            
            results = {}
            
            # Audio-to-Textæ£€ç´¢
            _, top_indices_a2t = torch.topk(sim_matrix, k=10, dim=1)
            
            # Text-to-Audioæ£€ç´¢
            _, top_indices_t2a = torch.topk(sim_matrix.t(), k=10, dim=1)
            
            for k in [1, 5, 10]:
                # A2Tå¬å›ç‡
                correct_a2t = 0
                for i in range(len(real_audio_features)):
                    if i in top_indices_a2t[i, :k]:
                        correct_a2t += 1
                recall_a2t = correct_a2t / len(real_audio_features)
                
                # T2Aå¬å›ç‡
                correct_t2a = 0
                for i in range(len(real_text_features)):
                    if i in top_indices_t2a[i, :k]:
                        correct_t2a += 1
                recall_t2a = correct_t2a / len(real_text_features)
                
                results[f'R@{k}_a2t'] = recall_a2t
                results[f'R@{k}_t2a'] = recall_t2a
        
        return results

# ===================== ä¸»å®éªŒæµç¨‹ =====================

class ClothoExperiment:
    """Clothoæ•°æ®é›†å®Œæ•´å®éªŒ"""
    
    def __init__(self, audio_dir: str, captions_file: str, device='cuda'):
        self.device = device
        self.audio_dir = audio_dir
        self.captions_file = captions_file
        self.vocab_size = 0 # å°†åœ¨è¿è¡Œæ—¶æ›´æ–°
        self.distiller = DataDistillation(device)
        self.trainer = RetrievalTrainer(device)
        self.evaluator = StandardEvaluator(device)
    
    def run_complete_experiment(self, num_real_samples=2893, num_syn_samples=20):
        """æ‰§è¡Œå®Œæ•´å››é˜¶æ®µå®éªŒ"""
        
        print("ğŸ† Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")
        print("=" * 80)
        
        # ===================== é˜¶æ®µé›¶ï¼šå‡†å¤‡çœŸå®æ•°æ®å’Œç‰¹å¾ =====================
        print("ğŸ“Š é˜¶æ®µé›¶ï¼šåŠ è½½çœŸå®æ•°æ®å¹¶é¢„è®¡ç®—ç‰¹å¾...")

        # 1. åˆ›å»ºåŠ è½½çœŸå®åŸå§‹æ•°æ®çš„ DataLoader
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åŠ è½½ num_real_samples ä¸ªæ ·æœ¬ç”¨äºè’¸é¦
        real_dataloader, vocab = get_high_performance_dataloader(
            audio_dir=self.audio_dir,
            captions_file=self.captions_file,
            batch_size=32, # å¯ä»¥æ ¹æ®æ˜¾å­˜è°ƒæ•´
            num_samples=num_real_samples
        )
        self.vocab_size = len(vocab) # æ›´æ–°è¯æ±‡è¡¨å¤§å°

        # 2. å‡†å¤‡ä¸¤ç§ä¸åŒçš„ç‰¹å¾æå–å™¨
        imagebind_extractor = ImageBindExtractor(pretrained=True, device=self.device)
        # ä½¿ç”¨æ­£ç¡®çš„æ„é€ å‡½æ•°åˆ›å»º ConvNetGRU
        convnet_extractor = ConvNetGRU(vocab_size=self.vocab_size, feature_dim=512).to(self.device)

        # 3. æ‰§è¡Œç‰¹å¾é¢„è®¡ç®—ï¼Œå¾—åˆ°å››ä¸ªçœŸå®çš„ç‰¹å¾åº“
        print("\n--- è®¡ç®—ImageBindç‰¹å¾ç©ºé—´ ---")
        real_imagebind_audio, real_imagebind_text = precompute_real_features(
            real_dataloader, imagebind_extractor, self.device
        )
        
        print("\n--- è®¡ç®—ConvNetç‰¹å¾ç©ºé—´ ---")
        real_convnet_audio, real_convnet_text = precompute_real_features(
            real_dataloader, convnet_extractor, self.device
        )
        
        print(f"\nImageBind ç‰¹å¾ç»´åº¦: Audio-{real_imagebind_audio.shape}, Text-{real_imagebind_text.shape}")
        print(f"ConvNet ç‰¹å¾ç»´åº¦: Audio-{real_convnet_audio.shape}, Text-{real_convnet_text.shape}")

        # ===================== é˜¶æ®µä¸€ï¼šå››ç§è’¸é¦æ–¹æ³• =====================
        print("\nğŸ”¬ é˜¶æ®µä¸€ï¼šæ‰§è¡Œå››ç§æ•°æ®è’¸é¦æ–¹æ³•...")
        
        # æ–¹æ³•ä¸€ï¼šAVDD on ConvNet
        result1 = self.distiller.distill(
            real_convnet_audio, real_convnet_text,
            method='avdd', feature_space='convnet',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•äºŒï¼šImageBindDC on ImageBind
        result2 = self.distiller.distill(
            real_imagebind_audio, real_imagebind_text,
            method='imagebind_dc', feature_space='imagebind',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•ä¸‰ï¼šAVDD on ImageBind
        result3 = self.distiller.distill(
            real_imagebind_audio, real_imagebind_text,
            method='avdd', feature_space='imagebind',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # æ–¹æ³•å››ï¼šImageBindDC on ConvNet
        result4 = self.distiller.distill(
            real_convnet_audio, real_convnet_text,
            method='imagebind_dc', feature_space='convnet',
            num_syn_samples=num_syn_samples, iterations=1000
        )
        
        # ===================== é˜¶æ®µäºŒ & ä¸‰ï¼šè®­ç»ƒå¹¶è¯„ä¼°æ£€ç´¢æ¨¡å‹ =====================
        print("\nğŸ”¬ é˜¶æ®µäºŒ & ä¸‰ï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå¹¶è¯„ä¼°æ£€ç´¢æ¨¡å‹...")
        
        distilled_datasets = [
            ("AVDD_ConvNet", result1, (real_convnet_audio, real_convnet_text)),
            ("ImageBindDC_ImageBind", result2, (real_imagebind_audio, real_imagebind_text)),
            ("AVDD_ImageBind", result3, (real_imagebind_audio, real_imagebind_text)),
            ("ImageBindDC_ConvNet", result4, (real_convnet_audio, real_convnet_text))
        ]
        
        final_results = {}
        
        for name, synthetic_data, real_eval_features in distilled_datasets:
            print(f"\n--- æ­£åœ¨å¤„ç†æ–¹æ³•: {name} ---")
            
            # å‡†å¤‡åˆæˆæ•°æ®é›†
            syn_dataset = SyntheticDataset(
                synthetic_data['synthetic_audio'], 
                synthetic_data['synthetic_text']
            )

            # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
            # æ³¨æ„ï¼šå­¦ç”Ÿæ¨¡å‹è¾“å…¥ç»´åº¦åº”ä¸åˆæˆç‰¹å¾ç»´åº¦åŒ¹é…
            input_dim = synthetic_data['synthetic_audio'].shape[1]
            feature_dim = 512 # å‡è®¾æˆ‘ä»¬ç»Ÿä¸€è®­ç»ƒä¸€ä¸ª512ç»´çš„å­¦ç”Ÿæ¨¡å‹
            trained_model = self.trainer.train(syn_dataset, input_dim=input_dim, feature_dim=feature_dim, epochs=100)
            
            # åœ¨çœŸå®çš„è¯„ä¼°æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°
            # æ³¨æ„ï¼šè¯„ä¼°æ—¶ï¼Œå­¦ç”Ÿæ¨¡å‹éœ€è¦å¤„ç†ä¸å®ƒè®­ç»ƒæ—¶ç›¸åŒç‰¹å¾ç©ºé—´çš„æ•°æ®
            eval_audio_features, eval_text_features = real_eval_features
            
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€éƒ¨åˆ†çœŸå®ç‰¹å¾ä½œä¸ºè¯„ä¼°é›†
            eval_subset_size = 500
            eval_results = self.evaluator.evaluate(
                trained_model, 
                eval_audio_features[:eval_subset_size], 
                eval_text_features[:eval_subset_size]
            )
            final_results[name] = eval_results
        
        # é˜¶æ®µå››ï¼šç»“æœæ±‡æ€»
        print("\n" + "=" * 80)
        print("ğŸ“Š æœ€ç»ˆ2x2å¯¹æ¯”çŸ©é˜µ")
        print("=" * 80)
        
        print(f"{'æ–¹æ³•':<25} {'Recall@1_A2T':<12} {'Recall@1_T2A':<12} {'Recall@5_A2T':<12} {'Recall@5_T2A':<12}")
        print("-" * 80)
        
        for name, result in results.items():
            print(f"{name:<25} {result['R@1_a2t']:<12.4f} {result['R@1_t2a']:<12.4f} "
                  f"{result['R@5_a2t']:<12.4f} {result['R@5_t2a']:<12.4f}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        final_report = {
            'num_real_samples': num_real_samples,
            'num_syn_samples': num_syn_samples,
            'results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        torch.save(final_report, f'clotho_experiment_{num_syn_samples}s_results.pth')
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: clotho_experiment_{num_syn_samples}s_results.pth")
        
        return results

# ===================== ä¸»å…¥å£ =====================

if __name__ == "__main__":
    print("ğŸš€ Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒå¯åŠ¨")

    # --- ç”¨æˆ·éœ€è¦æä¾›çš„è·¯å¾„ ---
    AUDIO_DIR = './clotho_data/development/'
    CAPTIONS_FILE = './clotho_data/clotho_captions_development.csv'
    # -------------------------

    if not os.path.exists(AUDIO_DIR) or not os.path.exists(CAPTIONS_FILE):
         print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®è·¯å¾„ {AUDIO_DIR} æˆ– {CAPTIONS_FILE}")
    else:
         # åˆ›å»ºå®éªŒå®ä¾‹
         experiment = ClothoExperiment(
             audio_dir=AUDIO_DIR,
             captions_file=CAPTIONS_FILE,
             device=device
         )
         # è¿è¡Œå®éªŒ
         experiment.run_complete_experiment(num_real_samples=100, num_syn_samples=20)