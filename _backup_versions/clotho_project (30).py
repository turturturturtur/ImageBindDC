#!/usr/bin/env python3
"""
Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ
éµå¾ªæŠ€æœ¯è§„æ ¼ä¹¦çš„å®Œæ•´å››é˜¶æ®µå®ç°
"""

import os
import json
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from typing import Dict, Tuple, List
import argparse
import wandb
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType

import sys
# å‡è®¾æ‚¨çš„é¡¹ç›®æ ¹ç›®å½•æ˜¯ 'audio-visual-mater'
PROJECT_ROOT = '/autodl-tmp/audio-visual-mater/'
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)
from transformers import DistilBertTokenizer, DistilBertModel
# ä»æ‚¨çš„æœ¬åœ°æ–‡ä»¶å¤¹å¯¼å…¥å¿…è¦çš„æ¨¡å—
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType
from nets.imagebind import data # ã€å…³é”®ã€‘ä»æ‚¨çš„netsæ–‡ä»¶å¤¹å¯¼å…¥data



class SyntheticDataset(Dataset):
    """ç”¨äºå·²è’¸é¦çš„åˆæˆç‰¹å¾çš„æ•°æ®é›†"""
    def __init__(self, synthetic_audio: torch.Tensor, synthetic_text: torch.Tensor):
        self.audio = synthetic_audio
        self.text = synthetic_text

    def __len__(self):
        return len(self.audio)

    def __getitem__(self, idx):
        return {
            'audio': self.audio[idx],
            'text': self.text[idx],
            'label': idx
        }
class MixedReplayDataloader:
    """
    ä¸€ä¸ªæ··åˆæ•°æ®åŠ è½½å™¨ï¼Œå®ƒåœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­éƒ½æ··åˆäº†åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ã€‚
    """
    def __init__(self, syn_dataset, real_dataset, batch_size=8, real_batch_ratio=0.5):
        self.syn_dataset = syn_dataset
        self.real_dataset = real_dataset
        self.batch_size = batch_size
        
        self.n_real_per_batch = int(batch_size * real_batch_ratio)
        self.n_syn_per_batch = batch_size - self.n_real_per_batch

        # ä¸ºä¸¤ä¸ªæ•°æ®é›†åˆ†åˆ«åˆ›å»ºæ— é™å¾ªç¯çš„åŠ è½½å™¨
        self.syn_loader = DataLoader(
            self.syn_dataset,
            batch_size=self.n_syn_per_batch,
            shuffle=True,
            num_workers=0,
            drop_last=True
        )
        self.real_loader = DataLoader(
            self.real_dataset,
            batch_size=self.n_real_per_batch,
            shuffle=True,
            num_workers=0,
            drop_last=True
        )

        self.syn_iterator = iter(self.syn_loader)
        self.real_iterator = iter(self.real_loader)
    
    def __iter__(self):
        return self

    def __next__(self):
        try:
            syn_batch = next(self.syn_iterator)
        except StopIteration:
            self.syn_iterator = iter(self.syn_loader)
            syn_batch = next(self.syn_iterator)

        try:
            real_batch = next(self.real_iterator)
        except StopIteration:
            self.real_iterator = iter(self.real_loader)
            real_batch = next(self.real_iterator)
            
        # åˆå¹¶ä¸¤ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        # æ³¨æ„ï¼šçœŸå®æ•°æ®çš„æ‰¹æ¬¡æ¥è‡ªRawClothoDatasetï¼Œéœ€è¦å…ˆé€šè¿‡æ•™å¸ˆæ¨¡å‹æå–ç‰¹å¾
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ä¼ å…¥çš„real_datasetå·²ç»æ˜¯ç‰¹å¾ï¼Œè¿™éœ€è¦åœ¨ä¸»æµç¨‹ä¸­å¤„ç†
        
        # è®©æˆ‘ä»¬é‡‡å–ä¸€ä¸ªæ›´ç®€å•çš„å®ç°ï¼šç›´æ¥åœ¨trainerä¸­å¤„ç†
        # è¿™ä¸ªç±»æš‚æ—¶ä¸ç”¨ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨trainerä¸­å®ç°æ··åˆ
        pass # æˆ‘ä»¬å°†åœ¨Trainerä¸­å®ç°è¿™ä¸ªé€»è¾‘ï¼Œè€Œä¸æ˜¯ç”¨ä¸€ä¸ªå¤æ‚çš„Dataloader
# æ·»åŠ è¿™ä¸ªç¼ºå¤±çš„å‡½æ•° (å®ƒä¾èµ–äºä¸€ä¸ªä¿®æ”¹ç‰ˆçš„RawClothoDatasetï¼Œè¿™é‡Œä¸€å¹¶æä¾›)
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
from tqdm import tqdm

class RawClothoDataset(Dataset):
    def __init__(self,
                 audio_dir: str,
                 captions_df: pd.DataFrame,
                 vocab: Dict,
                 sr: int = 22050,
                 n_mels: int = 64,
                 max_time_steps: int = 1024):
        self.audio_dir = audio_dir
        self.captions_df = captions_df
        self.vocab = vocab
        self.sr = sr
        self.n_mels = n_mels
        self.max_time_steps = max_time_steps

    def __len__(self):
        return len(self.captions_df)

    def __getitem__(self, idx):
        row = self.captions_df.iloc[idx]
        audio_path = os.path.join(self.audio_dir, row['file_name'])

        # 1. åŠ è½½éŸ³é¢‘å¹¶è®¡ç®—æ¢…å°”é¢‘è°±å›¾
        y, _ = librosa.load(audio_path, sr=self.sr)
        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)
        mel_spec_db = librosa.power_to_db(mel_spec)          # shape: (n_mels, T)

        # 2. ç»Ÿä¸€æ—¶é—´ç»´åº¦ï¼špadding æˆ–æˆªæ–­
        if mel_spec_db.shape[1] < self.max_time_steps:
            pad_width = self.max_time_steps - mel_spec_db.shape[1]
            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mel_spec_db = mel_spec_db[:, :self.max_time_steps]

        audio_mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # shape: (1, n_mels, T_fixed)

        # 3. æ–‡æœ¬åºåˆ—è½¬ ID
        caption_string = row['caption_1']
        words = caption_string.lower().split()
        word_ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]
        text_ids_tensor = torch.tensor(word_ids, dtype=torch.long)

        return {
            'audio_mel': audio_mel_tensor,
            'text_ids': text_ids_tensor,
            'audio_path': audio_path,
            'raw_caption': caption_string
        }
def build_vocab(captions: List[str], min_freq=5) -> Dict:
    word_counts = {}
    for caption in captions:
        for word in caption.lower().split():
            word_counts[word] = word_counts.get(word, 0) + 1

    vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items()) if count >= min_freq}
    vocab['<pad>'] = 0
    vocab['<unk>'] = 1
    return vocab

def collate_fn(batch):
    # ä¸º ConvNetGRU å¤„ç†æ•°æ®
    audio_mels = [item['audio_mel'] for item in batch]
    text_ids = [item['text_ids'] for item in batch]
    texts_padded = pad_sequence(text_ids, batch_first=True, padding_value=0)
    audios_stacked = torch.stack(audio_mels)

    # ä¸º ImageBind æ”¶é›†åŸå§‹æ•°æ® (ä¿æŒä¸ºList)
    audio_paths = [item['audio_path'] for item in batch]
    raw_captions = [item['raw_caption'] for item in batch]
    
    return {
        'audio_mel': audios_stacked,
        'text_ids': texts_padded,
        'audio_path': audio_paths,
        'raw_caption': raw_captions
    }
    
def get_high_performance_dataloader(audio_dir: str, captions_file: str, batch_size: int, num_samples: int, vocab: Dict = None) -> Tuple[DataLoader, Dict]:
    """
    åˆ›å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„Dataloaderã€‚
    æ–°å¢åŠŸèƒ½ï¼šå¯ä»¥æ¥æ”¶ä¸€ä¸ªå·²æœ‰çš„vocabï¼Œå¦‚æœvocabä¸ºNoneï¼Œåˆ™ä¼šè‡ªå·±æ„å»ºã€‚
    """
    df = pd.read_csv(captions_file)
    if num_samples:
        # æ³¨æ„ï¼šå¯¹äºè¯„ä¼°é›†ï¼Œæˆ‘ä»¬é€šå¸¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼Œå› æ­¤num_samplesåº”ä¸ºNone
        df = df.sample(n=num_samples, random_state=42).reset_index(drop=True)

    if vocab is None:
        print("æœªæä¾›è¯æ±‡è¡¨ï¼Œæ­£åœ¨ä»å­—å¹•ä¸­æ„å»º...")
        vocab = build_vocab(df['caption_1'].tolist())
    else:
        print("ä½¿ç”¨å·²æä¾›çš„è¯æ±‡è¡¨ã€‚")

    dataset = RawClothoDataset(audio_dir, df, vocab)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)
    
    # å§‹ç»ˆè¿”å›ä½¿ç”¨çš„dataloaderå’Œvocab
    return dataloader, vocab

def precompute_real_features(dataloader: DataLoader, feature_extractor, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
    if isinstance(feature_extractor, nn.Module):
        feature_extractor.to(device)
        feature_extractor.eval()
    
    all_audio_features, all_text_features = [], []
    print(f"ğŸš€ å¼€å§‹é¢„è®¡ç®—çœŸå®ç‰¹å¾ï¼Œä½¿ç”¨æ¨¡å‹: {feature_extractor.__class__.__name__}")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æ­£åœ¨æå–ç‰¹å¾"):
            # ã€æœ€ç»ˆç‰ˆã€‘æ™ºèƒ½æ•°æ®åˆ†å‘
            if isinstance(feature_extractor, ImageBindExtractor):
                # ä¸º ImageBind ä¼ é€’ã€æ–‡ä»¶è·¯å¾„ã€‘å’Œã€åŸå§‹å­—ç¬¦ä¸²ã€‘
                audio_feats = feature_extractor.extract_audio_features(batch['audio_path'])
                text_feats = feature_extractor.extract_text_features(batch['raw_caption'])

            elif isinstance(feature_extractor, PretrainedConvNetExtractor):
                # ä¸º PretrainedConvNetExtractor ä¼ é€’ã€é¢‘è°±å›¾ã€‘å’Œã€åŸå§‹å­—ç¬¦ä¸²ã€‘
                audio_data = batch['audio_mel'].to(device)
                text_data = batch['raw_caption'] # æ³¨æ„ï¼šæ–‡æœ¬ç«¯éœ€è¦åŸå§‹å­—ç¬¦ä¸²
                audio_feats = feature_extractor.forward_audio(audio_data)
                text_feats = feature_extractor.forward_text(text_data)
            
            else:
                # ä¸ºå…¶ä»–æ¨¡å‹ï¼ˆä¾‹å¦‚æˆ‘ä»¬æœ€åˆçš„ConvNetGRUï¼‰ä¼ é€’ã€é¢‘è°±å›¾ã€‘å’Œã€IDåºåˆ—ã€‘
                audio_data = batch['audio_mel'].to(device)
                text_data = batch['text_ids'].to(device)
                audio_feats = feature_extractor.forward_audio(audio_data)
                text_feats = feature_extractor.forward_text(text_data)

            all_audio_features.append(audio_feats.cpu())
            all_text_features.append(text_feats.cpu())
            
    print("âœ… çœŸå®ç‰¹å¾é¢„è®¡ç®—å®Œæˆï¼")
    return torch.cat(all_audio_features, dim=0).to(device), torch.cat(all_text_features, dim=0).to(device)
# ===================== ç‰¹å¾æå–å™¨ =====================
class PretrainedConvNetExtractor(nn.Module):
    """
    æœ€ç»ˆå¼ºåŒ–ç‰ˆæ•™å¸ˆæ¨¡å‹ï¼š
    - éŸ³é¢‘ç«¯: ä½¿ç”¨æ‚¨é¡¹ç›®ä¸­é¢„è®­ç»ƒçš„ net_sound
    - æ–‡æœ¬ç«¯: é¢„è®­ç»ƒçš„ DistilBERT
    """
    def __init__(self, feature_dim: int = 512, device='cuda'):
        super().__init__()
        self.device = device

        # 1. åŠ è½½æ‚¨é¡¹ç›®ä¸­é¢„è®­ç»ƒçš„éŸ³é¢‘ç½‘ç»œ (net_sound)
        print("æ­£åœ¨åŠ è½½æ‚¨æœ¬åœ°çš„é¢„è®­ç»ƒ ConvNet (net_sound)...")
        class MockConvNetArgs:
            def __init__(self):
                self.arch_sound, self.weights_sound = 'convNet', ''
                self.arch_frame, self.weights_frame = 'convNet', ''
                self.arch_classifier, self.weights_classifier = 'ensemble', ''
                self.cls_num, self.input_modality = 10, 'a'
        
        builder = ModelBuilder()
        self.audio_tower = builder.build_sound(
            arch=MockConvNetArgs().arch_sound,
            weights=MockConvNetArgs().weights_sound
        )
        for param in self.audio_tower.parameters():
            param.requires_grad = False
        
        # 2. åŠ è½½é¢„è®­ç»ƒçš„DistilBERTä½œä¸ºæ–‡æœ¬ç½‘ç»œ
        print("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒçš„ DistilBERT æ¨¡å‹...")
        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        self.text_tower = DistilBertModel.from_pretrained('distilbert-base-uncased')
        for param in self.text_tower.parameters():
            param.requires_grad = False

        # 3. æŠ•å½±å¤´ï¼Œå°†ä¸åŒç»´åº¦çš„ç‰¹å¾ç»Ÿä¸€åˆ°feature_dim
        self.audio_projector = nn.Linear(6272, feature_dim) # net_soundè¾“å‡º6272ç»´
        self.text_projector = nn.Linear(768, feature_dim) # DistilBERTè¾“å‡º768ç»´

    def forward_audio(self, audio_mel):
        feat = self.audio_tower(audio_mel)
        projected_feat = self.audio_projector(feat)
        return F.normalize(projected_feat, dim=-1)

    def forward_text(self, text_strings: List[str]) -> torch.Tensor:
        # ä½¿ç”¨BERTçš„åˆ†è¯å™¨è¿›è¡Œå¤„ç†
        inputs = self.tokenizer(text_strings, return_tensors="pt", padding=True, truncation=True, max_length=77).to(self.device)
        outputs = self.text_tower(**inputs)
        # ä½¿ç”¨[CLS] tokençš„è¾“å‡ºæ¥ä»£è¡¨æ•´ä¸ªå¥å­çš„ç‰¹å¾
        cls_token_feat = outputs.last_hidden_state[:, 0, :]
        projected_feat = self.text_projector(cls_token_feat)
        return F.normalize(projected_feat, dim=-1)
        
# class ConvNetGRU(nn.Module):
#     """ConvNet+GRUåŒå¡”æ¨¡å‹"""
    
#     def __init__(self, vocab_size: int, embedding_dim: int = 256, feature_dim: int = 512):
#         super().__init__()
        
#         # éŸ³é¢‘å¡” (è¿™éƒ¨åˆ†ä¿æŒä¸å˜)
#         self.audio_tower = nn.Sequential(
#             nn.Conv2d(1, 64, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(64, 128, 3, padding=1),
#             nn.ReLU(),
#             nn.AdaptiveAvgPool2d((1, 1)),
#             nn.Flatten(),
#             nn.Linear(128, feature_dim)
#         )
        
#         # æ–‡æœ¬å¡” (é‡æ„ä¸ºç‹¬ç«‹çš„å±‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªSequential)
#         self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
#         self.text_gru = nn.GRU(embedding_dim, feature_dim, batch_first=True)
    
#     def forward(self, x):
#         """ç®€åŒ–forwardå‡½æ•°ç”¨äºç‰¹å¾æå–"""
#         return x  # ç›´æ¥è¿”å›è¾“å…¥ï¼Œå› ä¸ºè¿™é‡Œåªæ˜¯ç‰¹å¾è½¬æ¢
    
#     def forward_audio(self, audio):
#         return F.normalize(self.audio_tower(audio), dim=-1)
    
#     def forward_text(self, text: torch.Tensor) -> torch.Tensor:
#         text = torch.clamp(text, max=self.text_embedding.num_embeddings - 1)
#         # 1. å°†è¾“å…¥çš„è¯æ±‡IDåºåˆ—é€šè¿‡Embeddingå±‚è½¬æ¢ä¸ºç‰¹å¾å‘é‡åºåˆ—
#         embedded_text = self.text_embedding(text)
        
#         # 2. å°†ç‰¹å¾å‘é‡åºåˆ—è¾“å…¥GRU
#         # GRUçš„ç¬¬äºŒä¸ªè¿”å›å€¼h_næ˜¯æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(num_layers, batch_size, hidden_size)
#         _, h_n = self.text_gru(embedded_text)
        
#         # 3. h_n.squeeze(0)å°†å½¢çŠ¶å˜ä¸º(batch_size, hidden_size)ï¼Œè¿™é€šå¸¸è¢«ç”¨ä½œæ•´ä¸ªåºåˆ—çš„ç‰¹å¾è¡¨ç¤º
#         text_feat = h_n.squeeze(0)
        
#         # 4. å½’ä¸€åŒ–ç‰¹å¾
#         return F.normalize(text_feat, dim=-1)

class ImageBindExtractor:
    """çœŸå®çš„ ImageBind ç‰¹å¾æå–å™¨ (æ¨¡ä»¿æ‚¨çš„æœ¬åœ°é¡¹ç›®)"""
    def __init__(self, pretrained=False, device='cpu'):
        self.device = device
        self.model = None
        self.embed_func = None

        if pretrained:
            print("æ­£åœ¨ä½¿ç”¨æ‚¨æœ¬åœ°çš„ get_network_imagebind å‡½æ•°åŠ è½½æ¨¡å‹...")
            
            class MockArgs:
                def __init__(self):
                    self.arch_frame = 'imagebind'
                    self.arch_classifier = 'ensemble'
                    self.cls_num = 10
                    self.weights_classifier = ''
                    self.input_modality = 'av'

            mock_args = MockArgs()
            
            nets, _ = get_network_imagebind(mock_args)
            net_imagebind, _ = nets
            
            self.model = net_imagebind
            self.model.to(self.device)
            self.model.eval()

            self.embed_func = self.model.module.embed if torch.cuda.device_count() > 1 else self.model.embed
            print("âœ… çœŸå® ImageBind æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def extract_audio_features(self, audio_path_list: List[str]) -> torch.Tensor:
        if self.model is None: raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
        # ã€å…³é”®ã€‘è°ƒç”¨æ‚¨æœ¬åœ°çš„ data.py ä¸­çš„å‡½æ•°
        inputs = { ModalityType.AUDIO: data.load_and_transform_audio_data(audio_path_list, self.device) }
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.AUDIO].detach()

    def extract_text_features(self, raw_caption_list: List[str]) -> torch.Tensor:
        if self.model is None:
            raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
    
        print("raw_caption_list å‰ 3 æ¡ï¼š", raw_caption_list[:3])
    
        # 1ï¸âƒ£ æ‹¿åˆ° token id
        text_tokens = data.load_and_transform_text(raw_caption_list, self.device)
    
        # 2ï¸âƒ£ æ‰“å°æœ€å¤§ idï¼Œçœ‹æ˜¯å¦çœŸçš„ â‰¥ 49408
        max_id = text_tokens.max().item()
        print("max token id =", max_id, " è¶Šç•Œï¼Ÿ", max_id >= 49408)
    
        # 3ï¸âƒ£ å…œåº• clamp
        text_tokens = torch.clamp(text_tokens, max=49407)
    
        inputs = {ModalityType.TEXT: text_tokens}
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.TEXT].detach()

# ===================== æ ‡å‡†Herdingç®—æ³• =====================

class StandardHerdingSelector:
    """æ ‡å‡†Herdingç®—æ³•çš„è§„èŒƒå®ç°"""

    def __init__(self, device='cuda'):
        self.device = device

    def select_coreset(self, features, coreset_size):
        if coreset_size >= features.shape[0]:
            return torch.arange(features.shape[0]), features

        # 1. è®¡ç®—ä¸€æ¬¡å…¨ä½“ç‰¹å¾çš„å‡å€¼
        overall_mean = features.mean(dim=0)

        selected_indices = []
        available_indices = list(range(features.shape[0]))

        # 2. åˆå§‹åŒ–å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
        current_coreset_mean = torch.zeros_like(overall_mean)

        for i in range(coreset_size):
            # 3. è®¡ç®—å½“å‰å‡å€¼ä¸ç›®æ ‡å‡å€¼çš„â€œè¯¯å·®å‘é‡â€
            error_vector = overall_mean - current_coreset_mean

            # 4. åœ¨å‰©ä½™æ ·æœ¬ä¸­ï¼Œå¯»æ‰¾ä¸è¯¯å·®å‘é‡å†…ç§¯æœ€å¤§çš„é‚£ä¸ª
            available_features = features[available_indices]
            projections = torch.matmul(available_features, error_vector)

            best_idx_in_remaining = torch.argmax(projections)
            selected_idx = available_indices.pop(best_idx_in_remaining)

            selected_indices.append(selected_idx)

            # 5. æ›´æ–°å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
            current_coreset_mean = (current_coreset_mean * i + features[selected_idx]) / (i + 1)

        selected_indices = torch.tensor(selected_indices)
        return selected_indices, features[selected_indices]

# ===================== æŸå¤±å‡½æ•° =====================

class AVDDLoss:
    """AVDDæŸå¤±å‡½æ•°"""
    
    def __init__(self, lam_icm=10.0, lam_cgm=10.0):
        self.lam_icm = lam_icm
        self.lam_cgm = lam_cgm
    
    def __call__(self, real_features, synthetic_features):
        """è®¡ç®—AVDDæŸå¤±"""
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features
        
        # 1. åˆ†å¸ƒåŒ¹é…æŸå¤± (DM Loss)
        loss_dm = F.mse_loss(syn_audio.mean(0), real_audio.mean(0)) + \
                 F.mse_loss(syn_text.mean(0), real_text.mean(0))
        
        # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤± (ICM Loss) - ä¿®æ­£ååŒ¹é…ç»Ÿè®¡é‡
        # å‡è®¾è¾“å…¥çš„ç‰¹å¾å·²ç»è¢«å½’ä¸€åŒ–
        real_sim_matrix = torch.matmul(real_audio, real_text.t())
        syn_sim_matrix = torch.matmul(syn_audio, syn_text.t())
        
        loss_icm = F.mse_loss(syn_sim_matrix.mean(), real_sim_matrix.mean()) + \
                   F.mse_loss(syn_sim_matrix.std(), real_sim_matrix.std())
        
        # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤± (CGM Loss)
        real_gap = real_audio.mean(0) - real_text.mean(0)
        syn_gap = syn_audio.mean(0) - syn_text.mean(0)
        loss_cgm = F.mse_loss(syn_gap, real_gap)
        
        return loss_dm + self.lam_icm * loss_icm + self.lam_cgm * loss_cgm
        
class ImageBindDCLoss:
    """
    ImageBindDCæŸå¤±å‡½æ•° - ä¿®æ­£åé‡‡ç”¨çœŸå®çš„CFDå®ç°ã€‚
    """
    def __init__(self, lam_cross=1.0, lam_joint=1.0, num_freqs=4096, device='cuda'):
        self.lam_cross = lam_cross
        self.lam_joint = lam_joint
        self.num_freqs = num_freqs
        self.device = device
        self._t_cache = {} # ç”¨äºç¼“å­˜ä¸åŒç»´åº¦çš„éšæœºé¢‘ç‡å‘é‡t

    def _get_t(self, feature_dim):
        """æ ¹æ®ç‰¹å¾ç»´åº¦è·å–æˆ–ç”Ÿæˆéšæœºé¢‘ç‡å‘é‡t"""
        if feature_dim not in self._t_cache:
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºé¢‘ç‡
            self._t_cache[feature_dim] = torch.randn(feature_dim, self.num_freqs, device=self.device)
        return self._t_cache[feature_dim]

    def compute_cfd(self, feat1: torch.Tensor, feat2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç‰¹å¾å‡½æ•°è·ç¦» (CFD)ã€‚
        å…¬å¼: L2^2(Î¦_x(t), Î¦_y(t))
        """
        feature_dim = feat1.shape[1]
        t = self._get_t(feature_dim) # è·å–éšæœºé¢‘ç‡å‘é‡

        # 1. å°†ç‰¹å¾æŠ•å½±åˆ°éšæœºé¢‘ç‡ä¸Š (t^T * x)
        proj1 = torch.matmul(feat1, t)
        proj2 = torch.matmul(feat2, t)

        # 2. è®¡ç®—ç‰¹å¾å‡½æ•° (é€šè¿‡é‡‡æ ·è¿‘ä¼¼æœŸæœ› E[exp(j * proj)])
        # exp(j*z) = cos(z) + j*sin(z)
        # E[cos(z)] å’Œ E[sin(z)] é€šè¿‡åœ¨batchç»´åº¦ä¸Šæ±‚å‡å€¼æ¥è¿‘ä¼¼
        phi1_real = torch.cos(proj1).mean(dim=0)
        phi1_imag = torch.sin(proj1).mean(dim=0)
        phi2_real = torch.cos(proj2).mean(dim=0)
        phi2_imag = torch.sin(proj2).mean(dim=0)

        # 3. è®¡ç®—ä¸¤ä¸ªå¤æ•°ç‰¹å¾å‡½æ•°ä¹‹é—´çš„L2è·ç¦»çš„å¹³æ–¹
        dist_sq = (phi1_real - phi2_real)**2 + (phi1_imag - phi2_imag)**2
        
        # 4. åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„CFDæŸå¤±
        return dist_sq.sum()

    def __call__(self, real_features, synthetic_features):
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features

        # ç”±äºçœŸå®æ•°æ®é‡è¿œå¤§äºåˆæˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­é‡‡æ ·ä¸€ä¸ªå­é›†æ¥è¿›è¡ŒåŒ¹é…
        num_syn = len(syn_audio)
        rand_idx = torch.randperm(len(real_audio))[:num_syn]
        real_audio_subset = real_audio[rand_idx]
        real_text_subset = real_text[rand_idx]

        # 1. å•æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Uni-modal Distribution Matching)
        loss_uni = self.compute_cfd(real_audio_subset, syn_audio) + \
                   self.compute_cfd(real_text_subset, syn_text)
        
        # 2. è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Cross-modal Distribution Matching)
        # éµå¾ªè®ºæ–‡å…¬å¼ CFD(Real_Audio + Syn_Text, Real_Text + Syn_Audio)
        mix_dist1 = real_audio_subset + syn_text
        mix_dist2 = real_text_subset + syn_audio
        loss_cross = self.compute_cfd(mix_dist1, mix_dist2)
        
        # 3. è”åˆåˆ†å¸ƒåŒ¹é… (Joint Distribution Matching)
        # æ‹¼æ¥ç‰¹å¾ï¼ŒåŒ¹é…å…¶è”åˆåˆ†å¸ƒ
        joint_real = torch.cat([real_audio_subset, real_text_subset], dim=1)
        joint_syn = torch.cat([syn_audio, syn_text], dim=1)
        loss_joint = self.compute_cfd(joint_real, joint_syn)
        
        return loss_uni + self.lam_cross * loss_cross + self.lam_joint * loss_joint
# ===================== æ•°æ®è’¸é¦ =====================

class DataDistillation:
    """æ•°æ®è’¸é¦å¼•æ“"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = StandardHerdingSelector(device)
    
    # åœ¨ DataDistillation ç±»ä¸­
    def distill(self, real_audio_features, real_text_features, args, method='avdd', feature_space='imagebind'):
        """æ ‡å‡†è’¸é¦æµç¨‹ï¼Œç­¾åå·²ä¿®æ”¹ä¸ºæ¥æ”¶args"""
        
        print(f"\nğŸ”¬ å¼€å§‹{method} on {feature_space}è’¸é¦...")
        
        num_syn_samples = args.ipc
        iterations = args.distill_iter
    
        audio_indices, audio_coreset = self.herding.select_coreset(real_audio_features, num_syn_samples)
        text_indices, text_coreset = self.herding.select_coreset(real_text_features, num_syn_samples)
        
        syn_audio = audio_coreset.clone().detach().requires_grad_(True)
        syn_text = text_coreset.clone().detach().requires_grad_(True)
        
        # ã€æœ€ç»ˆä¿®æ­£ã€‘ç¡®ä¿ImageBindDCLossä¹Ÿä½¿ç”¨argsä¸­çš„æƒé‡
        if method == 'avdd':
            loss_fn = AVDDLoss(lam_icm=args.lam_icm, lam_cgm=args.lam_cgm)
        else:  # imagebind_dc
            loss_fn = ImageBindDCLoss(lam_cross=args.lam_cross, lam_joint=args.lam_joint, device=self.device)
        
        current_lr = args.distill_lr_avdd if method == 'avdd' else args.distill_lr_cfd
        print(f"  ä½¿ç”¨å­¦ä¹ ç‡: {current_lr}")
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=current_lr)
        
        loss_history = []
        
        for iteration in range(iterations):
            loss = loss_fn(
                (real_audio_features, real_text_features),
                (syn_audio, syn_text)
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_history.append(loss.item())
            
            if iteration % 50 == 0 or iteration == iterations - 1:
                if not args.disable_wandb:
                    wandb.log({f"distill_loss/{method}_{feature_space}": loss.item()})
                print(f"è¿­ä»£ {iteration}/{iterations}, æŸå¤±: {loss.item():.6f}")
        
        return {
            'synthetic_audio': syn_audio.detach(),
            'synthetic_text': syn_text.detach(),
            'loss_history': loss_history
        }
# ===================== æ£€ç´¢æ¨¡å‹è®­ç»ƒ =====================
class StudentRetriever(nn.Module):
    """
    ç»Ÿä¸€çš„å­¦ç”Ÿæ£€ç´¢æ¨¡å‹ã€‚
    å®ƒåŒ…å«ä¸€ä¸ªå›ºå®šçš„ã€é¢„è®­ç»ƒçš„ImageBindä¸»å¹²å’Œä¸€ä¸ªå¯è®­ç»ƒçš„æ˜ å°„å¤´ã€‚
    """
    def __init__(self, feature_dim=512, device='cuda'):
        super().__init__()
        self.device = device
        print("åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹: å†»ç»“ImageBindä¸»å¹² + å¯è®­ç»ƒæ˜ å°„å¤´")

        # 1. åŠ è½½å¹¶å†»ç»“ImageBindä¸»å¹²ç½‘ç»œ
        class MockArgs:
            def __init__(self):
                self.arch_frame = 'imagebind'
                self.arch_classifier = 'ensemble'
                self.cls_num = 10
                self.weights_classifier = ''
                self.input_modality = 'av'
        nets, _ = get_network_imagebind(MockArgs())
        self.imagebind_backbone, _ = nets
        self.imagebind_backbone.to(self.device)
        for param in self.imagebind_backbone.parameters():
            param.requires_grad = False
        self.imagebind_backbone.eval()
        self.embed_func = self.imagebind_backbone.module.embed if torch.cuda.device_count() > 1 else self.imagebind_backbone.embed
        
        # 2. å®šä¹‰å¯è®­ç»ƒçš„æ˜ å°„å¤´ (è¾“å…¥ç»´åº¦ä¸ºImageBindçš„1024)
        self.audio_head = nn.Sequential(nn.Linear(1024, feature_dim))
        self.text_head = nn.Sequential(nn.Linear(1024, feature_dim))

    def forward(self, audio_features, text_features):
        audio_feat_projected = self.audio_head(audio_features)
        text_feat_projected = self.text_head(text_features)
        return F.normalize(audio_feat_projected, dim=-1), F.normalize(text_feat_projected, dim=-1)

    def project_real_data(self, audio_paths, text_strings):
        """åœ¨è¯„ä¼°æ—¶ï¼Œå…ˆç”¨å†»ç»“ä¸»å¹²æå–ç‰¹å¾ï¼Œå†ç”¨å¤´éƒ¨æŠ•å½±"""
        inputs = {
            ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, self.device),
            ModalityType.TEXT: data.load_and_transform_text(text_strings, self.device),
        }
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
            audio_ib_features = embeddings[ModalityType.AUDIO].detach()
            text_ib_features = embeddings[ModalityType.TEXT].detach()
            
            projected_audio, projected_text = self.forward(audio_ib_features, text_ib_features)
        return projected_audio, projected_text


class RetrievalModel(nn.Module):
    """
    ç”¨äº ConvNet ç‰¹å¾ç©ºé—´çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    è¿™æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹è®­ç»ƒçš„å¤šå±‚æ„ŸçŸ¥æœº (MLP)ã€‚
    """
    def __init__(self, input_dim=512, feature_dim=512):
        super().__init__()
        self.audio_encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim)
        )
        self.text_encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim)
        )
    
    def forward(self, audio, text):
        audio_feat = F.normalize(self.audio_encoder(audio), dim=-1)
        text_feat = F.normalize(self.text_encoder(text), dim=-1)
        return audio_feat, text_feat

class StudentRetriever(nn.Module):
    """
    ç”¨äº ImageBind ç‰¹å¾ç©ºé—´çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    ã€å…³é”®ã€‘ï¼šå®ƒåŒ…å«ä¸€ä¸ªå›ºå®šçš„ã€é¢„è®­ç»ƒçš„ImageBindä¸»å¹²å’Œä¸€ä¸ªå¯è®­ç»ƒçš„æ˜ å°„å¤´ã€‚
    """
    def __init__(self, feature_dim=512, device='cuda'):
        super().__init__()
        self.device = device
        print("åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹: å†»ç»“ImageBindä¸»å¹² + å¯è®­ç»ƒæ˜ å°„å¤´")

        # 1. åŠ è½½å¹¶å†»ç»“ImageBindä¸»å¹²ç½‘ç»œ
        class MockArgs:
            def __init__(self):
                self.arch_frame = 'imagebind'
                self.arch_classifier = 'ensemble'
                self.cls_num = 10
                self.weights_classifier = ''
                self.input_modality = 'av'
        nets, _ = get_network_imagebind(MockArgs())
        self.imagebind_backbone, _ = nets
        self.imagebind_backbone.to(self.device)
        for param in self.imagebind_backbone.parameters():
            param.requires_grad = False # å†»ç»“
        self.imagebind_backbone.eval()
        self.embed_func = self.imagebind_backbone.module.embed if torch.cuda.device_count() > 1 else self.imagebind_backbone.embed
        
        # 2. å®šä¹‰ã€å¯è®­ç»ƒã€‘çš„æ˜ å°„å¤´ (è¾“å…¥ç»´åº¦ä¸ºImageBindçš„1024)
        self.audio_head = nn.Sequential(nn.Linear(1024, feature_dim))
        self.text_head = nn.Sequential(nn.Linear(1024, feature_dim))

    def forward(self, audio_features, text_features):
        """è¿™ä¸ªforwardåªé€šè¿‡å¯è®­ç»ƒçš„å¤´éƒ¨"""
        audio_feat_projected = self.audio_head(audio_features)
        text_feat_projected = self.text_head(text_features)
        return F.normalize(audio_feat_projected, dim=-1), F.normalize(text_feat_projected, dim=-1)
        
class RetrievalTrainer:
    """æ£€ç´¢æ¨¡å‹è®­ç»ƒå™¨ï¼ˆæœ€ç»ˆç‰ˆ - å¼•å…¥çœŸå®æ•°æ®é‡æ”¾ + å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰"""
    
    def __init__(self, device='cuda'):
        self.device = device

    def info_nce_loss(self, audio_feat, text_feat, temperature=0.07):
        logits = torch.matmul(audio_feat, text_feat.t()) / temperature
        labels = torch.arange(len(audio_feat), device=self.device)
        return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)) / 2

    def train(self, syn_dataset, real_feature_dataset, args, student_model_type, feature_dim=512):
        print(f"\nğŸ¯ è®­ç»ƒå­¦ç”Ÿæ¨¡å‹... (ç±»å‹: {student_model_type}, Replay Ratio: 0.5)")
        
        real_dataloader = DataLoader(real_feature_dataset, batch_size=args.student_batch_size // 2, shuffle=True, num_workers=0)
        real_iterator = iter(real_dataloader)
        
        syn_dataloader = DataLoader(syn_dataset, batch_size=args.student_batch_size // 2, shuffle=True, num_workers=0)
        syn_iterator = iter(syn_dataloader)

        if student_model_type == 'convnet':
            input_dim = syn_dataset.audio.shape[1]
            model = RetrievalModel(input_dim=input_dim, feature_dim=feature_dim).to(self.device)
            optimizer = torch.optim.Adam(model.parameters(), lr=args.student_lr, weight_decay=1e-4)
        elif student_model_type == 'imagebind':
            model = StudentRetriever(feature_dim=feature_dim, device=self.device).to(self.device)
            optimizer = torch.optim.Adam(
                list(model.audio_head.parameters()) + list(model.text_head.parameters()),
                lr=args.student_lr,
                weight_decay=1e-4
            )
        
        # ã€æ–°å¢ã€‘åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.student_epochs)
        
        model.train()
        for epoch in range(args.student_epochs):
            total_loss = 0
            for i in range(len(syn_dataloader)):
                try:
                    syn_batch = next(syn_iterator)
                except StopIteration:
                    syn_iterator = iter(syn_dataloader)
                    syn_batch = next(syn_iterator)
                
                try:
                    real_batch = next(real_iterator)
                except StopIteration:
                    real_iterator = iter(real_dataloader)
                    real_batch = next(real_iterator)

                audio = torch.cat([syn_batch['audio'], real_batch['audio']], dim=0).to(self.device)
                text = torch.cat([syn_batch['text'], real_batch['text']], dim=0).to(self.device)
                
                audio_feat, text_feat = model(audio, text)
                loss = self.info_nce_loss(audio_feat, text_feat)
                optimizer.zero_grad(); loss.backward(); optimizer.step()
                total_loss += loss.item()

            if (epoch + 1) % 10 == 0 or epoch == args.student_epochs - 1:
                current_lr = scheduler.get_last_lr()[0]
                if not args.disable_wandb:
                    # ã€æ–°å¢ã€‘è®°å½•å½“å‰å­¦ä¹ ç‡åˆ°wandb
                    wandb.log({f"lr/{student_model_type}": current_lr, "epoch": epoch + 1})
                print(f"è®­ç»ƒè½®æ¬¡ {epoch+1}/{args.student_epochs}, å¹³å‡æŸå¤±: {total_loss/len(syn_dataloader):.4f}, å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ã€æ–°å¢ã€‘åœ¨æ¯ä¸ªepochç»“æŸåï¼Œæ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
        
        model.eval()
        return model
# ===================== æ ‡å‡†è¯„ä¼° =====================

class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨ï¼ˆæœ€ç»ˆç‰ˆï¼‰"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def evaluate(self, model, real_audio_features, real_text_features):
        """æ ‡å‡†Recall@Kè¯„ä¼°ï¼Œæ¥å£ç»Ÿä¸€"""
        
        print("\nğŸ¯ æ ‡å‡†æ£€ç´¢è¯„ä¼°...")
        model.eval()
        with torch.no_grad():
            # ä¸è®ºå­¦ç”Ÿæ¨¡å‹æ˜¯å“ªç§ï¼Œéƒ½é€šè¿‡å…¶forwardæ–¹æ³•è·å¾—å¯¹çœŸå®ç‰¹å¾çš„æœ€ç»ˆæŠ•å½±
            audio_feat, text_feat = model(real_audio_features.to(self.device), real_text_features.to(self.device))
            
            sim_matrix = torch.matmul(audio_feat, text_feat.t())
            
            results = {}
            for k in [1, 5, 10, 20]:
                # A2Tå¬å›ç‡ (æ›´é«˜æ•ˆçš„è®¡ç®—æ–¹å¼)
                _, topk_a2t = torch.topk(sim_matrix, k=k, dim=1)
                correct_a2t = torch.any(torch.arange(len(audio_feat), device=self.device)[:, None] == topk_a2t, dim=1)
                recall_a2t = torch.mean(correct_a2t.float()).item()
                
                # T2Aå¬å›ç‡
                _, topk_t2a = torch.topk(sim_matrix.t(), k=k, dim=1)
                correct_t2a = torch.any(torch.arange(len(text_feat), device=self.device)[:, None] == topk_t2a, dim=1)
                recall_t2a = torch.mean(correct_t2a.float()).item()
                
                results[f'R@{k}_a2t'] = recall_a2t
                results[f'R@{k}_t2a'] = recall_t2a
        
        return results
# ===================== ä¸»å®éªŒæµç¨‹ =====================

class ClothoExperiment:
    """Clothoæ•°æ®é›†å®Œæ•´å®éªŒ"""
    
    def __init__(self, dev_audio_dir: str, dev_captions_file: str, eval_audio_dir: str, eval_captions_file: str, device='cuda'):
            self.device = device
            # å¼€å‘é›†è·¯å¾„
            self.dev_audio_dir = dev_audio_dir
            self.dev_captions_file = dev_captions_file
            # è¯„ä¼°é›†è·¯å¾„
            self.eval_audio_dir = eval_audio_dir
            self.eval_captions_file = eval_captions_file
            
            self.vocab = None # ç”¨äºå­˜å‚¨è¯æ±‡è¡¨
            self.distiller = DataDistillation(device)
            self.trainer = RetrievalTrainer(device)
            self.evaluator = StandardEvaluator(device)
    
    def run_complete_experiment(self, args):
        """
        æ‰§è¡Œå®Œæ•´å››é˜¶æ®µå®éªŒï¼ˆæœ€ç»ˆç‰ˆ - é›†æˆçœŸå®æ•°æ®é‡æ”¾ç­–ç•¥ï¼‰ã€‚
        """
        print("ğŸ† Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")
        print("=" * 80)
        
        # ===================== é˜¶æ®µé›¶ï¼šå‡†å¤‡æ•°æ®å’Œç‰¹å¾æå–å™¨ =====================
        print("ğŸ“Š é˜¶æ®µé›¶ï¼šåŠ è½½æ•°æ®å¹¶å‡†å¤‡ç‰¹å¾æå–å™¨...")
    
        dev_dataloader, self.vocab = get_high_performance_dataloader(
            audio_dir=self.dev_audio_dir,
            captions_file=self.dev_captions_file,
            batch_size=32,
            num_samples=args.dev_samples
        )
        
        eval_dataloader, _ = get_high_performance_dataloader(
            audio_dir=self.eval_audio_dir,
            captions_file=self.eval_captions_file,
            batch_size=32,
            num_samples=None,
            vocab=self.vocab
        )
    
        imagebind_extractor = ImageBindExtractor(pretrained=True, device=self.device)
        convnet_extractor = PretrainedConvNetExtractor(feature_dim=512, device=self.device).to(self.device)
    
        # ===================== é˜¶æ®µé›¶(ç»­)ï¼šç‰¹å¾é¢„è®¡ç®— =====================
        print("\n--- åœ¨å¼€å‘é›†ä¸Šé¢„è®¡ç®—ã€è’¸é¦ç”¨ å’Œ é‡æ”¾ç”¨ã€‘ç‰¹å¾ ---")
        real_imagebind_audio, real_imagebind_text = precompute_real_features(dev_dataloader, imagebind_extractor, self.device)
        real_convnet_audio, real_convnet_text = precompute_real_features(dev_dataloader, convnet_extractor, self.device)
        
        print("\n--- åœ¨è¯„ä¼°é›†ä¸Šé¢„è®¡ç®—ã€è¯„ä¼°ç”¨ã€‘ç‰¹å¾ ---")
        eval_imagebind_audio, eval_imagebind_text = precompute_real_features(eval_dataloader, imagebind_extractor, self.device)
        eval_convnet_audio, eval_convnet_text = precompute_real_features(eval_dataloader, convnet_extractor, self.device)
        
        # ã€æ–°å¢ã€‘å°†é¢„è®¡ç®—å‡ºçš„çœŸå®å¼€å‘é›†ç‰¹å¾ä¹ŸåŒ…è£…æˆDatasetï¼Œç”¨äºè®­ç»ƒæ—¶çš„â€œé‡æ”¾â€
        real_replay_convnet_dataset = SyntheticDataset(real_convnet_audio, real_convnet_text)
        real_replay_imagebind_dataset = SyntheticDataset(real_imagebind_audio, real_imagebind_text)
    
        # ===================== é˜¶æ®µä¸€ï¼šå››ç§è’¸é¦æ–¹æ³• =====================
        print("\nğŸ”¬ é˜¶æ®µä¸€ï¼šæ‰§è¡Œå››ç§æ•°æ®è’¸é¦æ–¹æ³•...")
        
        result1 = self.distiller.distill(real_convnet_audio, real_convnet_text, args, method='avdd', feature_space='convnet')
        result2 = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='imagebind_dc', feature_space='imagebind')
        result3 = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='avdd', feature_space='imagebind')
        result4 = self.distiller.distill(real_convnet_audio, real_convnet_text, args, method='imagebind_dc', feature_space='convnet')
        
        # ===================== é˜¶æ®µäºŒ & ä¸‰ï¼šè®­ç»ƒå¹¶è¯„ä¼°æ£€ç´¢æ¨¡å‹ =====================
        print("\nğŸ”¬ é˜¶æ®µäºŒ & ä¸‰ï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå¹¶ä½¿ç”¨ã€è¯„ä¼°é›†ã€‘è¯„ä¼°...")
        
        # ã€ä¿®æ­£ã€‘distilled_datasets ç»“æ„ï¼ŒåŠ å…¥å¯¹åº”çš„çœŸå®æ•°æ®é‡æ”¾é›†
        distilled_datasets = [
            ("AVDD_ConvNet", result1, (eval_convnet_audio, eval_convnet_text), "convnet", real_replay_convnet_dataset),
            ("ImageBindDC_ImageBind", result2, (eval_imagebind_audio, eval_imagebind_text), "imagebind", real_replay_imagebind_dataset),
            ("AVDD_ImageBind", result3, (eval_imagebind_audio, eval_imagebind_text), "imagebind", real_replay_imagebind_dataset),
            ("ImageBindDC_ConvNet", result4, (eval_convnet_audio, eval_convnet_text), "convnet", real_replay_convnet_dataset)
        ]
        
        final_results = {}
        # ã€ä¿®æ­£ã€‘æ›´æ–°å¾ªç¯ç­¾åä»¥æ¥æ”¶æ–°çš„ real_replay_dataset
        for name, synthetic_data, real_eval_features, student_model_type, real_replay_dataset in distilled_datasets:
            print(f"\n--- æ­£åœ¨å¤„ç†æ–¹æ³•: {name} ---")
            syn_dataset = SyntheticDataset(synthetic_data['synthetic_audio'], synthetic_data['synthetic_text'])
            
            # ã€ä¿®æ­£ã€‘å°†çœŸå®ç‰¹å¾é‡æ”¾é›†ä¼ é€’ç»™ trainer
            trained_model = self.trainer.train(
                syn_dataset=syn_dataset, 
                real_feature_dataset=real_replay_dataset, # <-- å…³é”®ä¿®æ”¹
                args=args, 
                student_model_type=student_model_type,
                feature_dim=512
            )
            
            eval_audio_features, eval_text_features = real_eval_features
            eval_results = self.evaluator.evaluate(trained_model, eval_audio_features, eval_text_features)
            final_results[name] = eval_results

        # ===================== é˜¶æ®µå››ï¼šç»“æœæ±‡æ€» =====================
        print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”çŸ©é˜µï¼ˆRecall@1/5/10/20ï¼‰")
        print("=" * 110)
        
        print(f"{'æ–¹æ³•':<25} "
              f"{'R@1_A2T':<8} {'R@1_T2A':<8} "
              f"{'R@5_A2T':<8} {'R@5_T2A':<8} "
              f"{'R@10_A2T':<9} {'R@10_T2A':<9} "
              f"{'R@20_A2T':<9} {'R@20_T2A':<9}")
        print("-" * 110)
        
        for name, result in final_results.items():
            print(f"{name:<25} "
                  f"{result['R@1_a2t']:<8.4f} {result['R@1_t2a']:<8.4f} "
                  f"{result['R@5_a2t']:<8.4f} {result['R@5_t2a']:<8.4f} "
                  f"{result['R@10_a2t']:<9.4f} {result['R@10_t2a']:<9.4f} "
                  f"{result['R@20_a2t']:<9.4f} {result['R@20_t2a']:<9.4f}")
        # ä¿å­˜å®Œæ•´ç»“æœ
        final_report = {
            'args': vars(args),
            'results': final_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        filename = f'clotho_experiment_ipc{args.ipc}_seed{args.seed}_results.pth'
        torch.save(final_report, filename)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {filename}")
        
        return final_results
# ===================== ä¸»å…¥å£ =====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clotho æ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")

    # --- è·¯å¾„ä¸æ•°æ®å‚æ•° ---
    parser.add_argument('--base_path', type=str, default='/root/autodl-tmp/audio-visual-master/data/Clotho/', help='Clothoæ•°æ®é›†çš„æ ¹ç›®å½•')
    parser.add_argument('--dev_audio_dir_name', type=str, default='development', help='å¼€å‘é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--eval_audio_dir_name', type=str, default='evaluation', help='è¯„ä¼°é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--dev_captions_name', type=str, default='clotho_captions_development.csv', help='å¼€å‘é›†å­—å¹•æ–‡ä»¶å')
    parser.add_argument('--eval_captions_name', type=str, default='clotho_captions_evaluation.csv', help='è¯„ä¼°é›†å­—å¹•æ–‡ä»¶å')

    # --- æ ¸å¿ƒå®éªŒå‚æ•° ---
    parser.add_argument('--ipc', type=int, default=20, help='æ¯ä¸ªç±»åˆ«è’¸é¦å‡ºçš„åˆæˆæ ·æœ¬æ•°é‡ (num_syn_samples)')
    parser.add_argument('--dev_samples', type=int, default=2893, help='ç”¨äºè’¸é¦çš„å¼€å‘é›†æ ·æœ¬æ•° (num_real_samples)')
    parser.add_argument('--distill_iter', type=int, default=5000, help='è’¸é¦ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--student_epochs', type=int, default=100, help='è®­ç»ƒå­¦ç”Ÿæ£€ç´¢æ¨¡å‹çš„è½®æ•°')
    parser.add_argument('--distill_lr_avdd', type=float, default=0.01, help='[AVDDæ–¹æ³•] çš„è’¸é¦å­¦ä¹ ç‡')
    parser.add_argument('--distill_lr_cfd', type=float, default=0.001, help='[ImageBindDC/CFDæ–¹æ³•] çš„è’¸é¦å­¦ä¹ ç‡')
    parser.add_argument('--student_lr', type=float, default=0.001, help='å­¦ç”Ÿæ¨¡å‹è®­ç»ƒé˜¶æ®µçš„å­¦ä¹ ç‡')
    # åœ¨ "æ ¸å¿ƒå®éªŒå‚æ•°" éƒ¨åˆ†
    parser.add_argument('--student_batch_size', type=int, default=16, help='å­¦ç”Ÿæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤§å° (ä¸€åŠçœŸå®, ä¸€åŠåˆæˆ)')
    # --- AVDD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_icm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤±(ICM)çš„æƒé‡')
    parser.add_argument('--lam_cgm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤±(CGM)çš„æƒé‡')
    
    # --- ImageBindDC/CFD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_cross', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')
    parser.add_argument('--lam_joint', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è”åˆåˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')

    # --- å¯å¤ç°æ€§ä¸ç›‘æ§ ---
    parser.add_argument('--seed', type=int, default=42, help='å…¨å±€éšæœºç§å­')
    parser.add_argument('--num_runs', type=int, default=1, help='ä½¿ç”¨ä¸åŒéšæœºç§å­é‡å¤å®éªŒçš„æ¬¡æ•°')
    parser.add_argument('--wandb_project', type=str, default="clotho_distillation_final", help='Wandbé¡¹ç›®åç§°')
    parser.add_argument('--wandb_entity', type=str, default=None, help='Wandbçš„å®ä½“/ç”¨æˆ·å (å¯é€‰)')
    parser.add_argument('--disable_wandb', action='store_true', help='å¦‚æœè®¾ç½®, åˆ™ç¦ç”¨wandbæ—¥å¿—')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    args = parser.parse_args()

    # ä¸»å¾ªç¯ï¼Œç”¨äºå¤šæ¬¡éšæœºç§å­å®éªŒ
    for i in range(args.num_runs):
        current_seed = args.seed + i
        print(f"\n{'='*30}  è¿è¡Œç¬¬ {i+1}/{args.num_runs} æ¬¡, éšæœºç§å­: {current_seed}  {'='*30}\n")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(current_seed)
        np.random.seed(current_seed)

        # æ‹¼æ¥å®Œæ•´çš„è·¯å¾„
        DEV_AUDIO_DIR = os.path.join(args.base_path, args.dev_audio_dir_name)
        DEV_CAPTIONS_FILE = os.path.join(args.base_path, args.dev_captions_name)
        EVAL_AUDIO_DIR = os.path.join(args.base_path, args.eval_audio_dir_name)
        EVAL_CAPTIONS_FILE = os.path.join(args.base_path, args.eval_captions_name)

        # æ£€æŸ¥è·¯å¾„
        if not all(os.path.exists(p) for p in [DEV_AUDIO_DIR, DEV_CAPTIONS_FILE]):
            print(f"é”™è¯¯ï¼šè¯·ç¡®ä¿å¼€å‘é›†æ•°æ®è·¯å¾„éƒ½å­˜åœ¨ï¼")
            print(f"  - å¼€å‘é›†éŸ³é¢‘: {DEV_AUDIO_DIR}")
            print(f"  - å¼€å‘é›†å­—å¹•: {DEV_CAPTIONS_FILE}")
            # è¯„ä¼°é›†è·¯å¾„çš„æ£€æŸ¥å¯ä»¥æ”¾åˆ°å®éªŒæµç¨‹å†…éƒ¨ï¼Œå› ä¸ºå¯èƒ½åªåšè’¸é¦ä¸åšè¯„ä¼°
            continue # å¦‚æœå¼€å‘é›†ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡æœ¬æ¬¡è¿è¡Œ

        # åˆå§‹åŒ–Wandb
        if not args.disable_wandb:
            wandb.init(
                project=args.wandb_project,
                entity=args.wandb_entity,
                config=args,
                name=f"run_{i+1}_seed_{current_seed}",
                reinit=True # å…è®¸å¤šæ¬¡åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­åˆå§‹åŒ–
            )
    
        # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
        experiment = ClothoExperiment(
            dev_audio_dir=DEV_AUDIO_DIR,
            dev_captions_file=DEV_CAPTIONS_FILE,
            eval_audio_dir=EVAL_AUDIO_DIR,
            eval_captions_file=EVAL_CAPTIONS_FILE,
            device=device
        )
        experiment.run_complete_experiment(args) # å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™å®éªŒä¸»å‡½æ•°

        if not args.disable_wandb:
            wandb.finish()