#!/usr/bin/env python3
"""
Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ
éµå¾ªæŠ€æœ¯è§„æ ¼ä¹¦çš„å®Œæ•´å››é˜¶æ®µå®ç°
"""

import os
import json
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from typing import Dict, Tuple, List
import argparse
import wandb
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType

import sys
# å‡è®¾æ‚¨çš„é¡¹ç›®æ ¹ç›®å½•æ˜¯ 'audio-visual-mater'
PROJECT_ROOT = '/autodl-tmp/audio-visual-mater/'
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# ä»æ‚¨çš„æœ¬åœ°æ–‡ä»¶å¤¹å¯¼å…¥å¿…è¦çš„æ¨¡å—
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType
from nets.imagebind import data # ã€å…³é”®ã€‘ä»æ‚¨çš„netsæ–‡ä»¶å¤¹å¯¼å…¥data



class SyntheticDataset(Dataset):
    """ç”¨äºå·²è’¸é¦çš„åˆæˆç‰¹å¾çš„æ•°æ®é›†"""
    def __init__(self, synthetic_audio: torch.Tensor, synthetic_text: torch.Tensor):
        self.audio = synthetic_audio
        self.text = synthetic_text

    def __len__(self):
        return len(self.audio)

    def __getitem__(self, idx):
        return {
            'audio': self.audio[idx],
            'text': self.text[idx],
            'label': idx
        }

# æ·»åŠ è¿™ä¸ªç¼ºå¤±çš„å‡½æ•° (å®ƒä¾èµ–äºä¸€ä¸ªä¿®æ”¹ç‰ˆçš„RawClothoDatasetï¼Œè¿™é‡Œä¸€å¹¶æä¾›)
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
from tqdm import tqdm

class RawClothoDataset(Dataset):
    def __init__(self,
                 audio_dir: str,
                 captions_df: pd.DataFrame,
                 vocab: Dict,
                 sr: int = 22050,
                 n_mels: int = 64,
                 max_time_steps: int = 1024):
        self.audio_dir = audio_dir
        self.captions_df = captions_df
        self.vocab = vocab
        self.sr = sr
        self.n_mels = n_mels
        self.max_time_steps = max_time_steps

    def __len__(self):
        return len(self.captions_df)

    def __getitem__(self, idx):
        row = self.captions_df.iloc[idx]
        audio_path = os.path.join(self.audio_dir, row['file_name'])

        # 1. åŠ è½½éŸ³é¢‘å¹¶è®¡ç®—æ¢…å°”é¢‘è°±å›¾
        y, _ = librosa.load(audio_path, sr=self.sr)
        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)
        mel_spec_db = librosa.power_to_db(mel_spec)          # shape: (n_mels, T)

        # 2. ç»Ÿä¸€æ—¶é—´ç»´åº¦ï¼špadding æˆ–æˆªæ–­
        if mel_spec_db.shape[1] < self.max_time_steps:
            pad_width = self.max_time_steps - mel_spec_db.shape[1]
            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mel_spec_db = mel_spec_db[:, :self.max_time_steps]

        audio_mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # shape: (1, n_mels, T_fixed)

        # 3. æ–‡æœ¬åºåˆ—è½¬ ID
        caption_string = row['caption_1']
        words = caption_string.lower().split()
        word_ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]
        text_ids_tensor = torch.tensor(word_ids, dtype=torch.long)

        return {
            'audio_mel': audio_mel_tensor,
            'text_ids': text_ids_tensor,
            'audio_path': audio_path,
            'raw_caption': caption_string
        }
def build_vocab(captions: List[str], min_freq=5) -> Dict:
    word_counts = {}
    for caption in captions:
        for word in caption.lower().split():
            word_counts[word] = word_counts.get(word, 0) + 1

    vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items()) if count >= min_freq}
    vocab['<pad>'] = 0
    vocab['<unk>'] = 1
    return vocab

def collate_fn(batch):
    # ä¸º ConvNetGRU å¤„ç†æ•°æ®
    audio_mels = [item['audio_mel'] for item in batch]
    text_ids = [item['text_ids'] for item in batch]
    texts_padded = pad_sequence(text_ids, batch_first=True, padding_value=0)
    audios_stacked = torch.stack(audio_mels)

    # ä¸º ImageBind æ”¶é›†åŸå§‹æ•°æ® (ä¿æŒä¸ºList)
    audio_paths = [item['audio_path'] for item in batch]
    raw_captions = [item['raw_caption'] for item in batch]
    
    return {
        'audio_mel': audios_stacked,
        'text_ids': texts_padded,
        'audio_path': audio_paths,
        'raw_caption': raw_captions
    }
    
def get_high_performance_dataloader(audio_dir: str, captions_file: str, batch_size: int, num_samples: int, vocab: Dict = None) -> Tuple[DataLoader, Dict]:
    """
    åˆ›å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„Dataloaderã€‚
    æ–°å¢åŠŸèƒ½ï¼šå¯ä»¥æ¥æ”¶ä¸€ä¸ªå·²æœ‰çš„vocabï¼Œå¦‚æœvocabä¸ºNoneï¼Œåˆ™ä¼šè‡ªå·±æ„å»ºã€‚
    """
    df = pd.read_csv(captions_file)
    if num_samples:
        # æ³¨æ„ï¼šå¯¹äºè¯„ä¼°é›†ï¼Œæˆ‘ä»¬é€šå¸¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼Œå› æ­¤num_samplesåº”ä¸ºNone
        df = df.sample(n=num_samples, random_state=42).reset_index(drop=True)

    if vocab is None:
        print("æœªæä¾›è¯æ±‡è¡¨ï¼Œæ­£åœ¨ä»å­—å¹•ä¸­æ„å»º...")
        vocab = build_vocab(df['caption_1'].tolist())
    else:
        print("ä½¿ç”¨å·²æä¾›çš„è¯æ±‡è¡¨ã€‚")

    dataset = RawClothoDataset(audio_dir, df, vocab)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)
    
    # å§‹ç»ˆè¿”å›ä½¿ç”¨çš„dataloaderå’Œvocab
    return dataloader, vocab

def precompute_real_features(dataloader: DataLoader, feature_extractor, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
    if isinstance(feature_extractor, nn.Module):
        feature_extractor.to(device)
        feature_extractor.eval()
    
    all_audio_features, all_text_features = [], []
    print(f"ğŸš€ å¼€å§‹é¢„è®¡ç®—çœŸå®ç‰¹å¾ï¼Œä½¿ç”¨æ¨¡å‹: {feature_extractor.__class__.__name__}")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æ­£åœ¨æå–ç‰¹å¾"):
            if isinstance(feature_extractor, ImageBindExtractor):
                audio_feats = feature_extractor.extract_audio_features(batch['audio_path'])
                text_feats = feature_extractor.extract_text_features(batch['raw_caption'])
            else: 
                audio_data = batch['audio_mel'].to(device)
                text_data = batch['text_ids'].to(device)
                audio_feats = feature_extractor.forward_audio(audio_data)
                text_feats = feature_extractor.forward_text(text_data)

            all_audio_features.append(audio_feats.cpu())
            all_text_features.append(text_feats.cpu())
            
    print("âœ… çœŸå®ç‰¹å¾é¢„è®¡ç®—å®Œæˆï¼")
    return torch.cat(all_audio_features, dim=0).to(device), torch.cat(all_text_features, dim=0).to(device)
# ===================== ç‰¹å¾æå–å™¨ =====================

class ConvNetGRU(nn.Module):
    """ConvNet+GRUåŒå¡”æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 256, feature_dim: int = 512):
        super().__init__()
        
        # éŸ³é¢‘å¡” (è¿™éƒ¨åˆ†ä¿æŒä¸å˜)
        self.audio_tower = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, feature_dim)
        )
        
        # æ–‡æœ¬å¡” (é‡æ„ä¸ºç‹¬ç«‹çš„å±‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªSequential)
        self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.text_gru = nn.GRU(embedding_dim, feature_dim, batch_first=True)
    
    def forward(self, x):
        """ç®€åŒ–forwardå‡½æ•°ç”¨äºç‰¹å¾æå–"""
        return x  # ç›´æ¥è¿”å›è¾“å…¥ï¼Œå› ä¸ºè¿™é‡Œåªæ˜¯ç‰¹å¾è½¬æ¢
    
    def forward_audio(self, audio):
        return F.normalize(self.audio_tower(audio), dim=-1)
    
    def forward_text(self, text: torch.Tensor) -> torch.Tensor:
        text = torch.clamp(text, max=self.text_embedding.num_embeddings - 1)
        # 1. å°†è¾“å…¥çš„è¯æ±‡IDåºåˆ—é€šè¿‡Embeddingå±‚è½¬æ¢ä¸ºç‰¹å¾å‘é‡åºåˆ—
        embedded_text = self.text_embedding(text)
        
        # 2. å°†ç‰¹å¾å‘é‡åºåˆ—è¾“å…¥GRU
        # GRUçš„ç¬¬äºŒä¸ªè¿”å›å€¼h_næ˜¯æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(num_layers, batch_size, hidden_size)
        _, h_n = self.text_gru(embedded_text)
        
        # 3. h_n.squeeze(0)å°†å½¢çŠ¶å˜ä¸º(batch_size, hidden_size)ï¼Œè¿™é€šå¸¸è¢«ç”¨ä½œæ•´ä¸ªåºåˆ—çš„ç‰¹å¾è¡¨ç¤º
        text_feat = h_n.squeeze(0)
        
        # 4. å½’ä¸€åŒ–ç‰¹å¾
        return F.normalize(text_feat, dim=-1)

class ImageBindExtractor:
    """çœŸå®çš„ ImageBind ç‰¹å¾æå–å™¨ (æ¨¡ä»¿æ‚¨çš„æœ¬åœ°é¡¹ç›®)"""
    def __init__(self, pretrained=False, device='cpu'):
        self.device = device
        self.model = None
        self.embed_func = None

        if pretrained:
            print("æ­£åœ¨ä½¿ç”¨æ‚¨æœ¬åœ°çš„ get_network_imagebind å‡½æ•°åŠ è½½æ¨¡å‹...")
            
            class MockArgs:
                def __init__(self):
                    self.arch_frame = 'imagebind'
                    self.arch_classifier = 'ensemble'
                    self.cls_num = 10
                    self.weights_classifier = ''
                    self.input_modality = 'av'

            mock_args = MockArgs()
            
            nets, _ = get_network_imagebind(mock_args)
            net_imagebind, _ = nets
            
            self.model = net_imagebind
            self.model.to(self.device)
            self.model.eval()

            self.embed_func = self.model.module.embed if torch.cuda.device_count() > 1 else self.model.embed
            print("âœ… çœŸå® ImageBind æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def extract_audio_features(self, audio_path_list: List[str]) -> torch.Tensor:
        if self.model is None: raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
        # ã€å…³é”®ã€‘è°ƒç”¨æ‚¨æœ¬åœ°çš„ data.py ä¸­çš„å‡½æ•°
        inputs = { ModalityType.AUDIO: data.load_and_transform_audio_data(audio_path_list, self.device) }
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.AUDIO].detach()

    def extract_text_features(self, raw_caption_list: List[str]) -> torch.Tensor:
        if self.model is None:
            raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
    
        print("raw_caption_list å‰ 3 æ¡ï¼š", raw_caption_list[:3])
    
        # 1ï¸âƒ£ æ‹¿åˆ° token id
        text_tokens = data.load_and_transform_text(raw_caption_list, self.device)
    
        # 2ï¸âƒ£ æ‰“å°æœ€å¤§ idï¼Œçœ‹æ˜¯å¦çœŸçš„ â‰¥ 49408
        max_id = text_tokens.max().item()
        print("max token id =", max_id, " è¶Šç•Œï¼Ÿ", max_id >= 49408)
    
        # 3ï¸âƒ£ å…œåº• clamp
        text_tokens = torch.clamp(text_tokens, max=49407)
    
        inputs = {ModalityType.TEXT: text_tokens}
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.TEXT].detach()

# ===================== æ ‡å‡†Herdingç®—æ³• =====================

class StandardHerdingSelector:
    """æ ‡å‡†Herdingç®—æ³•çš„è§„èŒƒå®ç°"""

    def __init__(self, device='cuda'):
        self.device = device

    def select_coreset(self, features, coreset_size):
        if coreset_size >= features.shape[0]:
            return torch.arange(features.shape[0]), features

        # 1. è®¡ç®—ä¸€æ¬¡å…¨ä½“ç‰¹å¾çš„å‡å€¼
        overall_mean = features.mean(dim=0)

        selected_indices = []
        available_indices = list(range(features.shape[0]))

        # 2. åˆå§‹åŒ–å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
        current_coreset_mean = torch.zeros_like(overall_mean)

        for i in range(coreset_size):
            # 3. è®¡ç®—å½“å‰å‡å€¼ä¸ç›®æ ‡å‡å€¼çš„â€œè¯¯å·®å‘é‡â€
            error_vector = overall_mean - current_coreset_mean

            # 4. åœ¨å‰©ä½™æ ·æœ¬ä¸­ï¼Œå¯»æ‰¾ä¸è¯¯å·®å‘é‡å†…ç§¯æœ€å¤§çš„é‚£ä¸ª
            available_features = features[available_indices]
            projections = torch.matmul(available_features, error_vector)

            best_idx_in_remaining = torch.argmax(projections)
            selected_idx = available_indices.pop(best_idx_in_remaining)

            selected_indices.append(selected_idx)

            # 5. æ›´æ–°å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
            current_coreset_mean = (current_coreset_mean * i + features[selected_idx]) / (i + 1)

        selected_indices = torch.tensor(selected_indices)
        return selected_indices, features[selected_indices]

# ===================== æŸå¤±å‡½æ•° =====================

class AVDDLoss:
    """AVDDæŸå¤±å‡½æ•°"""
    
    def __init__(self, lam_icm=10.0, lam_cgm=10.0):
        self.lam_icm = lam_icm
        self.lam_cgm = lam_cgm
    
    def __call__(self, real_features, synthetic_features):
        """è®¡ç®—AVDDæŸå¤±"""
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features
        
        # 1. åˆ†å¸ƒåŒ¹é…æŸå¤± (DM Loss)
        loss_dm = F.mse_loss(syn_audio.mean(0), real_audio.mean(0)) + \
                 F.mse_loss(syn_text.mean(0), real_text.mean(0))
        
        # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤± (ICM Loss) - ä¿®æ­£ååŒ¹é…ç»Ÿè®¡é‡
        # å‡è®¾è¾“å…¥çš„ç‰¹å¾å·²ç»è¢«å½’ä¸€åŒ–
        real_sim_matrix = torch.matmul(real_audio, real_text.t())
        syn_sim_matrix = torch.matmul(syn_audio, syn_text.t())
        
        loss_icm = F.mse_loss(syn_sim_matrix.mean(), real_sim_matrix.mean()) + \
                   F.mse_loss(syn_sim_matrix.std(), real_sim_matrix.std())
        
        # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤± (CGM Loss)
        real_gap = real_audio.mean(0) - real_text.mean(0)
        syn_gap = syn_audio.mean(0) - syn_text.mean(0)
        loss_cgm = F.mse_loss(syn_gap, real_gap)
        
        return loss_dm + self.lam_icm * loss_icm + self.lam_cgm * loss_cgm
        
class ImageBindDCLoss:
    """
    ImageBindDCæŸå¤±å‡½æ•° - ä¿®æ­£åé‡‡ç”¨çœŸå®çš„CFDå®ç°ã€‚
    """
    def __init__(self, lam_cross=1.0, lam_joint=1.0, num_freqs=4096, device='cuda'):
        self.lam_cross = lam_cross
        self.lam_joint = lam_joint
        self.num_freqs = num_freqs
        self.device = device
        self._t_cache = {} # ç”¨äºç¼“å­˜ä¸åŒç»´åº¦çš„éšæœºé¢‘ç‡å‘é‡t

    def _get_t(self, feature_dim):
        """æ ¹æ®ç‰¹å¾ç»´åº¦è·å–æˆ–ç”Ÿæˆéšæœºé¢‘ç‡å‘é‡t"""
        if feature_dim not in self._t_cache:
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºé¢‘ç‡
            self._t_cache[feature_dim] = torch.randn(feature_dim, self.num_freqs, device=self.device)
        return self._t_cache[feature_dim]

    def compute_cfd(self, feat1: torch.Tensor, feat2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç‰¹å¾å‡½æ•°è·ç¦» (CFD)ã€‚
        å…¬å¼: L2^2(Î¦_x(t), Î¦_y(t))
        """
        feature_dim = feat1.shape[1]
        t = self._get_t(feature_dim) # è·å–éšæœºé¢‘ç‡å‘é‡

        # 1. å°†ç‰¹å¾æŠ•å½±åˆ°éšæœºé¢‘ç‡ä¸Š (t^T * x)
        proj1 = torch.matmul(feat1, t)
        proj2 = torch.matmul(feat2, t)

        # 2. è®¡ç®—ç‰¹å¾å‡½æ•° (é€šè¿‡é‡‡æ ·è¿‘ä¼¼æœŸæœ› E[exp(j * proj)])
        # exp(j*z) = cos(z) + j*sin(z)
        # E[cos(z)] å’Œ E[sin(z)] é€šè¿‡åœ¨batchç»´åº¦ä¸Šæ±‚å‡å€¼æ¥è¿‘ä¼¼
        phi1_real = torch.cos(proj1).mean(dim=0)
        phi1_imag = torch.sin(proj1).mean(dim=0)
        phi2_real = torch.cos(proj2).mean(dim=0)
        phi2_imag = torch.sin(proj2).mean(dim=0)

        # 3. è®¡ç®—ä¸¤ä¸ªå¤æ•°ç‰¹å¾å‡½æ•°ä¹‹é—´çš„L2è·ç¦»çš„å¹³æ–¹
        dist_sq = (phi1_real - phi2_real)**2 + (phi1_imag - phi2_imag)**2
        
        # 4. åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„CFDæŸå¤±
        return dist_sq.sum()

    def __call__(self, real_features, synthetic_features):
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features

        # ç”±äºçœŸå®æ•°æ®é‡è¿œå¤§äºåˆæˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­é‡‡æ ·ä¸€ä¸ªå­é›†æ¥è¿›è¡ŒåŒ¹é…
        num_syn = len(syn_audio)
        rand_idx = torch.randperm(len(real_audio))[:num_syn]
        real_audio_subset = real_audio[rand_idx]
        real_text_subset = real_text[rand_idx]

        # 1. å•æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Uni-modal Distribution Matching)
        loss_uni = self.compute_cfd(real_audio_subset, syn_audio) + \
                   self.compute_cfd(real_text_subset, syn_text)
        
        # 2. è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Cross-modal Distribution Matching)
        # éµå¾ªè®ºæ–‡å…¬å¼ CFD(Real_Audio + Syn_Text, Real_Text + Syn_Audio)
        mix_dist1 = real_audio_subset + syn_text
        mix_dist2 = real_text_subset + syn_audio
        loss_cross = self.compute_cfd(mix_dist1, mix_dist2)
        
        # 3. è”åˆåˆ†å¸ƒåŒ¹é… (Joint Distribution Matching)
        # æ‹¼æ¥ç‰¹å¾ï¼ŒåŒ¹é…å…¶è”åˆåˆ†å¸ƒ
        joint_real = torch.cat([real_audio_subset, real_text_subset], dim=1)
        joint_syn = torch.cat([syn_audio, syn_text], dim=1)
        loss_joint = self.compute_cfd(joint_real, joint_syn)
        
        return loss_uni + self.lam_cross * loss_cross + self.lam_joint * loss_joint
# ===================== æ•°æ®è’¸é¦ =====================

class DataDistillation:
    """æ•°æ®è’¸é¦å¼•æ“"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = StandardHerdingSelector(device)
    
    # åœ¨ DataDistillation ç±»ä¸­
    # åœ¨ DataDistillation ç±»ä¸­
    def distill(self, real_audio_features, real_text_features, args, method='avdd', feature_space='imagebind'):
        """æ ‡å‡†è’¸é¦æµç¨‹ï¼Œç­¾åå·²ä¿®æ”¹ä¸ºæ¥æ”¶args"""
        
        print(f"\nğŸ”¬ å¼€å§‹{method} on {feature_space}è’¸é¦...")
        
        num_syn_samples = args.ipc
        iterations = args.distill_iter
    
        audio_indices, audio_coreset = self.herding.select_coreset(real_audio_features, num_syn_samples)
        text_indices, text_coreset = self.herding.select_coreset(real_text_features, num_syn_samples)
        
        syn_audio = audio_coreset.clone().detach().requires_grad_(True)
        syn_text = text_coreset.clone().detach().requires_grad_(True)
        
        # ã€æœ€ç»ˆä¿®æ­£ã€‘ç¡®ä¿ImageBindDCLossä¹Ÿä½¿ç”¨argsä¸­çš„æƒé‡
        if method == 'avdd':
            loss_fn = AVDDLoss(lam_icm=args.lam_icm, lam_cgm=args.lam_cgm)
        else:  # imagebind_dc
            loss_fn = ImageBindDCLoss(lam_cross=args.lam_cross, lam_joint=args.lam_joint, device=self.device)
        
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=args.distill_lr)
        
        loss_history = []
        
        for iteration in range(iterations):
            loss = loss_fn(
                (real_audio_features, real_text_features),
                (syn_audio, syn_text)
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_history.append(loss.item())
            
            if iteration % 50 == 0 or iteration == iterations - 1:
                if not args.disable_wandb:
                    wandb.log({f"distill_loss/{method}_{feature_space}": loss.item()})
                print(f"è¿­ä»£ {iteration}/{iterations}, æŸå¤±: {loss.item():.6f}")
        
        return {
            'synthetic_audio': syn_audio.detach(),
            'synthetic_text': syn_text.detach(),
            'loss_history': loss_history
        }
# ===================== æ£€ç´¢æ¨¡å‹è®­ç»ƒ =====================
class ImageBindAsRetriever(nn.Module):
    """å†»ç»“ ImageBind + å¼ºåŒ–ç‰ˆè½»é‡æ˜ å°„å¤´ï¼Œç”¨äºåˆæˆç‰¹å¾è®­ç»ƒ"""

    def __init__(self, feature_dim=512, device='cuda'):
        super().__init__()
        self.device = device

        # æ˜ å°„å¤´ï¼ˆè¾“å…¥ç»´åº¦å›ºå®šä¸º 1024ï¼‰ï¼Œè¿›ä¸€æ­¥å¼ºåŒ–
        self.audio_head = nn.Sequential(
            nn.Linear(1024, 2048),  # å¢åŠ ä¸­é—´å±‚å®½åº¦
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 2048),  # æ–°å¢ä¸€ä¸ªéšè—å±‚ï¼Œå¹¶ä¿æŒå®½åº¦
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 1024),  # å†ä¸€ä¸ªéšè—å±‚
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim) # è¾“å‡ºåˆ°ç›®æ ‡ç‰¹å¾ç»´åº¦
        )
        self.text_head = nn.Sequential(
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim)
        )

    def forward(self, audio, text):
        audio = audio.to(self.device)
        text  = text.to(self.device)
        audio_feat = F.normalize(self.audio_head(audio), dim=-1)
        text_feat = F.normalize(self.text_head(text), dim=-1)
        return audio_feat, text_feat

class RetrievalModel(nn.Module):
    """æ£€ç´¢æ¨¡å‹ - å¼ºåŒ–ç‰ˆ"""
    
    def __init__(self, input_dim, feature_dim=512):
        super().__init__()
        
        self.audio_encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),  # è¾“å…¥å±‚ï¼Œå®½åº¦å¢å¤§
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 2048),   # æ–°å¢éšè—å±‚ï¼Œå¹¶å¢åŠ å®½åº¦
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 1024),   # å†ä¸€ä¸ªéšè—å±‚
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim) # è¾“å‡ºåˆ°ç›®æ ‡ç‰¹å¾ç»´åº¦
        )
        self.text_encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, feature_dim)
        )
    def forward(self, audio, text):
        audio_feat = F.normalize(self.audio_encoder(audio), dim=-1)
        text_feat = F.normalize(self.text_encoder(text), dim=-1)
        return audio_feat, text_feat

class RetrievalTrainer:
    """æ£€ç´¢æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, device='cuda'):
        self.device = device

    def info_nce_loss(self, audio_feat, text_feat, temperature=0.07):
        logits = torch.matmul(audio_feat, text_feat.t()) / temperature
        labels = torch.arange(len(audio_feat), device=self.device)
        loss_a2t = F.cross_entropy(logits, labels)
        loss_t2a = F.cross_entropy(logits.t(), labels)
        return (loss_a2t + loss_t2a) / 2

    # è¿™é‡Œæ˜¯ä¿®æ”¹çš„éƒ¨åˆ†
    def train(self, syn_dataset, input_dim, args, feature_dim=512, model_type='ImageBindAsRetriever'): # <-- æ–°å¢ model_type å‚æ•°
        print(f"\nğŸ¯ è®­ç»ƒå­¦ç”Ÿæ¨¡å‹... (epochs: {args.student_epochs}, lr: {args.student_lr}, Model Type: {model_type})") # <-- æ‰“å°æ¨¡å‹ç±»å‹
        dataloader = DataLoader(syn_dataset, batch_size=8, shuffle=True)

        if model_type == 'ImageBindAsRetriever':
            model = ImageBindAsRetriever(feature_dim=feature_dim, device=self.device).to(self.device)
        elif model_type == 'RetrievalModel':
            model = RetrievalModel(input_dim=input_dim, feature_dim=feature_dim).to(self.device)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
        
        optimizer = torch.optim.Adam(model.parameters(), lr=args.student_lr)

        model.train()
        for epoch in range(args.student_epochs):
            total_loss = 0
            for batch in dataloader:
                audio, text = batch['audio'].to(self.device), batch['text'].to(self.device)
                audio_feat, text_feat = model(audio, text)

                loss = self.info_nce_loss(audio_feat, text_feat)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if epoch % 10 == 0 or epoch == args.student_epochs - 1:
                print(f"è®­ç»ƒè½®æ¬¡ {epoch}/{args.student_epochs}, å¹³å‡æŸå¤±: {total_loss/len(dataloader):.4f}")

        return model

# ===================== æ ‡å‡†è¯„ä¼° =====================

class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨"""

    def __init__(self, device='cuda'):
        self.device = device

    # è¿™é‡Œæ˜¯ä¿®æ”¹çš„éƒ¨åˆ†
    def evaluate(self,
                 model,
                 real_audio_features: torch.Tensor, # <-- æ˜ç¡®ç±»å‹æç¤ºä¸ºå¼ é‡
                 real_text_features: torch.Tensor): # <-- æ˜ç¡®ç±»å‹æç¤ºä¸ºå¼ é‡
        """
        æ ‡å‡† Recall@K è¯„ä¼°ã€‚
        ç°åœ¨ç»Ÿä¸€æ¥æ”¶é¢„è®¡ç®—çš„ç‰¹å¾å¼ é‡ã€‚
        """

        print("\nğŸ¯ æ ‡å‡†æ£€ç´¢è¯„ä¼°...")

        model.eval()
        with torch.no_grad():
            # ---------- ç»Ÿä¸€è·å–ç‰¹å¾ ----------
            if real_audio_features is None or real_text_features is None:
                raise ValueError("è¯„ä¼°æ¨¡å‹æ—¶å¿…é¡»æä¾› real_audio_features å’Œ real_text_features")
            
            # ç»Ÿä¸€è°ƒç”¨æ¨¡å‹çš„ forward æ–¹æ³•ï¼Œä¼ å…¥ç‰¹å¾å¼ é‡
            audio_feat, text_feat = model(real_audio_features, real_text_features)

            audio_feat = audio_feat.to(self.device)
            text_feat  = text_feat.to(self.device)

            # ---------- è®¡ç®—ç›¸ä¼¼åº¦ ----------
            sim_matrix = torch.matmul(audio_feat, text_feat.t())

            results = {}
            for k in [1, 5, 10, 20]:
                # A2T
                _, topk_a2t = torch.topk(sim_matrix, k=k, dim=1)
                recall_a2t = torch.mean(
                    (torch.arange(len(audio_feat), device=self.device).unsqueeze(1) == topk_a2t).any(dim=1).float()
                ).item()

                # T2A
                _, topk_t2a = torch.topk(sim_matrix.t(), k=k, dim=1)
                recall_t2a = torch.mean(
                    (torch.arange(len(text_feat), device=self.device).unsqueeze(1) == topk_t2a).any(dim=1).float()
                ).item()

                results[f'R@{k}_a2t'] = recall_a2t
                results[f'R@{k}_t2a'] = recall_t2a

        return results
# ===================== ä¸»å®éªŒæµç¨‹ =====================

class ClothoExperiment:
    """Clothoæ•°æ®é›†å®Œæ•´å®éªŒ"""
    
    def __init__(self, dev_audio_dir: str, dev_captions_file: str, eval_audio_dir: str, eval_captions_file: str, device='cuda'):
            self.device = device
            # å¼€å‘é›†è·¯å¾„
            self.dev_audio_dir = dev_audio_dir
            self.dev_captions_file = dev_captions_file
            # è¯„ä¼°é›†è·¯å¾„
            self.eval_audio_dir = eval_audio_dir
            self.eval_captions_file = eval_captions_file
            
            self.vocab = None # ç”¨äºå­˜å‚¨è¯æ±‡è¡¨
            self.distiller = DataDistillation(device)
            self.trainer = RetrievalTrainer(device)
            self.evaluator = StandardEvaluator(device)
    
    # åœ¨ ClothoExperiment ç±»ä¸­
    def run_complete_experiment(self, args):
        """
        æ‰§è¡Œå®Œæ•´å››é˜¶æ®µå®éªŒã€‚
        å‡½æ•°ç­¾åå·²ä¿®æ”¹ä¸ºæ¥æ”¶argparseè§£æå‡ºçš„å‚æ•°å¯¹è±¡ã€‚
        """
        print("ğŸ† Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")
        print("=" * 80)
        
        # ===================== é˜¶æ®µé›¶ï¼šå‡†å¤‡æ•°æ®å’Œç‰¹å¾æå–å™¨ =====================
        print("ğŸ“Š é˜¶æ®µé›¶ï¼šåŠ è½½æ•°æ®å¹¶å‡†å¤‡ç‰¹å¾æå–å™¨...")
    
        # 1. åŠ è½½å¼€å‘é›†æ•°æ®ï¼Œå¹¶ã€æ„å»ºã€‘è¯æ±‡è¡¨
        print("\n--- åŠ è½½å¼€å‘é›† (ç”¨äºè’¸é¦) ---")
        dev_dataloader, self.vocab = get_high_performance_dataloader(
            audio_dir=self.dev_audio_dir,
            captions_file=self.dev_captions_file,
            batch_size=32,
            num_samples=args.dev_samples
        )
        
        # 2. åŠ è½½è¯„ä¼°é›†æ•°æ®ï¼Œå¹¶ã€å¤ç”¨ã€‘å¼€å‘é›†çš„è¯æ±‡è¡¨
        print("\n--- åŠ è½½è¯„ä¼°é›† (ç”¨äºæœ€ç»ˆæµ‹è¯•) ---")
        eval_dataloader, _ = get_high_performance_dataloader(
            audio_dir=self.eval_audio_dir,
            captions_file=self.eval_captions_file,
            batch_size=32,
            num_samples=None, # åŠ è½½å…¨éƒ¨è¯„ä¼°é›†æ ·æœ¬
            vocab=self.vocab  # å…³é”®ï¼šä¼ å…¥å¼€å‘é›†æ„å»ºçš„è¯æ±‡è¡¨
        )
    
        # 3. å‡†å¤‡ä¸¤ç§ä¸åŒçš„ç‰¹å¾æå–å™¨
        imagebind_extractor = ImageBindExtractor(pretrained=True, device=self.device)
        convnet_extractor = ConvNetGRU(vocab_size=len(self.vocab), feature_dim=512).to(self.device)
    
        # ===================== é˜¶æ®µé›¶(ç»­)ï¼šç‰¹å¾é¢„è®¡ç®— =====================
        print("\n--- åœ¨å¼€å‘é›†ä¸Šé¢„è®¡ç®—ã€è’¸é¦ç”¨ã€‘ç‰¹å¾ ---")
        real_imagebind_audio, real_imagebind_text = precompute_real_features(dev_dataloader, imagebind_extractor, self.device)
        real_convnet_audio, real_convnet_text = precompute_real_features(dev_dataloader, convnet_extractor, self.device)
        
        print("\n--- åœ¨è¯„ä¼°é›†ä¸Šé¢„è®¡ç®—ã€è¯„ä¼°ç”¨ã€‘ç‰¹å¾ ---")
        eval_imagebind_audio, eval_imagebind_text = precompute_real_features(eval_dataloader, imagebind_extractor, self.device)
        eval_convnet_audio, eval_convnet_text = precompute_real_features(eval_dataloader, convnet_extractor, self.device)
        
        # ===================== é˜¶æ®µä¸€ï¼šå››ç§è’¸é¦æ–¹æ³• (åœ¨å¼€å‘é›†ç‰¹å¾ä¸Šè¿›è¡Œ) =====================
        print("\nğŸ”¬ é˜¶æ®µä¸€ï¼šæ‰§è¡Œå››ç§æ•°æ®è’¸é¦æ–¹æ³•...")
        
        # ã€ä¿®æ­£ã€‘å°†argså¯¹è±¡ä¼ é€’ç»™distillæ–¹æ³•ï¼Œä»¥ä½¿ç”¨å…¶ä¸­å®šä¹‰çš„å­¦ä¹ ç‡å’ŒæŸå¤±æƒé‡
        result1 = self.distiller.distill(real_convnet_audio, real_convnet_text, args, method='avdd', feature_space='convnet')
        result2 = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='imagebind_dc', feature_space='imagebind')
        result3 = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='avdd', feature_space='imagebind')
        result4 = self.distiller.distill(real_convnet_audio, real_convnet_text, args, method='imagebind_dc', feature_space='convnet')
        
        # ===================== é˜¶æ®µäºŒ & ä¸‰ï¼šè®­ç»ƒå¹¶ã€åœ¨è¯„ä¼°é›†ä¸Šã€‘è¯„ä¼°æ£€ç´¢æ¨¡å‹ =====================
        print("\nğŸ”¬ é˜¶æ®µäºŒ & ä¸‰ï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå¹¶ä½¿ç”¨ã€è¯„ä¼°é›†ã€‘è¯„ä¼°...")
        
        distilled_datasets = [
            ("AVDD_ConvNet", result1, (eval_convnet_audio, eval_convnet_text), 'convnet'), # <-- æ–°å¢ä¸€ä¸ªå…ƒç´ æ¥è¡¨ç¤ºç‰¹å¾ç©ºé—´
            ("ImageBindDC_ImageBind", result2, (eval_imagebind_audio, eval_imagebind_text), 'imagebind'), # <-- æ–°å¢
            ("AVDD_ImageBind", result3, (eval_imagebind_audio, eval_imagebind_text), 'imagebind'), # <-- æ–°å¢
            ("ImageBindDC_ConvNet", result4, (eval_convnet_audio, eval_convnet_text), 'convnet') # <-- æ–°å¢
        ]
        
        final_results = {}
        # å¾ªç¯ç­¾åä¹Ÿè¦æ›´æ–°ï¼Œä»¥æ¥æ”¶æ–°çš„ feature_space å‚æ•°
        for name, synthetic_data, real_eval_features, current_feature_space in distilled_datasets: # <-- å¾ªç¯ç­¾åæ›´æ–°
            print(f"\n--- æ­£åœ¨å¤„ç†æ–¹æ³•: {name} ---")
            syn_dataset = SyntheticDataset(synthetic_data['synthetic_audio'], synthetic_data['synthetic_text'])
            
            input_dim = synthetic_data['synthetic_audio'].shape[1]
            
            # æ ¹æ®å½“å‰çš„ç‰¹å¾ç©ºé—´é€‰æ‹©å­¦ç”Ÿæ¨¡å‹ç±»å‹
            if current_feature_space == 'imagebind': # <--- ä¿®æ”¹åçš„åˆ¤æ–­é€»è¾‘
                student_model_type = 'ImageBindAsRetriever'
            elif current_feature_space == 'convnet': # <--- ä¿®æ”¹åçš„åˆ¤æ–­é€»è¾‘
                student_model_type = 'RetrievalModel'
            else:
                raise ValueError(f"æœªçŸ¥ç‰¹å¾ç©ºé—´: {current_feature_space}")

            # ã€ä¿®æ­£ã€‘å°†argså¯¹è±¡ä¼ é€’ç»™trainæ–¹æ³•ï¼Œä»¥ä½¿ç”¨å…¶ä¸­å®šä¹‰çš„å­¦ä¹ ç‡å’Œepochs
            # åŒæ—¶ä¹Ÿä¼ é€’é€‰æ‹©çš„å­¦ç”Ÿæ¨¡å‹ç±»å‹
            trained_model = self.trainer.train(
                syn_dataset, 
                input_dim, # input_dim ä¼šæ˜¯ 512 (ConvNet) æˆ– 1024 (ImageBind)
                args, 
                feature_dim=512, # å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºç‰¹å¾ç»´åº¦ï¼Œè¿™é€šå¸¸æ˜¯å›ºå®šçš„
                model_type=student_model_type 
            )
            
            # å…³é”®ï¼šä½¿ç”¨ä»æœªè§è¿‡çš„è¯„ä¼°é›†ç‰¹å¾è¿›è¡Œè¯„ä¼°
            eval_audio_features, eval_text_features = real_eval_features
            
            # ç”±äº StandardEvaluator.evaluate å·²ç»ç»Ÿä¸€æ¥å£ä¸ºæ¥æ”¶ç‰¹å¾å¼ é‡ï¼Œ
            # è¿™é‡Œç›´æ¥è°ƒç”¨å³å¯ï¼Œä¸å†éœ€è¦ isinstane åˆ¤æ–­
            eval_results = self.evaluator.evaluate(trained_model, eval_audio_features, eval_text_features)
            
            final_results[name] = eval_results
        
        # ===================== é˜¶æ®µå››ï¼šç»“æœæ±‡æ€» =====================
        print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”çŸ©é˜µï¼ˆRecall@1/5/10/20ï¼‰")
        print("=" * 110)
        
        print(f"{'æ–¹æ³•':<25} "
              f"{'R@1_A2T':<8} {'R@1_T2A':<8} "
              f"{'R@5_A2T':<8} {'R@5_T2A':<8} "
              f"{'R@10_A2T':<9} {'R@10_T2A':<9} "
              f"{'R@20_A2T':<9} {'R@20_T2A':<9}")
        print("-" * 110)
        
        for name, result in final_results.items():
            print(f"{name:<25} "
                  f"{result['R@1_a2t']:<8.4f} {result['R@1_t2a']:<8.4f} "
                  f"{result['R@5_a2t']:<8.4f} {result['R@5_t2a']:<8.4f} "
                  f"{result['R@10_a2t']:<9.4f} {result['R@10_t2a']:<9.4f} "
                  f"{result['R@20_a2t']:<9.4f} {result['R@20_t2a']:<9.4f}")
        # ä¿å­˜å®Œæ•´ç»“æœ
        final_report = {
            'args': vars(args),
            'results': final_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        filename = f'clotho_experiment_ipc{args.ipc}_seed{args.seed}_results.pth'
        torch.save(final_report, filename)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {filename}")
        
        return final_results
# ===================== ä¸»å…¥å£ =====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clotho æ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")

    # --- è·¯å¾„ä¸æ•°æ®å‚æ•° ---
    parser.add_argument('--base_path', type=str, default='/root/autodl-tmp/audio-visual-master/data/Clotho/', help='Clothoæ•°æ®é›†çš„æ ¹ç›®å½•')
    parser.add_argument('--dev_audio_dir_name', type=str, default='development', help='å¼€å‘é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--eval_audio_dir_name', type=str, default='evaluation', help='è¯„ä¼°é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--dev_captions_name', type=str, default='clotho_captions_development.csv', help='å¼€å‘é›†å­—å¹•æ–‡ä»¶å')
    parser.add_argument('--eval_captions_name', type=str, default='clotho_captions_evaluation.csv', help='è¯„ä¼°é›†å­—å¹•æ–‡ä»¶å')

    # --- æ ¸å¿ƒå®éªŒå‚æ•° ---
    parser.add_argument('--ipc', type=int, default=20, help='æ¯ä¸ªç±»åˆ«è’¸é¦å‡ºçš„åˆæˆæ ·æœ¬æ•°é‡ (num_syn_samples)')
    parser.add_argument('--dev_samples', type=int, default=2893, help='ç”¨äºè’¸é¦çš„å¼€å‘é›†æ ·æœ¬æ•° (num_real_samples)')
    parser.add_argument('--distill_iter', type=int, default=5000, help='è’¸é¦ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--student_epochs', type=int, default=100, help='è®­ç»ƒå­¦ç”Ÿæ£€ç´¢æ¨¡å‹çš„è½®æ•°')
    parser.add_argument('--distill_lr', type=float, default=0.001, help='è’¸é¦é˜¶æ®µä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡')
    parser.add_argument('--student_lr', type=float, default=0.001, help='å­¦ç”Ÿæ¨¡å‹è®­ç»ƒé˜¶æ®µçš„å­¦ä¹ ç‡')

    # --- AVDD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_icm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤±(ICM)çš„æƒé‡')
    parser.add_argument('--lam_cgm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤±(CGM)çš„æƒé‡')
    
    # --- ImageBindDC/CFD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_cross', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')
    parser.add_argument('--lam_joint', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è”åˆåˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')

    # --- å¯å¤ç°æ€§ä¸ç›‘æ§ ---
    parser.add_argument('--seed', type=int, default=42, help='å…¨å±€éšæœºç§å­')
    parser.add_argument('--num_runs', type=int, default=1, help='ä½¿ç”¨ä¸åŒéšæœºç§å­é‡å¤å®éªŒçš„æ¬¡æ•°')
    parser.add_argument('--wandb_project', type=str, default="clotho_distillation_final", help='Wandbé¡¹ç›®åç§°')
    parser.add_argument('--wandb_entity', type=str, default=None, help='Wandbçš„å®ä½“/ç”¨æˆ·å (å¯é€‰)')
    parser.add_argument('--disable_wandb', action='store_true', help='å¦‚æœè®¾ç½®, åˆ™ç¦ç”¨wandbæ—¥å¿—')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    args = parser.parse_args()

    # ä¸»å¾ªç¯ï¼Œç”¨äºå¤šæ¬¡éšæœºç§å­å®éªŒ
    for i in range(args.num_runs):
        current_seed = args.seed + i
        print(f"\n{'='*30}  è¿è¡Œç¬¬ {i+1}/{args.num_runs} æ¬¡, éšæœºç§å­: {current_seed}  {'='*30}\n")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(current_seed)
        np.random.seed(current_seed)

        # æ‹¼æ¥å®Œæ•´çš„è·¯å¾„
        DEV_AUDIO_DIR = os.path.join(args.base_path, args.dev_audio_dir_name)
        DEV_CAPTIONS_FILE = os.path.join(args.base_path, args.dev_captions_name)
        EVAL_AUDIO_DIR = os.path.join(args.base_path, args.eval_audio_dir_name)
        EVAL_CAPTIONS_FILE = os.path.join(args.base_path, args.eval_captions_name)

        # æ£€æŸ¥è·¯å¾„
        if not all(os.path.exists(p) for p in [DEV_AUDIO_DIR, DEV_CAPTIONS_FILE]):
            print(f"é”™è¯¯ï¼šè¯·ç¡®ä¿å¼€å‘é›†æ•°æ®è·¯å¾„éƒ½å­˜åœ¨ï¼")
            print(f"  - å¼€å‘é›†éŸ³é¢‘: {DEV_AUDIO_DIR}")
            print(f"  - å¼€å‘é›†å­—å¹•: {DEV_CAPTIONS_FILE}")
            # è¯„ä¼°é›†è·¯å¾„çš„æ£€æŸ¥å¯ä»¥æ”¾åˆ°å®éªŒæµç¨‹å†…éƒ¨ï¼Œå› ä¸ºå¯èƒ½åªåšè’¸é¦ä¸åšè¯„ä¼°
            continue # å¦‚æœå¼€å‘é›†ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡æœ¬æ¬¡è¿è¡Œ

        # åˆå§‹åŒ–Wandb
        if not args.disable_wandb:
            wandb.init(
                project=args.wandb_project,
                entity=args.wandb_entity,
                config=args,
                name=f"run_{i+1}_seed_{current_seed}",
                reinit=True # å…è®¸å¤šæ¬¡åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­åˆå§‹åŒ–
            )
    
        # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
        experiment = ClothoExperiment(
            dev_audio_dir=DEV_AUDIO_DIR,
            dev_captions_file=DEV_CAPTIONS_FILE,
            eval_audio_dir=EVAL_AUDIO_DIR,
            eval_captions_file=EVAL_CAPTIONS_FILE,
            device=device
        )
        experiment.run_complete_experiment(args) # å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™å®éªŒä¸»å‡½æ•°

        if not args.disable_wandb:
            wandb.finish()