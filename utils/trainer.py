import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import logging
from torch.utils.data import Dataset
from typing import Optional, Callable
from copy import deepcopy

# é…ç½®ä¸€ä¸ªç®€å•çš„æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class Trainer:
    """
    ä¸€ä¸ªé€šç”¨çš„è®­ç»ƒå™¨ç±»ï¼Œå°è£…äº†æ ‡å‡†çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹ã€‚
    """
    def __init__(self, 
                 model: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 loss_fn: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 synthetic_dataset: Dataset,
                 real_dataset: Dataset,
                 real_sampler: Callable,
                 real_batch_size: int,
                 lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
                 device: str = 'cuda',
                 epochs: int = 100,
                 val_train_epochs: int = 5,
                 checkpoint_dir: str = 'output/checkpoints',
                 noise_level: float = 0.0,
                 augment_transform: Optional[Callable] = None):
        """
        åˆå§‹åŒ– Trainerã€‚

        Args:
            model (nn.Module): éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
            optimizer (Optimizer): ä¼˜åŒ–å™¨ã€‚
            loss_fn (nn.Module): æŸå¤±å‡½æ•°ã€‚
            train_loader (DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨ã€‚
            val_loader (DataLoader): éªŒè¯æ•°æ®åŠ è½½å™¨ã€‚
            lr_scheduler (Scheduler, optional): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
            device (str): è®­ç»ƒè®¾å¤‡ ('cuda' or 'cpu')ã€‚
            epochs (int): è®­ç»ƒçš„æ€»è½®æ•°ã€‚
            checkpoint_dir (str): ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„ç›®å½•ã€‚
        """
        self.model = model.to(device)
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.lr_scheduler = lr_scheduler
        self.device = device
        self.epochs = epochs
        self.checkpoint_dir = checkpoint_dir
        self.val_train_epochs = val_train_epochs
        self.augment_transform = augment_transform
        self.synthetic_dataset = synthetic_dataset
        self.real_dataset = real_dataset
        self.real_sampler = real_sampler
        self.real_batch_size = real_batch_size
        self.noise_level = noise_level
        
        self.best_val_acc = 0.0
        os.makedirs(self.checkpoint_dir, exist_ok=True)

    def train(self):
        """
        å¯åŠ¨å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚
        """
        logging.info("Starting training...")
        for epoch in range(1, self.epochs + 1):
            # è®­ç»ƒä¸€ä¸ª epoch
            train_loss = self._train_one_epoch(epoch)
            
            # éªŒè¯ä¸€ä¸ª epoch
            val_acc = self._evaluate_synthetic_data_quality(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.lr_scheduler:
                self.lr_scheduler.step()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹å¹¶ä¿å­˜
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                logging.info(f"ğŸ‰ New best synthetic data found! Validation accuracy: {val_acc:.2f}%")
            
            self._save_checkpoint(epoch, is_best)
        
        logging.info("Training complete.")

    def _train_one_epoch(self, epoch: int) -> float:
        """
        æ‰§è¡Œå•è½®è®­ç»ƒã€‚
        ã€æœ€ç»ˆä¿®æ­£ç‰ˆã€‘
        æœ¬æ–¹æ³•åœ¨ DataLoader æä¾›çš„åˆæˆæ•°æ®æ‰¹æ¬¡åŸºç¡€ä¸Šï¼Œ
        å®æ—¶é‡‡æ ·ã€å½¢çŠ¶å’Œç±»åˆ«åˆ†å¸ƒéƒ½åŒ¹é…ã€‘çš„çœŸå®æ•°æ®æ¥è®¡ç®—æŸå¤±ã€‚
        """
        total_loss = 0.0
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.epochs} [Training on Synthetic Data]")
        for batch in progress_bar:
            
            # 1. è·å–åˆæˆæ•°æ®æ‰¹æ¬¡åŠå…¶æ ‡ç­¾
            syn_aud_batch = batch['audio'].to(self.device)
            syn_img_batch = batch['frame'].to(self.device)
            labels_batch = batch['label'].to(self.device)

            # 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘: æ ¹æ®åˆæˆæ‰¹æ¬¡çš„ç±»åˆ«åˆ†å¸ƒï¼Œé‡‡æ ·ã€ç­‰é‡ã€‘çš„çœŸå®æ•°æ®
            
            # æ‰¾å‡ºå½“å‰æ‰¹æ¬¡ä¸­æœ‰å“ªäº›ç±»åˆ«ï¼Œä»¥åŠæ¯ä¸ªç±»åˆ«æœ‰å¤šå°‘æ ·æœ¬
            unique_classes, counts = torch.unique(labels_batch, return_counts=True)
            
            list_real_aud_per_class = []
            list_real_img_per_class = []
            
            for c, num_samples in zip(unique_classes, counts):
                # ä¸ºç±»åˆ« cï¼Œé‡‡æ · num_samples ä¸ªçœŸå®æ ·æœ¬
                real_indices = self.real_sampler.sample(c.item(), num_samples.item())
                if not real_indices: continue

                real_data_c = self.real_dataset[real_indices]
                list_real_aud_per_class.append(real_data_c['audio'])
                list_real_img_per_class.append(real_data_c['frame'])

            if not list_real_aud_per_class or not list_real_img_per_class:
                continue
                
            # å°†æ‰€æœ‰é‡‡æ ·åˆ°çš„çœŸå®æ•°æ®æ‹¼æ¥æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚
            # æ³¨æ„ï¼šæ­¤æ—¶ aud_real_batch æ˜¯æŒ‰ç±»åˆ«æ’åºçš„
            aud_real_batch_sorted = torch.cat(list_real_aud_per_class, dim=0).to(self.device)
            img_real_batch_sorted = torch.cat(list_real_img_per_class, dim=0).to(self.device)

            # --- ä¸ºäº†è®©æŸå¤±å‡½æ•°èƒ½å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦å°† syn_batch ä¹ŸæŒ‰ç±»åˆ«æ’åº ---
            # è¿™æ ·ï¼Œæ’åºåçš„ syn_batch å’Œ real_batch å°±å®Œå…¨å¯¹é½äº†
            sorting_indices = torch.argsort(labels_batch)
            syn_aud_batch_sorted = syn_aud_batch[sorting_indices]
            syn_img_batch_sorted = syn_img_batch[sorting_indices]

            # è°ƒæ•´ç»´åº¦
            if aud_real_batch_sorted.dim()==4:
                aud_real_batch_sorted = aud_real_batch_sorted.unsqueeze(1)


            # REBUTTALï¼šæ·»åŠ å™ªå£°
            if self.noise_level > 0.0:
                noise_aud = torch.randn_like(aud_real_batch_sorted) * self.noise_level
                noise_img = torch.randn_like(img_real_batch_sorted) * self.noise_level
                aud_real_batch_sorted = aud_real_batch_sorted + noise_aud
                img_real_batch_sorted = img_real_batch_sorted + noise_img


            # 3. æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            # 4. å‰å‘ä¼ æ’­ (å¯¹æ’åºåã€å¯¹é½çš„æ•°æ®)
            inputs_syn = {"audio": syn_aud_batch_sorted, "image": syn_img_batch_sorted}
            features_syn = self.model.forward(inputs_syn, mode="embeddings")
            embd_aud_syn = features_syn["audio"]
            embd_img_syn = features_syn["vision"]

            inputs_real = {"audio": aud_real_batch_sorted, "image": img_real_batch_sorted}
            features_real = self.model.forward(inputs_real, mode="embeddings")
            embd_aud_real = features_real["audio"].detach()
            embd_img_real = features_real["vision"].detach()
            
            # 5. è®¡ç®—æŸå¤± (ç°åœ¨è¾“å…¥çš„ Tensor å½¢çŠ¶å’Œç±»åˆ«åˆ†å¸ƒéƒ½å·²å¯¹é½)
            loss = self.loss_fn(embd_aud_real, embd_aud_syn, embd_img_real, embd_img_syn)
            
            # 6. åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            self.optimizer.step()
            
            # 7. æ›´æ–°ç»Ÿè®¡
            total_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(self.train_loader)
        logging.info(f"Epoch {epoch} Training Summary: Average Loss: {avg_loss:.4f}")
        return avg_loss
    
    def _save_checkpoint(self, epoch: int, is_best: bool):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ã€‚
        ã€å·²ä¸ºä½ å®šåˆ¶ï¼Œä»¥é€‚åº”æ•°æ®è’¸é¦ä»»åŠ¡ã€‘
        è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¿å­˜çš„æ˜¯åˆæˆæ•°æ®å’Œå®ƒçš„ä¼˜åŒ–å™¨çŠ¶æ€ã€‚
        """
        # 1. å‡†å¤‡è¦ä¿å­˜çš„çŠ¶æ€å­—å…¸
        state = {
            'epoch': epoch,
            'best_val_acc': self.best_val_acc,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'synthetic_audio_data': self.synthetic_dataset.audio,
            'synthetic_image_data': self.synthetic_dataset.images,
            'synthetic_labels': self.synthetic_dataset.labels # å¦‚æœæ ‡ç­¾ä¹Ÿæ˜¯å¯å­¦ä¹ çš„
        }
        
        if self.lr_scheduler:
            state['scheduler_state_dict'] = self.lr_scheduler.state_dict()
            
        # 2. å®šä¹‰æ–‡ä»¶å
        filename = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
        
        # 3. ä¿å­˜æ£€æŸ¥ç‚¹
        torch.save(state, filename)
        logging.info(f"Saved synthetic data checkpoint: {filename}")
        
        # 4. å¦‚æœæ˜¯å½“å‰æœ€å¥½çš„ç»“æœï¼Œé¢å¤–ä¿å­˜ä¸€ä»½ä¸º 'best_syn_data.pth'
        if is_best:
            best_filename = os.path.join(self.checkpoint_dir, "best_syn_data.pth")
            torch.save(state, best_filename)
            logging.info(f"ğŸ‰ Saved best synthetic data to: {best_filename}")

    def _evaluate_synthetic_data_quality(self, epoch: int) -> float:
        """
        é€šè¿‡åªè®­ç»ƒæ¨¡å‹åˆ†ç±»å¤´å¹¶åœ¨çœŸå®æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼Œæ¥è¡¡é‡åˆæˆæ•°æ®çš„è´¨é‡ã€‚
        ã€å·²ä¸ºä½ æ¨¡å‹çš„ ImageBindClassifier ç»“æ„ç²¾ç¡®å®šåˆ¶ã€‘
        """
        logging.info(f"--- Starting validation for epoch {epoch}: Evaluating synthetic data quality ---")

        # --- æ­¥éª¤ 1: å‡†å¤‡ä¸€ä¸ªç”¨äºè¯„ä¼°çš„â€œå­¦ç”Ÿâ€æ¨¡å‹å‰¯æœ¬ ---
        # æˆ‘ä»¬ä½¿ç”¨åŸå§‹æ•™å¸ˆæ¨¡å‹çš„ä¸€ä¸ªæ·±æ‹·è´ï¼Œä»¥ç¡®ä¿åŸå§‹æ¨¡å‹ä¸å—å½±å“
        student_model = deepcopy(self.model)
        student_model.to(self.device)

        # --- æ­¥éª¤ 2: å†»ç»“ç‰¹å¾æå–å™¨ï¼Œåªè®­ç»ƒåˆ†ç±»å¤´ ---
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in student_model.parameters():
            param.requires_grad = False
        
        # ç„¶åï¼Œåªè§£å†»ä½ æŒ‡å®šçš„åˆ†ç±»å¤´çš„å‚æ•°
        logging.info("Unfreezing classifier weights for validation training...")
        for param in student_model.classifier_audio.parameters():
            param.requires_grad = True
        for param in student_model.classifier_image.parameters():
            param.requires_grad = True
        
        # åˆ›å»ºä¸€ä¸ªåªåŒ…å«å¯è®­ç»ƒå‚æ•°çš„ä¼˜åŒ–å™¨
        trainable_params = filter(lambda p: p.requires_grad, student_model.parameters())
        optimizer_student = torch.optim.Adam(trainable_params, lr=0.001)
        
        loss_fn_student = nn.CrossEntropyLoss()
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨å°±æ˜¯ self.train_loader (syn_loader)
        inner_train_loader = self.train_loader 

        # --- æ­¥éª¤ 3: å†…éƒ¨å¿«é€Ÿè®­ç»ƒå¾ªç¯ ---
        logging.info(f"Training classifier head for {self.val_train_epochs} epochs on synthetic data...")
        student_model.train() # å°†å­¦ç”Ÿæ¨¡å‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼
        for inner_epoch in range(self.val_train_epochs):
            for batch in inner_train_loader:
                # ä» batch ä¸­è·å–å¹²å‡€çš„åˆæˆæ•°æ®
                syn_aud_batch = batch['audio'].to(self.device)
                syn_img_batch = batch['frame'].to(self.device)
                labels = batch['label'].to(self.device)

                # åœ¨é€å…¥å­¦ç”Ÿæ¨¡å‹å‰ï¼Œè¿›è¡Œå®æ—¶æ•°æ®å¢å¼º
                if self.augment_transform:
                    syn_img_batch = self.augment_transform(syn_img_batch)
                    # ä½ ä¹Ÿå¯ä»¥ä¸ºéŸ³é¢‘åšå¢å¼º
                
                inputs = { "audio": syn_aud_batch, "image": syn_img_batch }
                
                optimizer_student.zero_grad()
                
                # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
                predictions = student_model.forward(inputs)
                
                loss = loss_fn_student(predictions, labels)
                
                loss.backward()
                optimizer_student.step()
        
        # --- æ­¥éª¤ 4: åœ¨çœŸå®æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„å­¦ç”Ÿæ¨¡å‹ ---
        logging.info("Evaluating the trained student model on the real test set...")
        student_model.eval() # å°†å­¦ç”Ÿæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼
        correct = 0
        total = 0
        with torch.no_grad():
            for batch in self.val_loader: # self.val_loader æ˜¯çœŸå®çš„ test_loader
                inputs = { "audio": batch['audio'].to(self.device), "image": batch['frame'].to(self.device) }
                labels = batch['label'].to(self.device)
                
                # è·å¾—æœ€ç»ˆçš„é¢„æµ‹æ¦‚ç‡
                outputs = student_model.forward(inputs)
                # ä»æ¦‚ç‡ä¸­å¾—åˆ°é¢„æµ‹çš„ç±»åˆ«
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        logging.info(f"Validation Summary: Student model accuracy on real test data: {accuracy:.2f}%")
        
        del student_model # é‡Šæ”¾å‰¯æœ¬å ç”¨çš„æ˜¾å­˜
        return accuracy