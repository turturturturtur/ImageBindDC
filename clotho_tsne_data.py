import os
import json
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from typing import Dict, Tuple, List
import argparse
import wandb
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType
from nets import ModelBuilder
import sys

PROJECT_ROOT = '/autodl-tmp/audio-visual-mater/'
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)
from transformers import DistilBertTokenizer, DistilBertModel
# ä»æ‚¨çš„æœ¬åœ°æ–‡ä»¶å¤¹å¯¼å…¥å¿…è¦çš„æ¨¡å—
from utils.train_utils_DM import get_network_imagebind
from nets.imagebind.models.imagebind_model import ModalityType
from nets.imagebind import data # ã€å…³é”®ã€‘ä»æ‚¨çš„netsæ–‡ä»¶å¤¹å¯¼å…¥data
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import math
import textwrap
import pandas as pd

class SyntheticDataset(Dataset):
    """ç”¨äºå·²è’¸é¦çš„åˆæˆç‰¹å¾çš„æ•°æ®é›†"""
    def __init__(self, synthetic_audio: torch.Tensor, synthetic_text: torch.Tensor):
        self.audio = synthetic_audio
        self.text = synthetic_text

    def __len__(self):
        return len(self.audio)

    def __getitem__(self, idx):
        return {
            'audio': self.audio[idx],
            'text': self.text[idx],
            'label': idx
        }



def _style_subplot(ax, is_correct):
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºç»Ÿä¸€è®¾ç½®å­å›¾çš„è¾¹æ¡†æ ·å¼å’Œé¢œè‰²ã€‚"""
    color = 'green' if is_correct else 'red'
    for spine in ax.spines.values():
        spine.set_edgecolor(color)
        spine.set_linewidth(3)
    ax.set_xticks([])
    ax.set_yticks([])

def visualize_retrieval_examples(
    model,
    eval_dataloader,
    eval_audio_features,
    eval_text_features,
    device,
    file_path,
    num_examples=3,
    top_k=3
):
    """
    å¯è§†åŒ–å®šæ€§æ£€ç´¢ç»“æœï¼ˆæœ€ç»ˆç‰ˆï¼‰ï¼Œä¿®å¤æ–‡æœ¬é‡å å¹¶ä¼˜åŒ–å¸ƒå±€ã€‚
    """
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆå®šæ€§æ£€ç´¢ç¤ºä¾‹å›¾ (Top-{top_k})ï¼Œå¹¶ä¿å­˜è‡³ {file_path}...")
    model.eval()

    total_samples = len(eval_dataloader.dataset)
    query_indices = np.random.choice(total_samples, num_examples, replace=False)

    fig, axes = plt.subplots(
        2 * num_examples, 
        1 + top_k, 
        figsize=(5 * (1 + top_k), 5 * 2 * num_examples), # å¢åŠ ç”»å¸ƒå¤§å°
        gridspec_kw={'width_ratios': [1.5] + [2.5] * top_k} # å¢åŠ æ–‡æœ¬åˆ—çš„ç›¸å¯¹å®½åº¦
    )
    fig.suptitle('Qualitative Retrieval Examples', fontsize=28, y=1.0)

    with torch.no_grad():
        projected_audio_feats, projected_text_feats = model(
            eval_audio_features.to(device),
            eval_text_features.to(device)
        )
        sim_matrix_a2t = torch.matmul(projected_audio_feats, projected_text_feats.t())
        sim_matrix_t2a = sim_matrix_a2t.t()

        for i, query_idx in enumerate(query_indices):
            # --- Part 1: éŸ³é¢‘æ£€ç´¢æ–‡æœ¬ (A->T) ---
            ax_row = i * 2
            
            query_audio_mel = eval_dataloader.dataset[query_idx]['audio_mel'].squeeze(0).numpy()
            axes[ax_row, 0].imshow(query_audio_mel, aspect='auto', origin='lower')
            axes[ax_row, 0].set_title(f"Query Audio #{query_idx}", fontsize=14)
            axes[ax_row, 0].axis('off')

            _, topk_text_indices = torch.topk(sim_matrix_a2t[query_idx], k=top_k)
            for j, retrieved_idx in enumerate(topk_text_indices):
                ax = axes[ax_row, j + 1]
                caption = eval_dataloader.dataset[retrieved_idx.item()]['raw_caption']
                # ã€å…³é”®ä¿®æ­£ã€‘ä½¿ç”¨textwrapè‡ªåŠ¨æ¢è¡Œï¼Œé¿å…æ–‡æœ¬æº¢å‡º
                wrapped_caption = '\n'.join(textwrap.wrap(caption, width=30))
                
                ax.text(0.5, 0.5, f"Rank #{j+1}\n\n\"{wrapped_caption}\"", 
                        ha='center', va='center', wrap=True, fontsize=12)
                
                is_correct = (retrieved_idx.item() == query_idx)
                _style_subplot(ax, is_correct) # è°ƒç”¨è¾…åŠ©å‡½æ•°æ¥è®¾ç½®æ ·å¼

            # --- Part 2: æ–‡æœ¬æ£€ç´¢éŸ³é¢‘ (T->A) ---
            ax_row = i * 2 + 1
            
            ax = axes[ax_row, 0]
            query_caption = eval_dataloader.dataset[query_idx]['raw_caption']
            wrapped_caption = '\n'.join(textwrap.wrap(query_caption, width=30))
            ax.text(0.5, 0.5, f"Query Text #{query_idx}\n\n\"{wrapped_caption}\"", 
                    ha='center', va='center', wrap=True, fontsize=12)
            ax.axis('off')

            _, topk_audio_indices = torch.topk(sim_matrix_t2a[query_idx], k=top_k)
            for j, retrieved_idx in enumerate(topk_audio_indices):
                ax = axes[ax_row, j + 1]
                audio_mel = eval_dataloader.dataset[retrieved_idx.item()]['audio_mel'].squeeze(0).numpy()
                is_correct = (retrieved_idx.item() == query_idx)
                
                ax.imshow(audio_mel, aspect='auto', origin='lower')
                ax.set_title(f"Rank #{j+1}", color=('green' if is_correct else 'red'), fontsize=14)
                _style_subplot(ax, is_correct) # è°ƒç”¨è¾…åŠ©å‡½æ•°æ¥è®¾ç½®æ ·å¼

    plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€
    plt.savefig(file_path, dpi=300)
    plt.close()
    print(f"âœ… å®šæ€§æ£€ç´¢ç¤ºä¾‹å›¾å·²ä¿å­˜ã€‚")


def visualize_tsne_comparison(results_list: list,
                              file_path: str,
                              num_samples: int = 500):
    """
    ã€å¯¹æ¯”ç‰ˆã€‘åœ¨ä¸€å¼ å¤§å›¾ä¸­å¹¶æ’å¯¹æ¯”å¤šç§æ–¹æ³•çš„t-SNEåµŒå…¥ç©ºé—´ã€‚
    æ­¤ç‰ˆæœ¬æ¥æ”¶å·²ç»æŠ•å½±å¥½çš„ç‰¹å¾ã€‚

    Args:
        results_list (list): åŒ…å«å¾…å¯è§†åŒ–æ–¹æ³•ç»“æœçš„åˆ—è¡¨ã€‚
                             æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œåº”åŒ…å« 'name', 'projected_audio', 'projected_text'ã€‚
        file_path (str): æœ€ç»ˆå¯¹æ¯”å›¾çš„ä¿å­˜è·¯å¾„ã€‚
        num_samples (int): ä¸ºåŠ é€Ÿè®¡ç®—å’Œä¿æŒå›¾åƒæ¸…æ™°ï¼Œä»ç‰¹å¾ä¸­é‡‡æ ·çš„æ•°é‡ã€‚
    """
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆç»Ÿä¸€çš„t-SNEåµŒå…¥ç©ºé—´å¯¹æ¯”å›¾...")

    num_methods = len(results_list)
    # åˆ›å»ºä¸€ä¸ªä¸€è¡ŒNåˆ—çš„å­å›¾å¸ƒå±€
    fig, axes = plt.subplots(1, num_methods, figsize=(8 * num_methods, 7), squeeze=False)
    fig.suptitle('Comparison of t-SNE Joint Embedding Spaces', fontsize=20, y=1.0)

    for i, result in enumerate(results_list):
        ax = axes[0, i]
        name = result['name']
        projected_audio = result['projected_audio']
        projected_text = result['projected_text']

        # å¯¹ä¼ å…¥çš„ç‰¹å¾è¿›è¡Œé‡‡æ ·
        if len(projected_audio) > num_samples:
            indices = np.random.choice(len(projected_audio), num_samples, replace=False)
            audio_features_sample = projected_audio[indices]
            text_features_sample = projected_text[indices]
        else:
            audio_features_sample = projected_audio
            text_features_sample = projected_text

        # å‡†å¤‡t-SNEçš„è¾“å…¥æ•°æ®
        all_features = np.vstack([audio_features_sample.numpy(), text_features_sample.numpy()])
        modality_labels = ['Audio'] * len(audio_features_sample) + ['Text'] * len(text_features_sample)

        # è¿è¡Œt-SNE
        tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)
        embeddings_2d = tsne.fit_transform(all_features)

        # åœ¨å¯¹åº”çš„å­å›¾ä¸Šç»˜åˆ¶æ•£ç‚¹å›¾
        plot_df = pd.DataFrame({
            't-SNE Dim 1': embeddings_2d[:, 0],
            't-SNE Dim 2': embeddings_2d[:, 1],
            'Modality': modality_labels
        })
        sns.scatterplot(
            data=plot_df, x='t-SNE Dim 1', y='t-SNE Dim 2', hue='Modality',
            palette={'Audio': 'skyblue', 'Text': 'coral'}, alpha=0.8, ax=ax
        )
        ax.set_title(name, fontsize=16)
        ax.grid(True)
        # åªåœ¨æœ€åä¸€ä¸ªå­å›¾æ˜¾ç¤ºå›¾ä¾‹
        if i < num_methods - 1:
            if ax.get_legend() is not None: ax.get_legend().remove()
        else:
            if ax.get_legend() is not None: ax.legend(title='Modality')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(file_path, dpi=300)
    plt.close()
    print(f"âœ… t-SNEå¯¹æ¯”å›¾å·²ä¿å­˜è‡³ {file_path}")

from torch.nn.utils.rnn import pad_sequence
import pandas as pd
from tqdm import tqdm

class RawClothoDataset(Dataset):
    def __init__(self,
                 audio_dir: str,
                 captions_df: pd.DataFrame,
                 vocab: Dict,
                 sr: int = 22050,
                 n_mels: int = 64,
                 max_time_steps: int = 1024):
        self.audio_dir = audio_dir
        self.captions_df = captions_df
        self.vocab = vocab
        self.sr = sr
        self.n_mels = n_mels
        self.max_time_steps = max_time_steps

    def __len__(self):
        return len(self.captions_df)

    def __getitem__(self, idx):
        row = self.captions_df.iloc[idx]
        audio_path = os.path.join(self.audio_dir, row['file_name'])

        # 1. åŠ è½½éŸ³é¢‘å¹¶è®¡ç®—æ¢…å°”é¢‘è°±å›¾
        y, _ = librosa.load(audio_path, sr=self.sr)
        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)
        mel_spec_db = librosa.power_to_db(mel_spec)          # shape: (n_mels, T)

        # 2. ç»Ÿä¸€æ—¶é—´ç»´åº¦ï¼špadding æˆ–æˆªæ–­
        if mel_spec_db.shape[1] < self.max_time_steps:
            pad_width = self.max_time_steps - mel_spec_db.shape[1]
            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mel_spec_db = mel_spec_db[:, :self.max_time_steps]

        audio_mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # shape: (1, n_mels, T_fixed)

        # 3. æ–‡æœ¬åºåˆ—è½¬ ID
        caption_string = row['caption_1']
        words = caption_string.lower().split()
        word_ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]
        text_ids_tensor = torch.tensor(word_ids, dtype=torch.long)

        return {
            'audio_mel': audio_mel_tensor,
            'text_ids': text_ids_tensor,
            'audio_path': audio_path,
            'raw_caption': caption_string
        }
def build_vocab(captions: List[str], min_freq=5) -> Dict:
    word_counts = {}
    for caption in captions:
        for word in caption.lower().split():
            word_counts[word] = word_counts.get(word, 0) + 1

    vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items()) if count >= min_freq}
    vocab['<pad>'] = 0
    vocab['<unk>'] = 1
    return vocab

def collate_fn(batch):
    # ä¸º ConvNetGRU å¤„ç†æ•°æ®
    audio_mels = [item['audio_mel'] for item in batch]
    text_ids = [item['text_ids'] for item in batch]
    texts_padded = pad_sequence(text_ids, batch_first=True, padding_value=0)
    audios_stacked = torch.stack(audio_mels)

    # ä¸º ImageBind æ”¶é›†åŸå§‹æ•°æ® (ä¿æŒä¸ºList)
    audio_paths = [item['audio_path'] for item in batch]
    raw_captions = [item['raw_caption'] for item in batch]
    
    return {
        'audio_mel': audios_stacked,
        'text_ids': texts_padded,
        'audio_path': audio_paths,
        'raw_caption': raw_captions
    }
    
def get_high_performance_dataloader(audio_dir: str, captions_file: str, batch_size: int, num_samples: int, vocab: Dict = None) -> Tuple[DataLoader, Dict]:
    """
    åˆ›å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„Dataloaderã€‚
    æ–°å¢åŠŸèƒ½ï¼šå¯ä»¥æ¥æ”¶ä¸€ä¸ªå·²æœ‰çš„vocabï¼Œå¦‚æœvocabä¸ºNoneï¼Œåˆ™ä¼šè‡ªå·±æ„å»ºã€‚
    """
    df = pd.read_csv(captions_file)
    if num_samples:
        # æ³¨æ„ï¼šå¯¹äºè¯„ä¼°é›†ï¼Œæˆ‘ä»¬é€šå¸¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼Œå› æ­¤num_samplesåº”ä¸ºNone
        df = df.sample(n=num_samples, random_state=42).reset_index(drop=True)

    if vocab is None:
        print("æœªæä¾›è¯æ±‡è¡¨ï¼Œæ­£åœ¨ä»å­—å¹•ä¸­æ„å»º...")
        vocab = build_vocab(df['caption_1'].tolist())
    else:
        print("ä½¿ç”¨å·²æä¾›çš„è¯æ±‡è¡¨ã€‚")

    dataset = RawClothoDataset(audio_dir, df, vocab)
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)
    
    # å§‹ç»ˆè¿”å›ä½¿ç”¨çš„dataloaderå’Œvocab
    return dataloader, vocab

def precompute_real_features(dataloader: DataLoader, feature_extractor, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
    if isinstance(feature_extractor, nn.Module):
        feature_extractor.to(device)
        feature_extractor.eval()
    
    all_audio_features, all_text_features = [], []
    print(f"ğŸš€ å¼€å§‹é¢„è®¡ç®—çœŸå®ç‰¹å¾ï¼Œä½¿ç”¨æ¨¡å‹: {feature_extractor.__class__.__name__}")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æ­£åœ¨æå–ç‰¹å¾"):
            # ã€æœ€ç»ˆç‰ˆã€‘æ™ºèƒ½æ•°æ®åˆ†å‘
            if isinstance(feature_extractor, ImageBindExtractor):
                # ä¸º ImageBind ä¼ é€’ã€æ–‡ä»¶è·¯å¾„ã€‘å’Œã€åŸå§‹å­—ç¬¦ä¸²ã€‘
                audio_feats = feature_extractor.extract_audio_features(batch['audio_path'])
                text_feats = feature_extractor.extract_text_features(batch['raw_caption'])

            elif isinstance(feature_extractor, PretrainedConvNetExtractor):
                # ä¸º PretrainedConvNetExtractor ä¼ é€’ã€é¢‘è°±å›¾ã€‘å’Œã€åŸå§‹å­—ç¬¦ä¸²ã€‘
                audio_data = batch['audio_mel'].to(device)
                text_data = batch['raw_caption'] # æ³¨æ„ï¼šæ–‡æœ¬ç«¯éœ€è¦åŸå§‹å­—ç¬¦ä¸²
                audio_feats = feature_extractor.forward_audio(audio_data)
                text_feats = feature_extractor.forward_text(text_data)
            
            else:
                # ä¸ºå…¶ä»–æ¨¡å‹ï¼ˆä¾‹å¦‚æˆ‘ä»¬æœ€åˆçš„ConvNetGRUï¼‰ä¼ é€’ã€é¢‘è°±å›¾ã€‘å’Œã€IDåºåˆ—ã€‘
                audio_data = batch['audio_mel'].to(device)
                text_data = batch['text_ids'].to(device)
                audio_feats = feature_extractor.forward_audio(audio_data)
                text_feats = feature_extractor.forward_text(text_data)

            all_audio_features.append(audio_feats.cpu())
            all_text_features.append(text_feats.cpu())
            
    print("âœ… çœŸå®ç‰¹å¾é¢„è®¡ç®—å®Œæˆï¼")
    return torch.cat(all_audio_features, dim=0).to(device), torch.cat(all_text_features, dim=0).to(device)
# ===================== ç‰¹å¾æå–å™¨ =====================


class ImageBindExtractor:
    """çœŸå®çš„ ImageBind ç‰¹å¾æå–å™¨ (æ¨¡ä»¿æ‚¨çš„æœ¬åœ°é¡¹ç›®)"""
    def __init__(self, pretrained=False, device='cpu'):
        self.device = device
        self.model = None
        self.embed_func = None

        if pretrained:
            print("æ­£åœ¨ä½¿ç”¨æ‚¨æœ¬åœ°çš„ get_network_imagebind å‡½æ•°åŠ è½½æ¨¡å‹...")
            
            class MockArgs:
                def __init__(self):
                    self.arch_frame = 'imagebind'
                    self.arch_classifier = 'ensemble'
                    self.cls_num = 10
                    self.weights_classifier = ''
                    self.input_modality = 'av'

            mock_args = MockArgs()
            
            nets, _ = get_network_imagebind(mock_args)
            net_imagebind, _ = nets
            
            self.model = net_imagebind
            self.model.to(self.device)
            self.model.eval()

            self.embed_func = self.model.module.embed if torch.cuda.device_count() > 1 else self.model.embed
            print("âœ… çœŸå® ImageBind æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def extract_audio_features(self, audio_path_list: List[str]) -> torch.Tensor:
        if self.model is None: raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
        # ã€å…³é”®ã€‘è°ƒç”¨æ‚¨æœ¬åœ°çš„ data.py ä¸­çš„å‡½æ•°
        inputs = { ModalityType.AUDIO: data.load_and_transform_audio_data(audio_path_list, self.device) }
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.AUDIO].detach()

    def extract_text_features(self, raw_caption_list: List[str]) -> torch.Tensor:
        if self.model is None:
            raise RuntimeError("ImageBindæ¨¡å‹æœªåŠ è½½ï¼")
    
        print("raw_caption_list å‰ 3 æ¡ï¼š", raw_caption_list[:3])
    
        # 1ï¸âƒ£ æ‹¿åˆ° token id
        text_tokens = data.load_and_transform_text(raw_caption_list, self.device)
    
        # 2ï¸âƒ£ æ‰“å°æœ€å¤§ idï¼Œçœ‹æ˜¯å¦çœŸçš„ â‰¥ 49408
        max_id = text_tokens.max().item()
        print("max token id =", max_id, " è¶Šç•Œï¼Ÿ", max_id >= 49408)
    
        # 3ï¸âƒ£ å…œåº• clamp
        text_tokens = torch.clamp(text_tokens, max=49407)
    
        inputs = {ModalityType.TEXT: text_tokens}
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
        return embeddings[ModalityType.TEXT].detach()

# ===================== æ ‡å‡†Herdingç®—æ³• =====================

class StandardHerdingSelector:
    """æ ‡å‡†Herdingç®—æ³•çš„è§„èŒƒå®ç°"""

    def __init__(self, device='cuda'):
        self.device = device

    def select_coreset(self, features, coreset_size):
        if coreset_size >= features.shape[0]:
            return torch.arange(features.shape[0]), features

        # 1. è®¡ç®—ä¸€æ¬¡å…¨ä½“ç‰¹å¾çš„å‡å€¼
        overall_mean = features.mean(dim=0)

        selected_indices = []
        available_indices = list(range(features.shape[0]))

        # 2. åˆå§‹åŒ–å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
        current_coreset_mean = torch.zeros_like(overall_mean)

        for i in range(coreset_size):
            # 3. è®¡ç®—å½“å‰å‡å€¼ä¸ç›®æ ‡å‡å€¼çš„â€œè¯¯å·®å‘é‡â€
            error_vector = overall_mean - current_coreset_mean

            # 4. åœ¨å‰©ä½™æ ·æœ¬ä¸­ï¼Œå¯»æ‰¾ä¸è¯¯å·®å‘é‡å†…ç§¯æœ€å¤§çš„é‚£ä¸ª
            available_features = features[available_indices]
            projections = torch.matmul(available_features, error_vector)

            best_idx_in_remaining = torch.argmax(projections)
            selected_idx = available_indices.pop(best_idx_in_remaining)

            selected_indices.append(selected_idx)

            # 5. æ›´æ–°å½“å‰æ ¸å¿ƒé›†çš„å‡å€¼
            current_coreset_mean = (current_coreset_mean * i + features[selected_idx]) / (i + 1)

        selected_indices = torch.tensor(selected_indices)
        return selected_indices, features[selected_indices]

# ===================== æŸå¤±å‡½æ•° =====================

class AVDDLoss:
    """AVDDæŸå¤±å‡½æ•°"""
    
    def __init__(self, lam_icm=10.0, lam_cgm=10.0):
        self.lam_icm = lam_icm
        self.lam_cgm = lam_cgm
    
    def __call__(self, real_features, synthetic_features):
        """è®¡ç®—AVDDæŸå¤±"""
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features
        
        # 1. åˆ†å¸ƒåŒ¹é…æŸå¤± (DM Loss)
        loss_dm = F.mse_loss(syn_audio.mean(0), real_audio.mean(0)) + \
                 F.mse_loss(syn_text.mean(0), real_text.mean(0))
        
        # 2. æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤± (ICM Loss) - ä¿®æ­£ååŒ¹é…ç»Ÿè®¡é‡
        # å‡è®¾è¾“å…¥çš„ç‰¹å¾å·²ç»è¢«å½’ä¸€åŒ–
        real_sim_matrix = torch.matmul(real_audio, real_text.t())
        syn_sim_matrix = torch.matmul(syn_audio, syn_text.t())
        
        loss_icm = F.mse_loss(syn_sim_matrix.mean(), real_sim_matrix.mean()) + \
                   F.mse_loss(syn_sim_matrix.std(), real_sim_matrix.std())
        
        # 3. è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤± (CGM Loss)
        real_gap = real_audio.mean(0) - real_text.mean(0)
        syn_gap = syn_audio.mean(0) - syn_text.mean(0)
        loss_cgm = F.mse_loss(syn_gap, real_gap)
        
        return loss_dm + self.lam_icm * loss_icm + self.lam_cgm * loss_cgm
        
class ImageBindDCLoss:
    """
    ImageBindDCæŸå¤±å‡½æ•° - ä¿®æ­£åé‡‡ç”¨çœŸå®çš„CFDå®ç°ã€‚
    """
    def __init__(self, lam_cross=1.0, lam_joint=1.0, num_freqs=4096, device='cuda'):
        self.lam_cross = lam_cross
        self.lam_joint = lam_joint
        self.num_freqs = num_freqs
        self.device = device
        self._t_cache = {} # ç”¨äºç¼“å­˜ä¸åŒç»´åº¦çš„éšæœºé¢‘ç‡å‘é‡t

    def _get_t(self, feature_dim):
        """æ ¹æ®ç‰¹å¾ç»´åº¦è·å–æˆ–ç”Ÿæˆéšæœºé¢‘ç‡å‘é‡t"""
        if feature_dim not in self._t_cache:
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºé¢‘ç‡
            self._t_cache[feature_dim] = torch.randn(feature_dim, self.num_freqs, device=self.device)
        return self._t_cache[feature_dim]

    def compute_cfd(self, feat1: torch.Tensor, feat2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„ç‰¹å¾å‡½æ•°è·ç¦» (CFD)ã€‚
        å…¬å¼: L2^2(Î¦_x(t), Î¦_y(t))
        """
        feature_dim = feat1.shape[1]
        t = self._get_t(feature_dim) # è·å–éšæœºé¢‘ç‡å‘é‡

        # 1. å°†ç‰¹å¾æŠ•å½±åˆ°éšæœºé¢‘ç‡ä¸Š (t^T * x)
        proj1 = torch.matmul(feat1, t)
        proj2 = torch.matmul(feat2, t)

        # 2. è®¡ç®—ç‰¹å¾å‡½æ•° (é€šè¿‡é‡‡æ ·è¿‘ä¼¼æœŸæœ› E[exp(j * proj)])
        # exp(j*z) = cos(z) + j*sin(z)
        # E[cos(z)] å’Œ E[sin(z)] é€šè¿‡åœ¨batchç»´åº¦ä¸Šæ±‚å‡å€¼æ¥è¿‘ä¼¼
        phi1_real = torch.cos(proj1).mean(dim=0)
        phi1_imag = torch.sin(proj1).mean(dim=0)
        phi2_real = torch.cos(proj2).mean(dim=0)
        phi2_imag = torch.sin(proj2).mean(dim=0)

        # 3. è®¡ç®—ä¸¤ä¸ªå¤æ•°ç‰¹å¾å‡½æ•°ä¹‹é—´çš„L2è·ç¦»çš„å¹³æ–¹
        dist_sq = (phi1_real - phi2_real)**2 + (phi1_imag - phi2_imag)**2
        
        # 4. åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„CFDæŸå¤±
        return dist_sq.sum()

    def __call__(self, real_features, synthetic_features):
        real_audio, real_text = real_features
        syn_audio, syn_text = synthetic_features

        # ç”±äºçœŸå®æ•°æ®é‡è¿œå¤§äºåˆæˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­é‡‡æ ·ä¸€ä¸ªå­é›†æ¥è¿›è¡ŒåŒ¹é…
        num_syn = len(syn_audio)
        rand_idx = torch.randperm(len(real_audio))[:num_syn]
        real_audio_subset = real_audio[rand_idx]
        real_text_subset = real_text[rand_idx]

        # 1. å•æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Uni-modal Distribution Matching)
        loss_uni = self.compute_cfd(real_audio_subset, syn_audio) + \
                   self.compute_cfd(real_text_subset, syn_text)
        
        # 2. è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é… (Cross-modal Distribution Matching)
        # éµå¾ªè®ºæ–‡å…¬å¼ CFD(Real_Audio + Syn_Text, Real_Text + Syn_Audio)
        mix_dist1 = real_audio_subset + syn_text
        mix_dist2 = real_text_subset + syn_audio
        loss_cross = self.compute_cfd(mix_dist1, mix_dist2)
        
        # 3. è”åˆåˆ†å¸ƒåŒ¹é… (Joint Distribution Matching)
        # æ‹¼æ¥ç‰¹å¾ï¼ŒåŒ¹é…å…¶è”åˆåˆ†å¸ƒ
        joint_real = torch.cat([real_audio_subset, real_text_subset], dim=1)
        joint_syn = torch.cat([syn_audio, syn_text], dim=1)
        loss_joint = self.compute_cfd(joint_real, joint_syn)
        
        return loss_uni + self.lam_cross * loss_cross + self.lam_joint * loss_joint
# ===================== æ•°æ®è’¸é¦ =====================

class DataDistillation:
    """æ•°æ®è’¸é¦å¼•æ“"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.herding = StandardHerdingSelector(device)
    
    # åœ¨ DataDistillation ç±»ä¸­
    def distill(self, real_audio_features, real_text_features, args, method='avdd', feature_space='imagebind'):
        """æ ‡å‡†è’¸é¦æµç¨‹ï¼Œç­¾åå·²ä¿®æ”¹ä¸ºæ¥æ”¶args"""
        
        print(f"\nğŸ”¬ å¼€å§‹{method} on {feature_space}è’¸é¦...")
        
        num_syn_samples = args.ipc
        iterations = args.distill_iter
    
        audio_indices, audio_coreset = self.herding.select_coreset(real_audio_features, num_syn_samples)
        text_indices, text_coreset = self.herding.select_coreset(real_text_features, num_syn_samples)
        
        syn_audio = audio_coreset.clone().detach().requires_grad_(True)
        syn_text = text_coreset.clone().detach().requires_grad_(True)
        
        # ã€æœ€ç»ˆä¿®æ­£ã€‘ç¡®ä¿ImageBindDCLossä¹Ÿä½¿ç”¨argsä¸­çš„æƒé‡
        if method == 'avdd':
            loss_fn = AVDDLoss(lam_icm=args.lam_icm, lam_cgm=args.lam_cgm)
        else:  # imagebind_dc
            loss_fn = ImageBindDCLoss(lam_cross=args.lam_cross, lam_joint=args.lam_joint, device=self.device)
        
        current_lr = args.distill_lr_avdd if method == 'avdd' else args.distill_lr_cfd
        print(f"  ä½¿ç”¨å­¦ä¹ ç‡: {current_lr}")
        optimizer = torch.optim.Adam([syn_audio, syn_text], lr=current_lr)
        
        loss_history = []
        
        for iteration in range(iterations):
            loss = loss_fn(
                (real_audio_features, real_text_features),
                (syn_audio, syn_text)
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_history.append(loss.item())
            
            if iteration % 50 == 0 or iteration == iterations - 1:
                if not args.disable_wandb:
                    wandb.log({f"distill_loss/{method}_{feature_space}": loss.item()})
                print(f"è¿­ä»£ {iteration}/{iterations}, æŸå¤±: {loss.item():.6f}")
        
        return {
            'synthetic_audio': syn_audio.detach(),
            'synthetic_text': syn_text.detach(),
            'loss_history': loss_history
        }
# ===================== æ£€ç´¢æ¨¡å‹è®­ç»ƒ =====================
class StudentRetriever(nn.Module):
    """
    ç»Ÿä¸€çš„å­¦ç”Ÿæ£€ç´¢æ¨¡å‹ã€‚
    å®ƒåŒ…å«ä¸€ä¸ªå›ºå®šçš„ã€é¢„è®­ç»ƒçš„ImageBindä¸»å¹²å’Œä¸€ä¸ªå¯è®­ç»ƒçš„æ˜ å°„å¤´ã€‚
    """
    def __init__(self, feature_dim=512, device='cuda'):
        super().__init__()
        self.device = device
        print("åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹: å†»ç»“ImageBindä¸»å¹² + å¯è®­ç»ƒæ˜ å°„å¤´")

        # 1. åŠ è½½å¹¶å†»ç»“ImageBindä¸»å¹²ç½‘ç»œ
        class MockArgs:
            def __init__(self):
                self.arch_frame = 'imagebind'
                self.arch_classifier = 'ensemble'
                self.cls_num = 10
                self.weights_classifier = ''
                self.input_modality = 'av'
        nets, _ = get_network_imagebind(MockArgs())
        self.imagebind_backbone, _ = nets
        self.imagebind_backbone.to(self.device)
        for param in self.imagebind_backbone.parameters():
            param.requires_grad = False
        self.imagebind_backbone.eval()
        self.embed_func = self.imagebind_backbone.module.embed if torch.cuda.device_count() > 1 else self.imagebind_backbone.embed
        
        # 2. å®šä¹‰å¯è®­ç»ƒçš„æ˜ å°„å¤´ (è¾“å…¥ç»´åº¦ä¸ºImageBindçš„1024)
        self.audio_head = nn.Sequential(nn.Linear(1024, feature_dim))
        self.text_head = nn.Sequential(nn.Linear(1024, feature_dim))

    def forward(self, audio_features, text_features):
        audio_feat_projected = self.audio_head(audio_features)
        text_feat_projected = self.text_head(text_features)
        return F.normalize(audio_feat_projected, dim=-1), F.normalize(text_feat_projected, dim=-1)

    def project_real_data(self, audio_paths, text_strings):
        """åœ¨è¯„ä¼°æ—¶ï¼Œå…ˆç”¨å†»ç»“ä¸»å¹²æå–ç‰¹å¾ï¼Œå†ç”¨å¤´éƒ¨æŠ•å½±"""
        inputs = {
            ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, self.device),
            ModalityType.TEXT: data.load_and_transform_text(text_strings, self.device),
        }
        with torch.no_grad():
            embeddings = self.embed_func(inputs)
            audio_ib_features = embeddings[ModalityType.AUDIO].detach()
            text_ib_features = embeddings[ModalityType.TEXT].detach()
            
            projected_audio, projected_text = self.forward(audio_ib_features, text_ib_features)
        return projected_audio, projected_text

class StudentRetriever(nn.Module):
    """
    ç”¨äº ImageBind ç‰¹å¾ç©ºé—´çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    ã€å…³é”®ã€‘ï¼šå®ƒåŒ…å«ä¸€ä¸ªå›ºå®šçš„ã€é¢„è®­ç»ƒçš„ImageBindä¸»å¹²å’Œä¸€ä¸ªå¯è®­ç»ƒçš„æ˜ å°„å¤´ã€‚
    """
    def __init__(self, feature_dim=512, device='cuda'):
        super().__init__()
        self.device = device
        print("åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹: å†»ç»“ImageBindä¸»å¹² + å¯è®­ç»ƒæ˜ å°„å¤´")

        # 1. åŠ è½½å¹¶å†»ç»“ImageBindä¸»å¹²ç½‘ç»œ
        class MockArgs:
            def __init__(self):
                self.arch_frame = 'imagebind'
                self.arch_classifier = 'ensemble'
                self.cls_num = 10
                self.weights_classifier = ''
                self.input_modality = 'av'
        nets, _ = get_network_imagebind(MockArgs())
        self.imagebind_backbone, _ = nets
        self.imagebind_backbone.to(self.device)
        for param in self.imagebind_backbone.parameters():
            param.requires_grad = False # å†»ç»“
        self.imagebind_backbone.eval()
        self.embed_func = self.imagebind_backbone.module.embed if torch.cuda.device_count() > 1 else self.imagebind_backbone.embed
        
        # 2. å®šä¹‰ã€å¯è®­ç»ƒã€‘çš„æ˜ å°„å¤´ (è¾“å…¥ç»´åº¦ä¸ºImageBindçš„1024)
        self.audio_head = nn.Sequential(nn.Linear(1024, feature_dim))
        self.text_head = nn.Sequential(nn.Linear(1024, feature_dim))

    def forward(self, audio_features, text_features):
        """è¿™ä¸ªforwardåªé€šè¿‡å¯è®­ç»ƒçš„å¤´éƒ¨"""
        audio_feat_projected = self.audio_head(audio_features)
        text_feat_projected = self.text_head(text_features)
        return F.normalize(audio_feat_projected, dim=-1), F.normalize(text_feat_projected, dim=-1)
        
class RetrievalTrainer:

    
    def __init__(self, device='cuda'):
        self.device = device

    def info_nce_loss(self, audio_feat, text_feat, temperature=0.07):
        logits = torch.matmul(audio_feat, text_feat.t()) / temperature
        labels = torch.arange(len(audio_feat), device=self.device)
        return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)) / 2

    def train(self, syn_dataset, real_feature_dataset, args, student_model_type, feature_dim=512):
        print(f"\nğŸ¯ è®­ç»ƒå­¦ç”Ÿæ¨¡å‹... (ç±»å‹: {student_model_type}, Replay Ratio: 0.5)")
        

        real_dataloader = DataLoader(real_feature_dataset, batch_size=args.student_batch_size // 2, shuffle=True, num_workers=0)
        real_iterator = iter(real_dataloader)
        
        syn_dataloader = DataLoader(syn_dataset, batch_size=args.student_batch_size // 2, shuffle=True, num_workers=0)
        syn_iterator = iter(syn_dataloader)

        if student_model_type == 'convnet':
            input_dim = syn_dataset.audio.shape[1]
            model = RetrievalModel(input_dim=input_dim, feature_dim=feature_dim).to(self.device)
            optimizer = torch.optim.Adam(model.parameters(), lr=args.student_lr, weight_decay=1e-4) # æ·»åŠ weight_decay
        elif student_model_type == 'imagebind':
            model = StudentRetriever(feature_dim=feature_dim, device=self.device).to(self.device)
            optimizer = torch.optim.Adam(
                list(model.audio_head.parameters()) + list(model.text_head.parameters()),
                lr=args.student_lr,
                weight_decay=1e-4
            )
        
        model.train()
        for epoch in range(args.student_epochs):
            total_loss = 0

            for i in range(len(syn_dataloader)):
                try:
                    syn_batch = next(syn_iterator)
                except StopIteration:
                    syn_iterator = iter(syn_dataloader)
                    syn_batch = next(syn_iterator)
                
                try:
                    real_batch = next(real_iterator)
                except StopIteration:
                    real_iterator = iter(real_dataloader)
                    real_batch = next(real_iterator)


                audio = torch.cat([syn_batch['audio'], real_batch['audio']], dim=0).to(self.device)
                text = torch.cat([syn_batch['text'], real_batch['text']], dim=0).to(self.device)
                
                audio_feat, text_feat = model(audio, text)
                loss = self.info_nce_loss(audio_feat, text_feat)
                optimizer.zero_grad(); loss.backward(); optimizer.step()
                total_loss += loss.item()

            if (epoch + 1) % 10 == 0 or epoch == args.student_epochs - 1:
                print(f"è®­ç»ƒè½®æ¬¡ {epoch+1}/{args.student_epochs}, å¹³å‡æŸå¤±: {total_loss/len(syn_dataloader):.4f}")
        
        model.eval()
        return model

# ===================== æ ‡å‡†è¯„ä¼° =====================

class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨ï¼ˆæœ€ç»ˆç‰ˆï¼‰"""
    
    def __init__(self, device='cuda'):
        self.device = device
    
    def evaluate(self, model, real_audio_features, real_text_features):
        """æ ‡å‡†Recall@Kè¯„ä¼°ï¼Œæ¥å£ç»Ÿä¸€"""
        
        print("\nğŸ¯ æ ‡å‡†æ£€ç´¢è¯„ä¼°...")
        model.eval()
        with torch.no_grad():
            # ä¸è®ºå­¦ç”Ÿæ¨¡å‹æ˜¯å“ªç§ï¼Œéƒ½é€šè¿‡å…¶forwardæ–¹æ³•è·å¾—å¯¹çœŸå®ç‰¹å¾çš„æœ€ç»ˆæŠ•å½±
            audio_feat, text_feat = model(real_audio_features.to(self.device), real_text_features.to(self.device))
            
            sim_matrix = torch.matmul(audio_feat, text_feat.t())
            
            results = {}
            for k in [1, 5, 10, 20]:
                # A2Tå¬å›ç‡ (æ›´é«˜æ•ˆçš„è®¡ç®—æ–¹å¼)
                _, topk_a2t = torch.topk(sim_matrix, k=k, dim=1)
                correct_a2t = torch.any(torch.arange(len(audio_feat), device=self.device)[:, None] == topk_a2t, dim=1)
                recall_a2t = torch.mean(correct_a2t.float()).item()
                
                # T2Aå¬å›ç‡
                _, topk_t2a = torch.topk(sim_matrix.t(), k=k, dim=1)
                correct_t2a = torch.any(torch.arange(len(text_feat), device=self.device)[:, None] == topk_t2a, dim=1)
                recall_t2a = torch.mean(correct_t2a.float()).item()
                
                results[f'R@{k}_a2t'] = recall_a2t
                results[f'R@{k}_t2a'] = recall_t2a
        
        return results
# ===================== ä¸»å®éªŒæµç¨‹ =====================

class ClothoExperiment:
    """Clothoæ•°æ®é›†å®Œæ•´å®éªŒ"""
    def __init__(self, dev_audio_dir: str, dev_captions_file: str, eval_audio_dir: str, eval_captions_file: str, device='cuda'):
            self.device = device
            # å¼€å‘é›†è·¯å¾„
            self.dev_audio_dir = dev_audio_dir
            self.dev_captions_file = dev_captions_file
            # è¯„ä¼°é›†è·¯å¾„
            self.eval_audio_dir = eval_audio_dir
            self.eval_captions_file = eval_captions_file
            
            self.vocab = None # ç”¨äºå­˜å‚¨è¯æ±‡è¡¨
            self.distiller = DataDistillation(device)
            self.trainer = RetrievalTrainer(device)
            self.evaluator = StandardEvaluator(device)
    
    # åœ¨ ClothoExperiment ç±»ä¸­
    def run_complete_experiment(self, args):
        """
        æ‰§è¡Œæœ€ç»ˆç‰ˆå®éªŒï¼ˆä»…ImageBindç©ºé—´ï¼‰ï¼Œå¯¹æ¯” DM, AVDD, ImageBindDCï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœã€‚
        """
        print("ğŸ† Clothoæ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ (ImageBind Space Only)")
        print("=" * 80)
        
        # ===================== é˜¶æ®µé›¶ï¼šå‡†å¤‡æ•°æ®å’ŒImageBindç‰¹å¾ =====================
        print("ğŸ“Š é˜¶æ®µé›¶ï¼šåŠ è½½æ•°æ®å¹¶ä¸ºImageBindé¢„è®¡ç®—ç‰¹å¾...")
    
        dev_dataloader, self.vocab = get_high_performance_dataloader(
            audio_dir=self.dev_audio_dir,
            captions_file=self.dev_captions_file,
            batch_size=32,
            num_samples=args.dev_samples
        )
        
        eval_dataloader, _ = get_high_performance_dataloader(
            audio_dir=self.eval_audio_dir,
            captions_file=self.eval_captions_file,
            batch_size=32,
            num_samples=None,
            vocab=self.vocab
        )
    
        imagebind_extractor = ImageBindExtractor(pretrained=True, device=self.device)
    
        print("\n--- åœ¨å¼€å‘é›†ä¸Šé¢„è®¡ç®—ã€è’¸é¦ç”¨ å’Œ é‡æ”¾ç”¨ã€‘ImageBindç‰¹å¾ ---")
        real_imagebind_audio, real_imagebind_text = precompute_real_features(dev_dataloader, imagebind_extractor, self.device)
        
        print("\n--- åœ¨è¯„ä¼°é›†ä¸Šé¢„è®¡ç®—ã€è¯„ä¼°ç”¨ã€‘ImageBindç‰¹å¾ ---")
        eval_imagebind_audio, eval_imagebind_text = precompute_real_features(eval_dataloader, imagebind_extractor, self.device)
        
        real_replay_imagebind_dataset = SyntheticDataset(real_imagebind_audio, real_imagebind_text)
    
        # ===================== é˜¶æ®µä¸€ï¼šä¸‰ç§è’¸é¦æ–¹æ³• =====================
        print("\nğŸ”¬ é˜¶æ®µä¸€ï¼šåœ¨ImageBindç©ºé—´ä¸Šæ‰§è¡Œä¸‰ç§æ•°æ®è’¸é¦æ–¹æ³•...")
        
        print("\n--- (1/3) AVDD on ImageBind ---")
        result_avdd = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='avdd')
        # ã€æ–°å¢ä»£ç ã€‘ä¿å­˜è’¸é¦äº§å‡ºçš„æ•°æ®
        avdd_save_path = f'distilled_data_avdd_ipc{args.ipc}_seed{args.seed}.pth'
        torch.save({
            'synthetic_audio': result_avdd['synthetic_audio'],
            'synthetic_text': result_avdd['synthetic_text']
        }, avdd_save_path)
        print(f"ğŸ’¾ AVDDè’¸é¦æ•°æ®å·²ä¿å­˜è‡³: {avdd_save_path}")
        
        print("\n--- (2/3) ImageBindDC on ImageBind ---")
        result_ibdc = self.distiller.distill(real_imagebind_audio, real_imagebind_text, args, method='imagebind_dc')
        # ã€æ–°å¢ä»£ç ã€‘ä¿å­˜è’¸é¦äº§å‡ºçš„æ•°æ®
        ibdc_save_path = f'distilled_data_imagebinddc_ipc{args.ipc}_seed{args.seed}.pth'
        torch.save({
            'synthetic_audio': result_ibdc['synthetic_audio'],
            'synthetic_text': result_ibdc['synthetic_text']
        }, ibdc_save_path)
        print(f"ğŸ’¾ ImageBindDCè’¸é¦æ•°æ®å·²ä¿å­˜è‡³: {ibdc_save_path}")
    
        print("\n--- (3/3) DM on ImageBind ---")
        import copy
        temp_args = copy.deepcopy(args)
        temp_args.lam_icm = 0.0
        temp_args.lam_cgm = 0.0
        result_dm = self.distiller.distill(real_imagebind_audio, real_imagebind_text, temp_args, method='avdd')
        # ã€æ–°å¢ä»£ç ã€‘ä¿å­˜è’¸é¦äº§å‡ºçš„æ•°æ®
        dm_save_path = f'distilled_data_dm_ipc{args.ipc}_seed{args.seed}.pth'
        torch.save({
            'synthetic_audio': result_dm['synthetic_audio'],
            'synthetic_text': result_dm['synthetic_text']
        }, dm_save_path)
        print(f"ğŸ’¾ DMè’¸é¦æ•°æ®å·²ä¿å­˜è‡³: {dm_save_path}")
    
        # ===================== é˜¶æ®µäºŒ & ä¸‰ï¼šè®­ç»ƒã€è¯„ä¼°ä¸å¯è§†åŒ– =====================
        print("\nğŸ”¬ é˜¶æ®µäºŒ & ä¸‰ï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒã€è¯„ä¼°ä¸å¯è§†åŒ–...")
        
        distilled_datasets = [
            ("AVDD", result_avdd, (eval_imagebind_audio, eval_imagebind_text)),
            ("ImageBindDC", result_ibdc, (eval_imagebind_audio, eval_imagebind_text)),
            ("DM", result_dm, (eval_imagebind_audio, eval_imagebind_text))
        ]
        
        final_results = {}
        # ã€æ–°å¢ã€‘åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºæ”¶é›†æ‰€æœ‰æ–¹æ³•çš„t-SNEæ•°æ®
        tsne_data_to_plot = []

        for name, synthetic_data, real_eval_features in distilled_datasets:
            print(f"\n--- æ­£åœ¨å¤„ç†æ–¹æ³•: {name} ---")
            syn_dataset = SyntheticDataset(synthetic_data['synthetic_audio'], synthetic_data['synthetic_text'])
            
            trained_model = self.trainer.train(
                syn_dataset=syn_dataset, 
                real_feature_dataset=real_replay_imagebind_dataset,
                args=args, 
                student_model_type='imagebind',
                feature_dim=512
            )
            
            eval_audio_features, eval_text_features = real_eval_features
            eval_results = self.evaluator.evaluate(trained_model, eval_audio_features, eval_text_features)
            final_results[name] = eval_results
    
            # --- å¯è§†åŒ–è°ƒç”¨ ---
            
            # ã€æ›¿æ¢ã€‘åŸæœ‰çš„t-SNEç”Ÿæˆéƒ¨åˆ†
            # ä¸å†åœ¨æ­¤å¤„å•ç‹¬ç”Ÿæˆt-SNEå›¾ï¼Œè€Œæ˜¯æ”¶é›†æ•°æ®ç”¨äºåç»­çš„ç»Ÿä¸€ç»˜å›¾
            with torch.no_grad():
                projected_audio, projected_text = trained_model(
                    eval_audio_features.to(self.device), 
                    eval_text_features.to(self.device)
                )
            # å°†å½“å‰æ–¹æ³•çš„ç»“æœæ·»åŠ åˆ°æ”¶é›†ä¸­
            tsne_data_to_plot.append({
                'name': name,
                'projected_audio': projected_audio.cpu(), # ç§»åŠ¨åˆ°CPUä»¥èŠ‚çº¦æ˜¾å­˜
                'projected_text': projected_text.cpu()
            })
            
            # (å®šæ€§æ£€ç´¢éƒ¨åˆ†ä¿æŒä¸å˜)
            retrieval_save_path = f'retrieval_examples_{name}_seed{args.seed}.png'
            visualize_retrieval_examples(
                model=trained_model,
                eval_dataloader=eval_dataloader,
                eval_audio_features=eval_audio_features,
                eval_text_features=eval_text_features,
                device=self.device,
                file_path=retrieval_save_path,
                num_examples=3,
                top_k=3
            )

        # ã€æ–°å¢ã€‘åœ¨æ‰€æœ‰æ–¹æ³•å¾ªç¯ç»“æŸåï¼Œè°ƒç”¨æ–°çš„å¯¹æ¯”å¯è§†åŒ–å‡½æ•°
        tsne_comparison_save_path = f'tsne_comparison_seed{args.seed}.png'
        # å¯¹åˆ—è¡¨è¿›è¡Œæ’åºï¼Œç¡®ä¿ç»˜å›¾é¡ºåºæ˜¯ DM -> AVDD -> Ours
        tsne_data_to_plot.sort(key=lambda x: {"DM": 0, "AVDD": 1, "ImageBindDC": 2}.get(x['name'], 99))
        visualize_tsne_comparison(
            results_list=tsne_data_to_plot,
            file_path=tsne_comparison_save_path
        )
        
        # ===================== é˜¶æ®µå››ï¼šç»“æœæ±‡æ€» =====================
        print("\n" + "=" * 80)
        print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”çŸ©é˜µ")
        print("=" * 80)
        
        # å®šä¹‰æ‰“å°è¡¨å¤´çš„æ ¼å¼
        header = (f"{'æ–¹æ³•':<25} "
                  f"{'R@1_A2T':<12} {'R@1_T2A':<12} "
                  f"{'R@5_A2T':<12} {'R@5_T2A':<12} "
                  f"{'R@10_A2T':<12} {'R@10_T2A':<12}")
        
        print(header)
        print("-" * len(header))
        
        # ä¸ºäº†æ¸…æ™°å¯¹æ¯”ï¼ŒæŒ‰é¢„è®¾é¡ºåºæ‰“å°ç»“æœ
        method_order = ["DM", "AVDD", "ImageBindDC"]
        
        for name in method_order:
            if name in final_results:
                result = final_results[name]
                # ä½¿ç”¨ .get(key, 0.0) æ¥å®‰å…¨åœ°è®¿é—®ç»“æœï¼Œä»¥é˜²æœªæ¥æŸä¸ªæŒ‡æ ‡è®¡ç®—å¤±è´¥
                print(f"{name:<25} "
                      f"{result.get('R@1_a2t', 0.0):<12.4f} {result.get('R@1_t2a', 0.0):<12.4f} "
                      f"{result.get('R@5_a2t', 0.0):<12.4f} {result.get('R@5_t2a', 0.0):<12.4f} "
                      f"{result.get('R@10_a2t', 0.0):<12.4f} {result.get('R@10_t2a', 0.0):<12.4f}")
    
        # ä¿å­˜å®Œæ•´ç»“æœ
        final_report = {
            'args': vars(args),
            'results': final_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        filename = f'clotho_experiment_ipc{args.ipc}_seed{args.seed}_results.pth'
        torch.save(final_report, filename)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {filename}")
        
        return final_results
# ===================== ä¸»å…¥å£ =====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Clotho æ•°æ®é›†æ ‡å‡†æ•°æ®è’¸é¦å®éªŒ")

    # --- è·¯å¾„ä¸æ•°æ®å‚æ•° ---
    parser.add_argument('--base_path', type=str, default='/root/autodl-tmp/audio-visual-master/data/Clotho/', help='Clothoæ•°æ®é›†çš„æ ¹ç›®å½•')
    parser.add_argument('--dev_audio_dir_name', type=str, default='development', help='å¼€å‘é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--eval_audio_dir_name', type=str, default='evaluation', help='è¯„ä¼°é›†éŸ³é¢‘æ–‡ä»¶å¤¹å')
    parser.add_argument('--dev_captions_name', type=str, default='clotho_captions_development.csv', help='å¼€å‘é›†å­—å¹•æ–‡ä»¶å')
    parser.add_argument('--eval_captions_name', type=str, default='clotho_captions_evaluation.csv', help='è¯„ä¼°é›†å­—å¹•æ–‡ä»¶å')

    # --- æ ¸å¿ƒå®éªŒå‚æ•° ---
    parser.add_argument('--ipc', type=int, default=20, help='æ¯ä¸ªç±»åˆ«è’¸é¦å‡ºçš„åˆæˆæ ·æœ¬æ•°é‡ (num_syn_samples)')
    parser.add_argument('--dev_samples', type=int, default=2893, help='ç”¨äºè’¸é¦çš„å¼€å‘é›†æ ·æœ¬æ•° (num_real_samples)')
    parser.add_argument('--distill_iter', type=int, default=5000, help='è’¸é¦ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--student_epochs', type=int, default=100, help='è®­ç»ƒå­¦ç”Ÿæ£€ç´¢æ¨¡å‹çš„è½®æ•°')
    parser.add_argument('--distill_lr_avdd', type=float, default=0.001, help='[AVDDæ–¹æ³•] çš„è’¸é¦å­¦ä¹ ç‡')
    parser.add_argument('--distill_lr_cfd', type=float, default=0.01, help='[ImageBindDC/CFDæ–¹æ³•] çš„è’¸é¦å­¦ä¹ ç‡')
    parser.add_argument('--student_lr', type=float, default=0.001, help='å­¦ç”Ÿæ¨¡å‹è®­ç»ƒé˜¶æ®µçš„å­¦ä¹ ç‡')
    # åœ¨ "æ ¸å¿ƒå®éªŒå‚æ•°" éƒ¨åˆ†
    parser.add_argument('--student_batch_size', type=int, default=16, help='å­¦ç”Ÿæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤§å° (ä¸€åŠçœŸå®, ä¸€åŠåˆæˆ)')
    # --- AVDD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_icm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, æ¨¡æ€é—´ä¸€è‡´æ€§æŸå¤±(ICM)çš„æƒé‡')
    parser.add_argument('--lam_cgm', type=float, default=10.0, help='AVDDæŸå¤±ä¸­, è·¨æ¨¡æ€å…¨å±€åŒ¹é…æŸå¤±(CGM)çš„æƒé‡')
    
    # --- ImageBindDC/CFD æŸå¤±æƒé‡ ---
    parser.add_argument('--lam_cross', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è·¨æ¨¡æ€åˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')
    parser.add_argument('--lam_joint', type=float, default=1.0, help='CFDæŸå¤±ä¸­, è”åˆåˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡')

    # --- å¯å¤ç°æ€§ä¸ç›‘æ§ ---
    parser.add_argument('--seed', type=int, default=42, help='å…¨å±€éšæœºç§å­')
    parser.add_argument('--num_runs', type=int, default=1, help='ä½¿ç”¨ä¸åŒéšæœºç§å­é‡å¤å®éªŒçš„æ¬¡æ•°')
    parser.add_argument('--wandb_project', type=str, default="clotho_distillation_final", help='Wandbé¡¹ç›®åç§°')
    parser.add_argument('--wandb_entity', type=str, default=None, help='Wandbçš„å®ä½“/ç”¨æˆ·å (å¯é€‰)')
    parser.add_argument('--disable_wandb', action='store_true', help='å¦‚æœè®¾ç½®, åˆ™ç¦ç”¨wandbæ—¥å¿—')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    args = parser.parse_args()

    # ä¸»å¾ªç¯ï¼Œç”¨äºå¤šæ¬¡éšæœºç§å­å®éªŒ
    for i in range(args.num_runs):
        current_seed = args.seed + i
        print(f"\n{'='*30}  è¿è¡Œç¬¬ {i+1}/{args.num_runs} æ¬¡, éšæœºç§å­: {current_seed}  {'='*30}\n")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(current_seed)
        np.random.seed(current_seed)

        # æ‹¼æ¥å®Œæ•´çš„è·¯å¾„
        DEV_AUDIO_DIR = os.path.join(args.base_path, args.dev_audio_dir_name)
        DEV_CAPTIONS_FILE = os.path.join(args.base_path, args.dev_captions_name)
        EVAL_AUDIO_DIR = os.path.join(args.base_path, args.eval_audio_dir_name)
        EVAL_CAPTIONS_FILE = os.path.join(args.base_path, args.eval_captions_name)

        # æ£€æŸ¥è·¯å¾„
        if not all(os.path.exists(p) for p in [DEV_AUDIO_DIR, DEV_CAPTIONS_FILE]):
            print(f"é”™è¯¯ï¼šè¯·ç¡®ä¿å¼€å‘é›†æ•°æ®è·¯å¾„éƒ½å­˜åœ¨ï¼")
            print(f"  - å¼€å‘é›†éŸ³é¢‘: {DEV_AUDIO_DIR}")
            print(f"  - å¼€å‘é›†å­—å¹•: {DEV_CAPTIONS_FILE}")
            # è¯„ä¼°é›†è·¯å¾„çš„æ£€æŸ¥å¯ä»¥æ”¾åˆ°å®éªŒæµç¨‹å†…éƒ¨ï¼Œå› ä¸ºå¯èƒ½åªåšè’¸é¦ä¸åšè¯„ä¼°
            continue # å¦‚æœå¼€å‘é›†ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡æœ¬æ¬¡è¿è¡Œ

        # åˆå§‹åŒ–Wandb
        if not args.disable_wandb:
            wandb.init(
                project=args.wandb_project,
                entity=args.wandb_entity,
                config=args,
                name=f"run_{i+1}_seed_{current_seed}",
                reinit=True # å…è®¸å¤šæ¬¡åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­åˆå§‹åŒ–
            )
    
        # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
        experiment = ClothoExperiment(
            dev_audio_dir=DEV_AUDIO_DIR,
            dev_captions_file=DEV_CAPTIONS_FILE,
            eval_audio_dir=EVAL_AUDIO_DIR,
            eval_captions_file=EVAL_CAPTIONS_FILE,
            device=device
        )
        experiment.run_complete_experiment(args) # å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™å®éªŒä¸»å‡½æ•°

        if not args.disable_wandb:
            wandb.finish()